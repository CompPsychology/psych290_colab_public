{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CompPsychology/psych290_colab_public/blob/main/notebooks/week-06/W6_Tutorial_09_DLATK_1to3gram_tuneCorrelation_(dla_tutorial).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zypR6Pv_VXSQ"
      },
      "source": [
        "# W6 Tutorial 9 -- 1to3gram extraction, fine-tuning and correlation word clouds (2025-04)\n",
        "\n",
        "(c) Johannes Eichstaedt & the World Well-Being Project, 2023.\n",
        "\n",
        "\n",
        "✋🏻✋🏻 NOTE - You need to create a copy of this notebook before you work through it. Click on \"Save a copy in Drive\" option in the File menu, and safe it to your Google Drive.\n",
        "\n",
        "✉️🐞 If you find a bug/something doesn't work, please slack us a screenshot, or email johannes.courses@gmail.com.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial we will explore what we need to do to correlate words **and phrases** directly with outcomes. This will involve the same `--correlate` command as before, but importantly, we need to pre-process the 1to3grams with DLATK:\n",
        "* extract them\n",
        "* filter rare words\n",
        "* filter phrases to those that are really informative -- \"United States\" (yes) vs. \"the dog\" (no)\n",
        "* correlate them against outcomes, and create word clouds\n",
        "* and while we are at it: let's also make wordclouds of sets of dictionaries, like LIWC"
      ],
      "metadata": {
        "id": "iYu4tGLHrS0O"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRyhkbB7VXSR"
      },
      "source": [
        "Let's set up Colab, as usual."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Setting up Colab with DLATK and SQLite\n"
      ],
      "metadata": {
        "id": "aJ-sQIKura3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "database=\"dla_tutorial\""
      ],
      "metadata": {
        "id": "Tcyp5vsoriq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1a) Install DLATK"
      ],
      "metadata": {
        "id": "zVf7Sw7zroJ5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-JeYt4fVXSR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d276710a-f0d4-4bb4-c742-35204ae5d36c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dlatk'...\n",
            "remote: Enumerating objects: 6996, done.\u001b[K\n",
            "remote: Counting objects: 100% (1176/1176), done.\u001b[K\n",
            "remote: Compressing objects: 100% (168/168), done.\u001b[K\n",
            "remote: Total 6996 (delta 1082), reused 1023 (delta 1008), pack-reused 5820 (from 2)\u001b[K\n",
            "Receiving objects: 100% (6996/6996), 62.40 MiB | 6.03 MiB/s, done.\n",
            "Resolving deltas: 100% (4942/4942), done.\n",
            "Collecting image<=1.5.33 (from -r dlatk/install/requirements.txt (line 1))\n",
            "  Downloading image-1.5.33.tar.gz (15 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langid<=1.1.6,>=1.1.4 (from -r dlatk/install/requirements.txt (line 2))\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib<=3.10.0,>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 3)) (3.10.0)\n",
            "Collecting mysqlclient<=2.1.1 (from -r dlatk/install/requirements.txt (line 4))\n",
            "  Downloading mysqlclient-2.1.1.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.1/88.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk<=3.9.1,>=3.1 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 5)) (3.9.1)\n",
            "Requirement already satisfied: numpy<=2.0.2 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 6)) (2.0.2)\n",
            "Requirement already satisfied: pandas<=2.2.2,>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 7)) (2.2.2)\n",
            "Requirement already satisfied: patsy<=1.0.1,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 8)) (1.0.1)\n",
            "Collecting python-dateutil<=2.8.2,>=2.5.0 (from -r dlatk/install/requirements.txt (line 9))\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: rpy2<=3.5.17 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 10)) (3.5.17)\n",
            "Requirement already satisfied: scikit-learn<=1.6.1 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 11)) (1.6.1)\n",
            "Collecting scipy<=1.14.1,>=0.13.3 (from -r dlatk/install/requirements.txt (line 12))\n",
            "  Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting SQLAlchemy<=2.0.39,>=0.9.9 (from -r dlatk/install/requirements.txt (line 13))\n",
            "  Downloading sqlalchemy-2.0.39-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: statsmodels<=0.14.4,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 14)) (0.14.4)\n",
            "Requirement already satisfied: wordcloud<=1.9.4,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 15)) (1.9.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from image<=1.5.33->-r dlatk/install/requirements.txt (line 1)) (11.2.1)\n",
            "Collecting django (from image<=1.5.33->-r dlatk/install/requirements.txt (line 1))\n",
            "  Downloading django-5.2.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from image<=1.5.33->-r dlatk/install/requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10.0,>=1.3.1->-r dlatk/install/requirements.txt (line 3)) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10.0,>=1.3.1->-r dlatk/install/requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10.0,>=1.3.1->-r dlatk/install/requirements.txt (line 3)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10.0,>=1.3.1->-r dlatk/install/requirements.txt (line 3)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10.0,>=1.3.1->-r dlatk/install/requirements.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10.0,>=1.3.1->-r dlatk/install/requirements.txt (line 3)) (3.2.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<=3.9.1,>=3.1->-r dlatk/install/requirements.txt (line 5)) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<=3.9.1,>=3.1->-r dlatk/install/requirements.txt (line 5)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<=3.9.1,>=3.1->-r dlatk/install/requirements.txt (line 5)) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk<=3.9.1,>=3.1->-r dlatk/install/requirements.txt (line 5)) (4.67.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<=2.2.2,>=0.17.1->-r dlatk/install/requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<=2.2.2,>=0.17.1->-r dlatk/install/requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: cffi>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from rpy2<=3.5.17->-r dlatk/install/requirements.txt (line 10)) (1.17.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from rpy2<=3.5.17->-r dlatk/install/requirements.txt (line 10)) (3.1.6)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from rpy2<=3.5.17->-r dlatk/install/requirements.txt (line 10)) (5.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<=1.6.1->-r dlatk/install/requirements.txt (line 11)) (3.6.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<=2.0.39,>=0.9.9->-r dlatk/install/requirements.txt (line 13)) (3.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<=2.0.39,>=0.9.9->-r dlatk/install/requirements.txt (line 13)) (4.13.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.15.1->rpy2<=3.5.17->-r dlatk/install/requirements.txt (line 10)) (2.22)\n",
            "Collecting asgiref>=3.8.1 (from django->image<=1.5.33->-r dlatk/install/requirements.txt (line 1))\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from django->image<=1.5.33->-r dlatk/install/requirements.txt (line 1)) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->rpy2<=3.5.17->-r dlatk/install/requirements.txt (line 10)) (3.0.2)\n",
            "Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.2/41.2 MB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlalchemy-2.0.39-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m88.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading django-5.2.1-py3-none-any.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Building wheels for collected packages: image, langid, mysqlclient\n",
            "  Building wheel for image (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for image: filename=image-1.5.33-py2.py3-none-any.whl size=19482 sha256=c6553ddd4ff6bd4d1e8a0d729920659d8f6589466ad75b833b1039c287fa6abc\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/40/4f/3c9a8d0f22a1a6f966975a460e5cb509a1e7dc42e2ce5d9a6d\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941171 sha256=3097386c64d2e0045e91d36875cd33a198409079b995d3812f2dc6946d5dba64\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/6a/b6/b7eb43a6ad55b139c15c5daa29f3707659cfa6944d3c696f5b\n",
            "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.1.1-cp311-cp311-linux_x86_64.whl size=106435 sha256=6c736bae2cc711cfd97cb2aae6cd18890393cd2ffb1dfdef6cbe822f27ce57dd\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/44/b2/cdf313664c12c8b11f88afd90079823c63369ab9135fc79185\n",
            "Successfully built image langid mysqlclient\n",
            "Installing collected packages: SQLAlchemy, scipy, python-dateutil, mysqlclient, langid, asgiref, django, image\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.40\n",
            "    Uninstalling SQLAlchemy-2.0.40:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.40\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.2\n",
            "    Uninstalling scipy-1.15.2:\n",
            "      Successfully uninstalled scipy-1.15.2\n",
            "  Attempting uninstall: python-dateutil\n",
            "    Found existing installation: python-dateutil 2.9.0.post0\n",
            "    Uninstalling python-dateutil-2.9.0.post0:\n",
            "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
            "Successfully installed SQLAlchemy-2.0.39 asgiref-3.8.1 django-5.2.1 image-1.5.33 langid-1.1.6 mysqlclient-2.1.1 python-dateutil-2.8.2 scipy-1.14.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil"
                ]
              },
              "id": "941cb82dd40e45e28eb5ed20cb0b4d6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./dlatk\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk<=3.9.1,>=3.1 in /usr/local/lib/python3.11/dist-packages (from dlatk==1.3.1) (3.9.1)\n",
            "Requirement already satisfied: numpy<=2.0.2 in /usr/local/lib/python3.11/dist-packages (from dlatk==1.3.1) (2.0.2)\n",
            "Requirement already satisfied: pandas<=2.2.2,>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from dlatk==1.3.1) (2.2.2)\n",
            "Requirement already satisfied: patsy<=1.0.1,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from dlatk==1.3.1) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<=2.8.2,>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from dlatk==1.3.1) (2.8.2)\n",
            "Requirement already satisfied: scikit-learn<=1.6.1 in /usr/local/lib/python3.11/dist-packages (from dlatk==1.3.1) (1.6.1)\n",
            "Requirement already satisfied: scipy<=1.14.1,>=0.13.3 in /usr/local/lib/python3.11/dist-packages (from dlatk==1.3.1) (1.14.1)\n",
            "Requirement already satisfied: statsmodels<=0.14.4,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from dlatk==1.3.1) (0.14.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<=3.9.1,>=3.1->dlatk==1.3.1) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<=3.9.1,>=3.1->dlatk==1.3.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<=3.9.1,>=3.1->dlatk==1.3.1) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk<=3.9.1,>=3.1->dlatk==1.3.1) (4.67.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<=2.2.2,>=0.17.1->dlatk==1.3.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<=2.2.2,>=0.17.1->dlatk==1.3.1) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<=2.8.2,>=2.5.0->dlatk==1.3.1) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<=1.6.1->dlatk==1.3.1) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels<=0.14.4,>=0.5.0->dlatk==1.3.1) (24.2)\n",
            "Building wheels for collected packages: dlatk\n",
            "  Building wheel for dlatk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dlatk: filename=dlatk-1.3.1-py3-none-any.whl size=35635918 sha256=daf0bf39d672200675081d12681c6ebddd5226c26da72c85125f61fba9ee9172\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lshjrv8b/wheels/cc/c9/65/e1ecc64bac68518c07b286fe86921aa938e11a0c3a87d8ff93\n",
            "Successfully built dlatk\n",
            "Installing collected packages: dlatk\n",
            "Successfully installed dlatk-1.3.1\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: langid in /usr/local/lib/python3.11/dist-packages (1.1.6)\n",
            "Collecting jupysql\n",
            "  Downloading jupysql-0.11.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from wordcloud) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud) (11.2.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud) (3.10.0)\n",
            "Requirement already satisfied: prettytable>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from jupysql) (3.16.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.11/dist-packages (from jupysql) (2.0.39)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.11/dist-packages (from jupysql) (0.5.3)\n",
            "Requirement already satisfied: ipython-genutils>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from jupysql) (0.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from jupysql) (3.1.6)\n",
            "Requirement already satisfied: sqlglot>=11.3.7 in /usr/local/lib/python3.11/dist-packages (from jupysql) (25.20.2)\n",
            "Collecting jupysql-plugin>=0.4.2 (from jupysql)\n",
            "  Downloading jupysql_plugin-0.4.5-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting ploomber-core>=0.2.7 (from jupysql)\n",
            "  Downloading ploomber_core-0.2.26-py3-none-any.whl.metadata (527 bytes)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ploomber-core>=0.2.7->jupysql) (6.0.2)\n",
            "Collecting posthog (from ploomber-core>=0.2.7->jupysql)\n",
            "  Downloading posthog-4.0.1-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable>=3.12.0->jupysql) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->jupysql) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy->jupysql) (3.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy->jupysql) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from posthog->ploomber-core>=0.2.7->jupysql) (2.32.3)\n",
            "Collecting backoff>=1.10.0 (from posthog->ploomber-core>=0.2.7->jupysql)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog->ploomber-core>=0.2.7->jupysql) (1.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.7->jupysql) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.7->jupysql) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.7->jupysql) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.7->jupysql) (2025.4.26)\n",
            "Downloading jupysql-0.11.1-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupysql_plugin-0.4.5-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.8/192.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ploomber_core-0.2.26-py3-none-any.whl (22 kB)\n",
            "Downloading posthog-4.0.1-py2.py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: backoff, posthog, ploomber-core, jupysql-plugin, jupysql\n",
            "Successfully installed backoff-2.2.1 jupysql-0.11.1 jupysql-plugin-0.4.5 ploomber-core-0.2.26 posthog-4.0.1\n"
          ]
        }
      ],
      "source": [
        "# installing DLATK and necessary packages\n",
        "!git clone -b psych290 https://github.com/dlatk/dlatk.git\n",
        "!pip install -r dlatk/install/requirements.txt\n",
        "!pip install dlatk/\n",
        "!pip install wordcloud langid jupysql"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1b) Mount Google Drive and copy databases"
      ],
      "metadata": {
        "id": "ORz4GM0ArvYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive & copy to Colab\n",
        "\n",
        "# connects & mounts your Google Drive to this colab space\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# this copies dlatk_lexica.db from your Google Drive to Colab\n",
        "!cp -f \"/content/drive/MyDrive/sqlite_databases/dlatk_lexica.db\" \"sqlite_data\"\n",
        "\n",
        "# this copies {database}.db from your Google Drive to Colab\n",
        "!cp -f \"/content/drive/MyDrive/sqlite_databases/{database}.db\" \"sqlite_data\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7XWXktOrzWP",
        "outputId": "08993243-65ee-426f-aa2a-ff40ff23f655"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1c) Setup database connection"
      ],
      "metadata": {
        "id": "oCj9VppSr5Nz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loads the %%sql extension\n",
        "%load_ext sql\n",
        "\n",
        "# connects the extension to the database - mounts both databases as engines\n",
        "from sqlalchemy import create_engine\n",
        "tutorial_db_engine = create_engine(f\"sqlite:///sqlite_data/{database}.db?charset=utf8mb4\")\n",
        "dlatk_lexica_engine = create_engine(f\"sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4\")\n",
        "\n",
        "# attaches the dlatk_lexica.db so tutorial_db_engine can query both databases\n",
        "from IPython import get_ipython\n",
        "from sqlalchemy import event\n",
        "\n",
        "# auto‑attach the lexica db whenever tutorial_db_engine connects\n",
        "@event.listens_for(tutorial_db_engine, \"connect\")\n",
        "def _attach_lexica(dbapi_conn, connection_record):\n",
        "    dbapi_conn.execute(\"ATTACH DATABASE 'sqlite_data/dlatk_lexica.db' AS dlatk_lexica;\")\n",
        "\n",
        "%sql tutorial_db_engine\n",
        "\n",
        "#set the output limit to 50\n",
        "%config SqlMagic.displaylimit = 50"
      ],
      "metadata": {
        "id": "8q73fy-7r50h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1d) (ONLY If nedded: SOFT RELOAD): If you have a **\"database lock\"** problem"
      ],
      "metadata": {
        "id": "zEaWoSEh7gi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you face a \"database locked\" issue, restart the session (Runtime ==> Restart Session) & run this cell to get set back up!\n"
      ],
      "metadata": {
        "id": "mhYoe9Tu2x8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "database = \"tutorial_07\"\n",
        "\n",
        "%reload_ext sql\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "tutorial_db_engine = create_engine(f\"sqlite:///sqlite_data/{database}.db?charset=utf8mb4\")\n",
        "dlatk_lexica_engine = create_engine(f\"sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4\")\n",
        "\n",
        "# set the output limit to 50\n",
        "%config SqlMagic.displaylimit = 50\n",
        "\n",
        "from IPython import get_ipython\n",
        "from sqlalchemy import event\n",
        "\n",
        "# auto‑attach the lexica db whenever tutorial_db_engine connects\n",
        "@event.listens_for(tutorial_db_engine, \"connect\")\n",
        "def _attach_lexica(dbapi_conn, connection_record):\n",
        "    dbapi_conn.execute(\"ATTACH DATABASE 'sqlite_data/dlatk_lexica.db' AS dlatk_lexica;\")\n",
        "\n",
        "%sql tutorial_db_engine"
      ],
      "metadata": {
        "id": "fvWfMDXB7gEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qutxUqkwVXSS"
      },
      "source": [
        "### If needed, one-gram extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsxkjAVcVXSS"
      },
      "source": [
        "This tutorial needs 1-gram feature table and the one obtained using `LIWC2015`. *If you haven't already* (you can check with `%sqlcmd tables`), you can extract them using the below DLATK commands."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSNYhE11VXSS"
      },
      "outputs": [],
      "source": [
        "database = \"dla_tutorial\"\n",
        "msgs_table = \"msgs\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !dlatkInterface.py \\\n",
        "#     --corpdb {database} \\\n",
        "#     --corptable {msgs_table} \\\n",
        "#     --correl_field user_id \\\n",
        "#     --add_ngrams -n 1"
      ],
      "metadata": {
        "id": "9rxznCK_VXSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRCEDAwYVXSS"
      },
      "source": [
        "Similarly, we can produce the `LIWC2015` feature table as below.\n",
        "\n",
        "### If needed, LIWC feature extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUlyrxrvVXSS"
      },
      "outputs": [],
      "source": [
        "database = \"dla_tutorial\"\n",
        "msgs_table = \"msgs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y58wXaZBVXSS"
      },
      "outputs": [],
      "source": [
        "# !dlatkInterface.py \\\n",
        "#     --corpdb {database} \\\n",
        "#     --corptable {msgs_table} \\\n",
        "#     --correl_field user_id \\\n",
        "#     --add_lex_table -l LIWC2015"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzPxKX_tVXSS"
      },
      "source": [
        "The above command produces `feat$cat_LIWC2015$msgs$user_id$1gra` table with LIWC2015 dictionary features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1RrwpznJVXSS"
      },
      "source": [
        "## 2) One-gram ocurrence filtering\n",
        "\n",
        "Firstly, let's count how many unique features (types) people use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr-v9CmwVXSS"
      },
      "outputs": [],
      "source": [
        "feat_1gram_user = 'feat$1gram$msgs$user_id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "erDvUziZVXSS",
        "outputId": "108267cc-7538-4992-b526-caa79c2450a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+----------------------+\n",
              "| COUNT(DISTINCT feat) |\n",
              "+----------------------+\n",
              "|        137687        |\n",
              "+----------------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>COUNT(DISTINCT feat)</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>137687</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(DISTINCT feat)\n",
        "FROM {{feat_1gram_user}};"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyIkTA0FVXSS"
      },
      "source": [
        "That's almost 140,000 distinct tokens!\n",
        "\n",
        "Let's see how many of them were used less than 50 times (putting them in the the tail of the Zipf distribution).\n",
        "\n",
        "#### 🤓💻\n",
        "\n",
        "Observe the inner query, it's the same as creating the `word_counts` table from the previous homeworks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDoFByY7VXST"
      },
      "outputs": [],
      "source": [
        "feat_1gram_user = 'feat$1gram$msgs$user_id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWogyaQuVXST"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(*)\n",
        "FROM (\n",
        "  SELECT SUM(value) AS feat_count\n",
        "  FROM {{feat_1gram_user}}\n",
        "  GROUP BY feat) AS a\n",
        "WHERE feat_count <= 50;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3mXMhCcVXST"
      },
      "source": [
        "Gahh!! That's clearly a lot -- almost 130,000 tokens appear 50 times or less.\n",
        "\n",
        "FYI: this ran pretty fast because feat tables always come with indices on all columns -- makes it easy to sum over.\n",
        "\n",
        "Here is another way to look at this: which words were used by at leat 50 users?\n",
        "\n",
        "#### 🤓💻\n",
        "\n",
        "Before reading the answer below -- do you remember how to get distinct user counts for which something is true...?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thRRS2UVVXST"
      },
      "outputs": [],
      "source": [
        "feat_1gram_user = 'feat$1gram$msgs$user_id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFtlb3ckVXST"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(*)\n",
        "FROM (\n",
        "  SELECT count(distinct(group_id)) AS group_count\n",
        "  FROM {{feat_1gram_user}}\n",
        "  GROUP BY feat) AS a\n",
        "WHERE group_count <= 50;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mAC3ndTVXST"
      },
      "source": [
        "That's even more. So out of 137,687 types, 133,103 are used by 50 users or less, leaving `137,687 - 133,103 = 4,584` features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uKdLnx1VXST"
      },
      "source": [
        "We could go on like this with SQL, and shortlist our feature table down to words that occur across at least 50 groups, something like that.\n",
        "\n",
        "**But let's not do that.**\n",
        "\n",
        "Instead, let's just ask DLATK to do that for us, with the `--feat_occ_filter --set_p_occ 0.05` flags during feature extraction\n",
        "\n",
        "* `--feat_occ_filter` \"activates\" the feature occurrence filtering\n",
        "* `--set_p_occ 0.05` sets the feature occurrence threshold to 5% of the sample size. That means if we are extracting for 1,000 groups, we only want to retain features that have been used by **at least 50 users (groups)** (so it's not just using a minimal token count.)\n",
        "\n",
        "It seems a little complicated at first, but it's a nice way to set good thresholds for samples of different sizes, and ensure that you have enough groups (users) who have used that word to run correlations over (rule of thumb: at least 40 to 50).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2a) More nuanced explanation:\n",
        "\n",
        "Occurrence filtering depends a little bit on how many words are typically nested within a given group.\n",
        "\n",
        "(In the language space, power calculations are in 3D -- groups * num of features * words/group).\n",
        "\n",
        "Typical occurrence thresholds for different types of groups:\n",
        "\n",
        "* Occurrence thresholds between 1 and 10% are typical when we are grouping by **people** (such as with `--correl_field user_id`).\n",
        "\n",
        "* When we aggregate language to larger levels (such as **counties, cities, states**), we sometimes go as high as 30% of the groups (because we just have so much language from a county \\[50,000+ words+\\] -- and if a word doesn't appear among 50k words, it's probably pretty deep in the tail).\n",
        "\n",
        "* Conversely, if we extract language at the **message level** (say a single blog post), we set this threshold to be much lower -- if we have 30,000 blog posts, for a word to occur in 30, the occurrence threshold would be 1/1000, or 0.001\n",
        "\n",
        "**The key thing to remember here: you are setting the FRACTION of groups in which a feature should occur. That should be >40 groups. Based on the numbers of total groups you have, you can work out what this fraction should be.**\n",
        "\n",
        "Anyway, here we have ~1,000 users: let's extract features with the occurrence threshold of 5%, which will mean that a feature has to occure in at least `1,000 * 0.05 = 50` groups to be included. `--feat_occ_filter --set_p_occ 0.05`\n",
        "\n",
        "👆 Please make sure you understand this. ⚠️\n",
        "\n",
        "(BTW, as always in these dlatk commands, the order of the flags does not matter)."
      ],
      "metadata": {
        "id": "njqpbsw7vbHY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdR15DWEVXST"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cc-hB5-OVXST",
        "outputId": "0a01e020-ed9e-47a8-f249-8e15d2910374",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TopicExtractor: gensim Mallet wrapper unavailable, using Mallet directly.\n",
            "\n",
            "-----\n",
            "DLATK Interface Initiated: 2025-05-08 18:04:33\n",
            "-----\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "query: PRAGMA table_info(msgs)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$1gram$msgs$user_id\n",
            "SQL Query: CREATE TABLE feat$1gram$msgs$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(36), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$1gram$msgs$user_id, column:group_id \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX correl_field$1gram$msgs$user_id ON feat$1gram$msgs$user_id (group_id)\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$1gram$msgs$user_id, column:feat \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX feature$1gram$msgs$user_id ON feat$1gram$msgs$user_id (feat)\n",
            "query: PRAGMA table_info(msgs)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$meta_1gram$msgs$user_id\n",
            "SQL Query: CREATE TABLE feat$meta_1gram$msgs$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(16), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$meta_1gram$msgs$user_id, column:group_id \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$meta_1gram$msgs$user_id, column:feat \n",
            "\n",
            "\n",
            "finding messages for 1000 'user_id's\n",
            "WARNING: The table msgs does not have: a PRIMARY key on message_id. Consider adding.\n",
            "         Please check that all messages have a unique message_id, this can significantly impact all downstream analysis\n",
            "WARNING: The table msgs does not have: an index on user_id. Consider adding.\n",
            " [0%] Inserted 423 total ngram rows covering 1 user_ids\n",
            "Messages Read: 5k\n",
            " [5%] Inserted 168371 total ngram rows covering 50 user_ids\n",
            "Messages Read: 10k\n",
            " [10%] Inserted 312049 total ngram rows covering 100 user_ids\n",
            "Messages Read: 15k\n",
            " [15%] Inserted 441234 total ngram rows covering 150 user_ids\n",
            " [20%] Inserted 532745 total ngram rows covering 200 user_ids\n",
            " [25%] Inserted 582174 total ngram rows covering 250 user_ids\n",
            "Messages Read: 20k\n",
            " [30%] Inserted 642663 total ngram rows covering 300 user_ids\n",
            " [35%] Inserted 681322 total ngram rows covering 350 user_ids\n",
            " [40%] Inserted 743276 total ngram rows covering 400 user_ids\n",
            " [45%] Inserted 799722 total ngram rows covering 450 user_ids\n",
            " [50%] Inserted 845084 total ngram rows covering 500 user_ids\n",
            "Messages Read: 25k\n",
            " [55%] Inserted 891613 total ngram rows covering 550 user_ids\n",
            " [60%] Inserted 944308 total ngram rows covering 600 user_ids\n",
            " [65%] Inserted 989187 total ngram rows covering 650 user_ids\n",
            " [70%] Inserted 1044189 total ngram rows covering 700 user_ids\n",
            " [75%] Inserted 1088857 total ngram rows covering 750 user_ids\n",
            " [80%] Inserted 1136799 total ngram rows covering 800 user_ids\n",
            "Messages Read: 30k\n",
            " [85%] Inserted 1173547 total ngram rows covering 850 user_ids\n",
            " [90%] Inserted 1203121 total ngram rows covering 900 user_ids\n",
            " [95%] Inserted 1235410 total ngram rows covering 950 user_ids\n",
            " [100%] Inserted 1264232 total ngram rows covering 1000 user_ids\n",
            "Done Reading / Inserting.\n",
            "Adding Keys (if goes to keycache, then decrease MAX_TO_DISABLE_KEYS or run myisamchk -n).\n",
            "Done\n",
            "\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            " feat$1gram$msgs$user_id [threshold: 49]\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "SQL Query: DROP TABLE IF EXISTS feat$1gram$msgs$user_id$0_05\n",
            " feat$1gram$msgs$user_id <new table feat$1gram$msgs$user_id$0_05 will have 4751 distinct features.>\n",
            "SQL Query: CREATE TABLE feat$1gram$msgs$user_id$0_05 ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(36), value INTEGER, group_norm DOUBLE)\n",
            "0.1m feature instances written\n",
            "0.2m feature instances written\n",
            "0.3m feature instances written\n",
            "0.4m feature instances written\n",
            "0.5m feature instances written\n",
            "0.6m feature instances written\n",
            "0.7m feature instances written\n",
            "0.8m feature instances written\n",
            "Done inserting.\n",
            "Enabling keys.\n",
            "done.\n",
            "-------\n",
            "Settings:\n",
            "\n",
            "Database - dla_tutorial\n",
            "Corpus - msgs\n",
            "Group ID - user_id\n",
            "Feature table(s) - feat$1gram$msgs$user_id$0_05\n",
            "-------\n",
            "Interface Runtime: 139.17 seconds\n",
            "DLATK exits with success! A good day indeed  ¯\\_(ツ)_/¯.\n"
          ]
        }
      ],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --add_ngrams -n 1 \\\n",
        "    --feat_occ_filter --set_p_occ 0.05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXmuEMn9VXST"
      },
      "source": [
        "This took longer (\\~2.5 min) than just extracting the 1grams (\\~2.2min) -- as it created two tables: the unfiltered one `feat$1gram$msgs$user_id` and the filtered one `feat$1gram$msgs$user_id$0_05`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzoX_ei5VXST"
      },
      "source": [
        "The above command produced the table `feat$1gram$msgs$user_id$0_05`.\n",
        "\n",
        "<h2> ⚠️ PSA\n",
        "\n",
        "**The occurrence threshold is based on the number of included groups. DLATK will only consider groups that have a minimal token count: 500 is the default. So `--group_freq_thresh` will be used automatically and should be set explicitly (default  = 500).**\n",
        "\n",
        "This means that the determination of how many groups it uses for the 5\\% calculation will be based on those groups that meet the GFT threshold. In this case, it's 978 groups that contain 500 words or more. 5\\% of 978 = 48.9. Please scroll through the output and double check: DLATK retained all features that met this `[threshold: 49]` groups.\n",
        "\n",
        "**We should basically always set `--group_freq_thresh` explicitly, in every DLATK command. It never hurts.**\n",
        "\n",
        "FYI, `--group_freq_thresh 0` is what we use only if when we work with very short documents (such as Tweets) as groups (typically: `--correl_field message_id`).\n",
        "\n",
        "<h2> ⚠️ END of PSA\n",
        "\n",
        "Back to the main story here: Note the `0_05` at the end in the feat table name: `feat$1gram$msgs$user_id$0_05`.\n",
        "\n",
        "* **`0_05` <- THIS IS NEW -- this captures the fact that it was filtered down to those features only used by at least 5\\% of the sample** \\[which had word_count > group_freq_thresh `[default: 500]`\\]\n",
        "\n",
        "Let's review:\n",
        "\n",
        "* `feat` <- this is a feature table (the result of `--add_ngrams`)\n",
        "\n",
        "* `1gram` <- with 1grams in it (single tokens) (the result of `-n 1`)\n",
        "\n",
        "* `msgs` <- from the message table `msgs` (the result of `--corptable msgs`)\n",
        "\n",
        "* `user_id` <- extracted and aggregated for the unit of analysis identified by `user_id` (the result of `--correl_field user_id`)\n",
        "\n",
        "So the `0_05` keeps track of the last filtering step.\n",
        "\n",
        "Let's confirm the output, by seeing if there are any rare tokens left, and then seeing how many unique tokens have survived the filtering and are in the new feature table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE6penaJVXST"
      },
      "source": [
        "So how many features are left?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6N0H8c_aVXST"
      },
      "outputs": [],
      "source": [
        "feat_1gram_occ05_user = 'feat$1gram$msgs$user_id$0_05'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EzdAJ84fVXST",
        "outputId": "5a6b7b80-f75d-42d0-ea8b-a016b4b1b1d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+---------------+\n",
              "| unique_tokens |\n",
              "+---------------+\n",
              "|      4751     |\n",
              "+---------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>unique_tokens</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>4751</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(DISTINCT feat) AS unique_tokens\n",
        "FROM {{feat_1gram_occ05_user}};"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXQHNkvZVXST"
      },
      "source": [
        "~5,000 is a good number!\n",
        "\n",
        "Generally, between say 3,000 and 10,000 unique 1grams/tokens (=types) means we've done reasonable occurrence filtering.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJbCyjpLVXST"
      },
      "source": [
        "let's see what the rarest words are that we have retained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bP1dtaR3VXST"
      },
      "outputs": [],
      "source": [
        "feat_1gram_occ05_user = 'feat$1gram$msgs$user_id$0_05'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gawmhTM5VXST",
        "outputId": "bc36e7f2-6769-4ec8-c15b-0294f321ecd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+-------------+------------+-------------+\n",
              "|     feat    | feat_count | group_count |\n",
              "+-------------+------------+-------------+\n",
              "|    newest   |     54     |      49     |\n",
              "|  repeating  |     59     |      49     |\n",
              "|   spelled   |     59     |      49     |\n",
              "| entertained |     60     |      51     |\n",
              "|     fist    |     60     |      51     |\n",
              "|   phrases   |     60     |      52     |\n",
              "|   traveled  |     60     |      50     |\n",
              "|  inspiring  |     61     |      50     |\n",
              "|     bust    |     62     |      52     |\n",
              "|    cancel   |     62     |      50     |\n",
              "+-------------+------------+-------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>feat</th>\n",
              "            <th>feat_count</th>\n",
              "            <th>group_count</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>newest</td>\n",
              "            <td>54</td>\n",
              "            <td>49</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>repeating</td>\n",
              "            <td>59</td>\n",
              "            <td>49</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>spelled</td>\n",
              "            <td>59</td>\n",
              "            <td>49</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>entertained</td>\n",
              "            <td>60</td>\n",
              "            <td>51</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>fist</td>\n",
              "            <td>60</td>\n",
              "            <td>51</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>phrases</td>\n",
              "            <td>60</td>\n",
              "            <td>52</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>traveled</td>\n",
              "            <td>60</td>\n",
              "            <td>50</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>inspiring</td>\n",
              "            <td>61</td>\n",
              "            <td>50</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>bust</td>\n",
              "            <td>62</td>\n",
              "            <td>52</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>cancel</td>\n",
              "            <td>62</td>\n",
              "            <td>50</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT feat, SUM(value) AS feat_count, COUNT(DISTINCT(group_id)) AS group_count\n",
        "FROM {{feat_1gram_occ05_user}}\n",
        "GROUP BY feat\n",
        "ORDER by feat_count ASC\n",
        "LIMIT 10;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyjeryJlVXSW"
      },
      "source": [
        "Alrighty. `newest` was used 54 times by 49 people, and just made the cut -- on the bubble."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8VfUPTWVXSW"
      },
      "source": [
        "### 2b) Occurrence filtering using an existing table\n",
        "\n",
        "In the above DLATK command, we re-extracted the table from scratch using `--add_ngrams -n 1` and then filtered the occurrences of 1-grams.\n",
        "\n",
        "However, it's often desirable to take in the unfiltered version of the 1-gram feature table when we already have it, and then just filter it down further as needed, by feeding the `--feat_table` flag when we use `--feat_occ_filter`. It's much faster. Run the below command to see!!\n",
        "\n",
        "This is the most efficient way to tune your occurrence threshold to get 3,000-10,000 1grams:\n",
        "\n",
        "* **Step 1** - Extract the unfiltered feature table\n",
        "* **Step 2** - Then try filtering it down with different occurrence thresholds (as below)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_K9mxrokVXSW"
      },
      "source": [
        "For example, below we make the threshold more strict by changing it to 7\\% of the groups (978 * 7\\% = 68) -- fewer words will have been used by this higher number of groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGQVBjikVXSW"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "feat_1gram_user = 'feat$1gram$msgs$user_id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xja-34J0VXSW",
        "outputId": "7f5156ce-1756-450a-8255-75085ac6538f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TopicExtractor: gensim Mallet wrapper unavailable, using Mallet directly.\n",
            "\n",
            "-----\n",
            "DLATK Interface Initiated: 2025-05-08 18:07:46\n",
            "-----\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            " feat$1gram$msgs$user_id [threshold: 68]\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "SQL Query: DROP TABLE IF EXISTS feat$1gram$msgs$user_id$0_07\n",
            " feat$1gram$msgs$user_id <new table feat$1gram$msgs$user_id$0_07 will have 3514 distinct features.>\n",
            "SQL Query: CREATE TABLE feat$1gram$msgs$user_id$0_07 ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(36), value INTEGER, group_norm DOUBLE)\n",
            "0.1m feature instances written\n",
            "0.2m feature instances written\n",
            "0.3m feature instances written\n",
            "0.4m feature instances written\n",
            "0.5m feature instances written\n",
            "0.6m feature instances written\n",
            "0.7m feature instances written\n",
            "Done inserting.\n",
            "Enabling keys.\n",
            "done.\n",
            "-------\n",
            "Settings:\n",
            "\n",
            "Database - dla_tutorial\n",
            "Corpus - msgs\n",
            "Group ID - user_id\n",
            "Feature table(s) - feat$1gram$msgs$user_id$0_07\n",
            "-------\n",
            "Interface Runtime: 10.81 seconds\n",
            "DLATK exits with success! A good day indeed  ¯\\_(ツ)_/¯.\n"
          ]
        }
      ],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --correl_field user_id \\\n",
        "    --feat_table '{feat_1gram_user}' \\\n",
        "    --feat_occ_filter --set_p_occ 0.07"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEjrlpwPVXSX"
      },
      "source": [
        "After ~21 seconds, this results in a new table with 3,514 distinct features (see DLATK output: `new table ... will have 3514 distinct features`. Also note the group threshold resulting from the occurrence threshold: `[threshold: 68]`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGk0oKBGVXSX"
      },
      "source": [
        "#### 👩‍🔬💻 Exercise\n",
        "\n",
        "Can you get the number of types (distinct words) from the `feat$1gram$msgs$user_id$0_07` table?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ash7CrgXVXSa"
      },
      "source": [
        "**Answer**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1a2Qq6DeRwzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DZhJTitVXSa"
      },
      "source": [
        "## 3) Extracting 2-grams, filter on occurrence and PMI\n",
        "\n",
        "Now that we have explored 1-gram extraction quite a bit, let's go to the next step. Let's extract 2grams -- sequences of tokens of length two (such as \"happy, birthday\" and \"yay, !\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaA7sflZVXSa"
      },
      "source": [
        "It's the same command as in 1-gram extraction -- the only change being `--add_ngrams -n 2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6_b4MP7VXSa"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7jGULZJVXSa",
        "outputId": "60c1b75c-79d1-424a-9761-d3048932051f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TopicExtractor: gensim Mallet wrapper unavailable, using Mallet directly.\n",
            "\n",
            "-----\n",
            "DLATK Interface Initiated: 2025-05-06 21:27:26\n",
            "-----\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "query: PRAGMA table_info(msgs)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$2gram$msgs$user_id\n",
            "SQL Query: CREATE TABLE feat$2gram$msgs$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(70), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$2gram$msgs$user_id, column:group_id \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX correl_field$2gram$msgs$user_id ON feat$2gram$msgs$user_id (group_id)\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$2gram$msgs$user_id, column:feat \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX feature$2gram$msgs$user_id ON feat$2gram$msgs$user_id (feat)\n",
            "query: PRAGMA table_info(msgs)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$meta_2gram$msgs$user_id\n",
            "SQL Query: CREATE TABLE feat$meta_2gram$msgs$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(16), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$meta_2gram$msgs$user_id, column:group_id \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$meta_2gram$msgs$user_id, column:feat \n",
            "\n",
            "\n",
            "finding messages for 1000 'user_id's\n",
            "WARNING: The table msgs does not have: a PRIMARY key on message_id. Consider adding.\n",
            "         Please check that all messages have a unique message_id, this can significantly impact all downstream analysis\n",
            "WARNING: The table msgs does not have: an index on user_id. Consider adding.\n",
            " [0%] Inserted 920 total ngram rows covering 1 user_ids\n",
            "Messages Read: 5k\n",
            " [5%] Inserted 854774 total ngram rows covering 50 user_ids\n",
            "Messages Read: 10k\n",
            " [10%] Inserted 1499683 total ngram rows covering 100 user_ids\n",
            "Messages Read: 15k\n",
            " [15%] Inserted 2033874 total ngram rows covering 150 user_ids\n",
            " [20%] Inserted 2374295 total ngram rows covering 200 user_ids\n",
            " [25%] Inserted 2527305 total ngram rows covering 250 user_ids\n",
            "Messages Read: 20k\n",
            " [30%] Inserted 2727151 total ngram rows covering 300 user_ids\n",
            " [35%] Inserted 2845717 total ngram rows covering 350 user_ids\n",
            " [40%] Inserted 3058938 total ngram rows covering 400 user_ids\n",
            " [45%] Inserted 3242314 total ngram rows covering 450 user_ids\n",
            " [50%] Inserted 3380949 total ngram rows covering 500 user_ids\n",
            "Messages Read: 25k\n",
            " [55%] Inserted 3524058 total ngram rows covering 550 user_ids\n",
            " [60%] Inserted 3706277 total ngram rows covering 600 user_ids\n",
            " [65%] Inserted 3836747 total ngram rows covering 650 user_ids\n",
            " [70%] Inserted 4004094 total ngram rows covering 700 user_ids\n",
            " [75%] Inserted 4138590 total ngram rows covering 750 user_ids\n",
            " [80%] Inserted 4291379 total ngram rows covering 800 user_ids\n",
            "Messages Read: 30k\n",
            " [85%] Inserted 4399536 total ngram rows covering 850 user_ids\n",
            " [90%] Inserted 4475568 total ngram rows covering 900 user_ids\n",
            " [95%] Inserted 4562498 total ngram rows covering 950 user_ids\n",
            " [100%] Inserted 4634952 total ngram rows covering 1000 user_ids\n",
            "Done Reading / Inserting.\n",
            "Adding Keys (if goes to keycache, then decrease MAX_TO_DISABLE_KEYS or run myisamchk -n).\n",
            "Done\n",
            "\n",
            "-------\n",
            "Settings:\n",
            "\n",
            "Database - dla_tutorial\n",
            "Corpus - msgs\n",
            "Group ID - user_id\n",
            "Feature table(s) - feat$2gram$msgs$user_id\n",
            "-------\n",
            "Interface Runtime: 359.32 seconds\n",
            "DLATK exits with success! A good day indeed  ¯\\_(ツ)_/¯.\n"
          ]
        }
      ],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --add_ngrams -n 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnWYv8PzVXSa"
      },
      "source": [
        "This takes about 6 min. The above command produces `feat$2gram$msgs$user_id` table containing 2-grams.\n",
        "\n",
        "So, how many of them are there BTW? Let's check with SQL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-edRqF4VXSa"
      },
      "outputs": [],
      "source": [
        "feat_2gram_user = 'feat$2gram$msgs$user_id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hWNpmqJVXSa",
        "outputId": "ec87761b-2163-4aca-dfcc-759e80ba303e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+----------------------+\n",
              "| COUNT(DISTINCT feat) |\n",
              "+----------------------+\n",
              "|       1536988        |\n",
              "+----------------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>COUNT(DISTINCT feat)</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>1536988</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(DISTINCT feat)\n",
        "FROM {{feat_2gram_user}};"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_ckYi2fVXSa"
      },
      "source": [
        "Take a moment to count the digits -- that's 1.5 million. That's clearly an insane amount!!\n",
        "\n",
        "Let us filter it down with occurrence threshold of 0.05 (like we did with 1-grams above). Note that we are using the `--feat_table` trick here: we are ingesting the unfiltered table. That runs much faster (as it doesn't create the unfiltered table again)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZy3PRieVXSa"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "feat_2gram_user = 'feat$2gram$msgs$user_id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwsSNeLaVXSa",
        "outputId": "1de36297-3a83-43c3-f818-7e44ee282a8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TopicExtractor: gensim Mallet wrapper unavailable, using Mallet directly.\n",
            "\n",
            "-----\n",
            "DLATK Interface Initiated: 2025-05-06 21:33:50\n",
            "-----\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            " feat$2gram$msgs$user_id [threshold: 49]\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "    checked 1000000 features\n",
            "SQL Query: DROP TABLE IF EXISTS feat$2gram$msgs$user_id$0_05\n",
            " feat$2gram$msgs$user_id <new table feat$2gram$msgs$user_id$0_05 will have 11214 distinct features.>\n",
            "SQL Query: CREATE TABLE feat$2gram$msgs$user_id$0_05 ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(70), value INTEGER, group_norm DOUBLE)\n",
            "0.1m feature instances written\n",
            "0.2m feature instances written\n",
            "0.3m feature instances written\n",
            "0.4m feature instances written\n",
            "0.5m feature instances written\n",
            "0.6m feature instances written\n",
            "0.7m feature instances written\n",
            "0.8m feature instances written\n",
            "0.9m feature instances written\n",
            "1.0m feature instances written\n",
            "1.1m feature instances written\n",
            "1.2m feature instances written\n",
            "1.3m feature instances written\n",
            "Done inserting.\n",
            "Enabling keys.\n",
            "done.\n",
            "-------\n",
            "Settings:\n",
            "\n",
            "Database - dla_tutorial\n",
            "Corpus - msgs\n",
            "Group ID - user_id\n",
            "Feature table(s) - feat$2gram$msgs$user_id$0_05\n",
            "-------\n",
            "Interface Runtime: 26.65 seconds\n",
            "DLATK exits with success! A good day indeed  ¯\\_(ツ)_/¯.\n"
          ]
        }
      ],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --feat_table '{feat_2gram_user}' \\\n",
        "    --feat_occ_filter --set_p_occ 0.05"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Khj0daHCVXSb"
      },
      "source": [
        "Takes 20 seconds. The above command produces the table `feat$2gram$msgs$user_id$0_05`.\n",
        "\n",
        "How many feats are left then?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RbNmafCVXSb"
      },
      "outputs": [],
      "source": [
        "feat_2gram_occ05_user = 'feat$2gram$msgs$user_id$0_05'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3GN2G0WQVXSb",
        "outputId": "d7108da4-a91c-4dfc-b16e-6ddd42f60250",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+----------------------+\n",
              "| COUNT(DISTINCT feat) |\n",
              "+----------------------+\n",
              "|        11214         |\n",
              "+----------------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>COUNT(DISTINCT feat)</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>11214</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(DISTINCT feat)\n",
        "FROM {{feat_2gram_occ05_user}};"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpVhJ4C1VXSb"
      },
      "source": [
        "Looks like there are ~11k of those. Let's have a look at a random sample of 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRBbG2eFVXSb"
      },
      "outputs": [],
      "source": [
        "feat_2gram_occ05_user = 'feat$2gram$msgs$user_id$0_05'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dxFtROIVXSb",
        "outputId": "efbaf826-4fb2-4533-86c3-2ef11ee8e5ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+---------+----------+---------------+-------+------------------------+\n",
              "|    id   | group_id |      feat     | value |       group_norm       |\n",
              "+---------+----------+---------------+-------+------------------------+\n",
              "|  321397 | 2311511  |     to lay    |   1   | 4.044325810887325e-05  |\n",
              "| 1278070 | 4184069  |    . being    |   1   |  0.000333000333000333  |\n",
              "|  467553 | 3022585  |    will try   |   1   | 2.4646325232907772e-05 |\n",
              "|  192820 | 1694057  |     ones .    |   2   | 2.3073373327180433e-05 |\n",
              "|  945510 | 3745004  |  the ability  |   1   | 5.145885864251531e-05  |\n",
              "| 1046289 | 3860954  |    think i    |   1   | 0.0006779661016949153  |\n",
              "|  783875 | 3556477  |     that a    |   1   | 0.00010470107842110774 |\n",
              "|  52451  |  671748  |    to post    |   2   | 4.2003570303475795e-05 |\n",
              "|  510349 | 3178789  | remember what |   1   | 8.863676653075696e-05  |\n",
              "|  768442 | 3536867  |  decided that |   1   | 0.0007183908045977011  |\n",
              "+---------+----------+---------------+-------+------------------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>id</th>\n",
              "            <th>group_id</th>\n",
              "            <th>feat</th>\n",
              "            <th>value</th>\n",
              "            <th>group_norm</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>321397</td>\n",
              "            <td>2311511</td>\n",
              "            <td>to lay</td>\n",
              "            <td>1</td>\n",
              "            <td>4.044325810887325e-05</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1278070</td>\n",
              "            <td>4184069</td>\n",
              "            <td>. being</td>\n",
              "            <td>1</td>\n",
              "            <td>0.000333000333000333</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>467553</td>\n",
              "            <td>3022585</td>\n",
              "            <td>will try</td>\n",
              "            <td>1</td>\n",
              "            <td>2.4646325232907772e-05</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>192820</td>\n",
              "            <td>1694057</td>\n",
              "            <td>ones .</td>\n",
              "            <td>2</td>\n",
              "            <td>2.3073373327180433e-05</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>945510</td>\n",
              "            <td>3745004</td>\n",
              "            <td>the ability</td>\n",
              "            <td>1</td>\n",
              "            <td>5.145885864251531e-05</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1046289</td>\n",
              "            <td>3860954</td>\n",
              "            <td>think i</td>\n",
              "            <td>1</td>\n",
              "            <td>0.0006779661016949153</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>783875</td>\n",
              "            <td>3556477</td>\n",
              "            <td>that a</td>\n",
              "            <td>1</td>\n",
              "            <td>0.00010470107842110774</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>52451</td>\n",
              "            <td>671748</td>\n",
              "            <td>to post</td>\n",
              "            <td>2</td>\n",
              "            <td>4.2003570303475795e-05</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>510349</td>\n",
              "            <td>3178789</td>\n",
              "            <td>remember what</td>\n",
              "            <td>1</td>\n",
              "            <td>8.863676653075696e-05</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>768442</td>\n",
              "            <td>3536867</td>\n",
              "            <td>decided that</td>\n",
              "            <td>1</td>\n",
              "            <td>0.0007183908045977011</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT *\n",
        "FROM {{feat_2gram_occ05_user}}\n",
        "ORDER by RANDOM()\n",
        "LIMIT 10;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCGDcDtDVXSb"
      },
      "source": [
        "As you can observe, a lot of those 2 grams are uninformative -- often just random combinations of highly frequent words and punctuation. Let's drop them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-vlF8EEVXSb"
      },
      "source": [
        "### 3a) Using Pointwise-Mutual Information to filter two-grams\n",
        "\n",
        "[Pointwise mutual information](https://en.wikipedia.org/wiki/Pointwise_mutual_information) can be used to drop them.\n",
        "\n",
        "\\begin{equation}\n",
        "pmi(a,b) = log \\left( \\frac{p(a,b)}{p(a) p(b)} \\right)\n",
        "\\end{equation}\n",
        "\n",
        "We cover the details in the lecture but basically the idea is we want to see how much more likely the phrase (\"happy birthday\") is, for example, than the (independent) likelihoods of \"happy\" and \"birthday\" would suggest. We calculate a ratio of these, and then filter on it. The typical PMI thresholds (3 to 6) are based on empirical testing across many data sets (they are rules of thumb).\n",
        "\n",
        "So let's filter down the extracted 2-grams to only those that meet a PMI threshold of 3 with the `--feat_colloc_filter --set_pmi_threshold 3` flags. Same as occurrence filtering - the first flag \"activates\" the filter, the second sets it.\n",
        "\n",
        "Note that we are ingesting **the occurrence filtered** --feat_table here (`feat$2gram$msgs$user_id$0_05`) -- same idea as before, but now we just add the PMI filtering on top of the occurence filtering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4TGLf0LVXSb"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "feat_2gram_occ05_user = 'feat$2gram$msgs$user_id$0_05'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4seG2PiwVXSb",
        "outputId": "261d988e-d259-450a-8980-15df9686345f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TopicExtractor: gensim Mallet wrapper unavailable, using Mallet directly.\n",
            "\n",
            "-----\n",
            "DLATK Interface Initiated: 2025-05-06 21:35:37\n",
            "-----\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "feat$2gram$msgs$user_id$0_05\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "SQL Query: DROP TABLE IF EXISTS feat$2gram$msgs$user_id$0_05$pmi3_0\n",
            " feat$2gram$msgs$user_id$0_05 <new table feat$2gram$msgs$user_id$0_05$pmi3_0 will have 3517 distinct features.>\n",
            "SQL Query: CREATE TABLE feat$2gram$msgs$user_id$0_05$pmi3_0 ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(70), value INTEGER, group_norm DOUBLE)\n",
            "0.1m feature instances written\n",
            "0.2m feature instances written\n",
            "0.3m feature instances written\n",
            "0.4m feature instances written\n",
            "Done inserting.\n",
            "Enabling keys.\n",
            "done.\n",
            "-------\n",
            "Settings:\n",
            "\n",
            "Database - dla_tutorial\n",
            "Corpus - msgs\n",
            "Group ID - user_id\n",
            "Feature table(s) - feat$2gram$msgs$user_id$0_05$pmi3_0\n",
            "-------\n",
            "Interface Runtime: 12.45 seconds\n",
            "DLATK exits with success! A good day indeed  ¯\\_(ツ)_/¯.\n"
          ]
        }
      ],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --feat_table '{feat_2gram_occ05_user}' \\\n",
        "    --feat_colloc_filter --set_pmi_threshold 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPQ4zez7VXSb"
      },
      "source": [
        "The above command produces table `feat$2gram$msgs$user_id$0_05$pmi3_0`. Note the `pmi3_0` at the end.\n",
        "\n",
        "As you can see, the filtering steps are stacking up in the table name -- it keeps tracks of what happened to it. It's a simple blockchain, if you will. 💎👐\n",
        "\n",
        "Let's check how many features are left."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIWk6c0DVXSb"
      },
      "outputs": [],
      "source": [
        "feat_2gram_occ05_pmi3_user = 'feat$2gram$msgs$user_id$0_05$pmi3_0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5PHbiE8VXSb",
        "outputId": "c1f9d73f-4506-4d0d-e486-1307a676f890",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+----------------------+\n",
              "| COUNT(DISTINCT feat) |\n",
              "+----------------------+\n",
              "|         3517         |\n",
              "+----------------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>COUNT(DISTINCT feat)</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>3517</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(DISTINCT feat)\n",
        "FROM {{feat_2gram_occ05_pmi3_user}};"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12-7DEgpVXSb"
      },
      "source": [
        "~3500 of them. Let's look at some of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zyg_w_5-VXSb",
        "outputId": "b05be2c9-9ce9-4958-a9f4-76be8cb35700",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+--------+----------+--------------+-------+------------------------+\n",
              "|   id   | group_id |     feat     | value |       group_norm       |\n",
              "+--------+----------+--------------+-------+------------------------+\n",
              "| 258911 | 3565927  |   we went    |   2   | 0.00011866619200189867 |\n",
              "| 390225 | 4043736  |    if you    |   1   | 0.0006600660066006601  |\n",
              "| 172957 | 3252533  |  which was   |   1   | 4.671150971599402e-05  |\n",
              "| 72647  | 1826527  | i'm supposed |   5   | 5.260721350111527e-05  |\n",
              "| 106830 | 2366391  |   he makes   |   2   | 3.238813946332853e-05  |\n",
              "| 410137 | 4148541  |    a bit     |   1   | 0.00035335689045936394 |\n",
              "| 278633 | 3632734  |   i forgot   |   2   |  0.003780718336483932  |\n",
              "| 396474 | 4068475  |  don't know  |   1   |  0.001037344398340249  |\n",
              "| 116816 | 2607577  |   i'm not    |   22  | 0.00034844309290759924 |\n",
              "| 77894  | 1960271  |   who are    |   4   | 0.0001362629875660024  |\n",
              "+--------+----------+--------------+-------+------------------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>id</th>\n",
              "            <th>group_id</th>\n",
              "            <th>feat</th>\n",
              "            <th>value</th>\n",
              "            <th>group_norm</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>258911</td>\n",
              "            <td>3565927</td>\n",
              "            <td>we went</td>\n",
              "            <td>2</td>\n",
              "            <td>0.00011866619200189867</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>390225</td>\n",
              "            <td>4043736</td>\n",
              "            <td>if you</td>\n",
              "            <td>1</td>\n",
              "            <td>0.0006600660066006601</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>172957</td>\n",
              "            <td>3252533</td>\n",
              "            <td>which was</td>\n",
              "            <td>1</td>\n",
              "            <td>4.671150971599402e-05</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>72647</td>\n",
              "            <td>1826527</td>\n",
              "            <td>i'm supposed</td>\n",
              "            <td>5</td>\n",
              "            <td>5.260721350111527e-05</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>106830</td>\n",
              "            <td>2366391</td>\n",
              "            <td>he makes</td>\n",
              "            <td>2</td>\n",
              "            <td>3.238813946332853e-05</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>410137</td>\n",
              "            <td>4148541</td>\n",
              "            <td>a bit</td>\n",
              "            <td>1</td>\n",
              "            <td>0.00035335689045936394</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>278633</td>\n",
              "            <td>3632734</td>\n",
              "            <td>i forgot</td>\n",
              "            <td>2</td>\n",
              "            <td>0.003780718336483932</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>396474</td>\n",
              "            <td>4068475</td>\n",
              "            <td>don't know</td>\n",
              "            <td>1</td>\n",
              "            <td>0.001037344398340249</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>116816</td>\n",
              "            <td>2607577</td>\n",
              "            <td>i'm not</td>\n",
              "            <td>22</td>\n",
              "            <td>0.00034844309290759924</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>77894</td>\n",
              "            <td>1960271</td>\n",
              "            <td>who are</td>\n",
              "            <td>4</td>\n",
              "            <td>0.0001362629875660024</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT *\n",
        "FROM {{feat_2gram_occ05_pmi3_user}}\n",
        "ORDER by RANDOM()\n",
        "LIMIT 10;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-Dsi_jkVXSb"
      },
      "source": [
        "Better, but it still looks like we are getting phrases that aren't really that phrase-like. Let's filter with a stronger PMI threshold.\n",
        "\n",
        "Let's do another one with PMI = 7. Same trick! Runs through quickly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gqf4JZvVXSb"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "feat_2gram_occ05_user = 'feat$2gram$msgs$user_id$0_05'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hG5-wYJHVXSb",
        "outputId": "861c12d0-e82b-4d24-c03b-57ec059a4ca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TopicExtractor: gensim Mallet wrapper unavailable, using Mallet directly.\n",
            "\n",
            "-----\n",
            "DLATK Interface Initiated: 2025-05-06 21:36:15\n",
            "-----\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "feat$2gram$msgs$user_id$0_05\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "SQL Query: DROP TABLE IF EXISTS feat$2gram$msgs$user_id$0_05$pmi7_0\n",
            " feat$2gram$msgs$user_id$0_05 <new table feat$2gram$msgs$user_id$0_05$pmi7_0 will have 218 distinct features.>\n",
            "SQL Query: CREATE TABLE feat$2gram$msgs$user_id$0_05$pmi7_0 ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(70), value INTEGER, group_norm DOUBLE)\n",
            "Done inserting.\n",
            "Enabling keys.\n",
            "done.\n",
            "-------\n",
            "Settings:\n",
            "\n",
            "Database - dla_tutorial\n",
            "Corpus - msgs\n",
            "Group ID - user_id\n",
            "Feature table(s) - feat$2gram$msgs$user_id$0_05$pmi7_0\n",
            "-------\n",
            "Interface Runtime: 6.41 seconds\n",
            "DLATK exits with success! A good day indeed  ¯\\_(ツ)_/¯.\n"
          ]
        }
      ],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --feat_table '{feat_2gram_occ05_user}' \\\n",
        "    --feat_colloc_filter --set_pmi_threshold 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XefgXKtdVXSb"
      },
      "source": [
        "The above command produces table `feat$2gram$msgs$user_id$0_05$pmi7_0`.\n",
        "\n",
        "So, how many feats are left now?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2c79IiaVXSb"
      },
      "outputs": [],
      "source": [
        "feat_2gram_occ05_pmi7_user = 'feat$2gram$msgs$user_id$0_05$pmi7_0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eRhh4GSVXSb",
        "outputId": "a73dec83-fa6e-4b12-c750-b04011cfc5d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+----------------------+\n",
              "| COUNT(DISTINCT feat) |\n",
              "+----------------------+\n",
              "|         218          |\n",
              "+----------------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>COUNT(DISTINCT feat)</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>218</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(DISTINCT feat)\n",
        "FROM {{feat_2gram_occ05_pmi7_user}};"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxQv_vQ9VXSb"
      },
      "source": [
        "Ok, it's ~200, that's not many."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OSUPqp84VXSb",
        "outputId": "b3ff0e69-fd3c-467e-f243-56dfcb2c8afe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+-------+----------+--------------------+-------+------------------------+\n",
              "|   id  | group_id |        feat        | value |       group_norm       |\n",
              "+-------+----------+--------------------+-------+------------------------+\n",
              "| 15866 | 3665752  |     take care      |   1   |  7.31528895391368e-05  |\n",
              "|  572  |  664485  |     six months     |   9   | 3.4418273808840904e-05 |\n",
              "|  3302 | 1778579  |     cell phone     |   1   | 6.994474365251452e-05  |\n",
              "| 16061 | 3678942  |     last night     |   1   | 0.00025680534155110427 |\n",
              "| 14990 | 3594873  |     staring at     |   2   | 0.00011970313622216902 |\n",
              "|  4031 | 1841346  |    worry about     |   2   | 0.00022408963585434174 |\n",
              "|  5380 | 2259900  |     few weeks      |   1   | 0.0003901677721420211  |\n",
              "|  2206 | 1223561  | difference between |   2   | 0.00012205541315757354 |\n",
              "| 11730 | 3422339  |     10 minutes     |   3   | 0.00010277492291880781 |\n",
              "| 11023 | 3382977  |     reminds me     |   1   | 3.893171377404033e-05  |\n",
              "+-------+----------+--------------------+-------+------------------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>id</th>\n",
              "            <th>group_id</th>\n",
              "            <th>feat</th>\n",
              "            <th>value</th>\n",
              "            <th>group_norm</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>15866</td>\n",
              "            <td>3665752</td>\n",
              "            <td>take care</td>\n",
              "            <td>1</td>\n",
              "            <td>7.31528895391368e-05</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>572</td>\n",
              "            <td>664485</td>\n",
              "            <td>six months</td>\n",
              "            <td>9</td>\n",
              "            <td>3.4418273808840904e-05</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>3302</td>\n",
              "            <td>1778579</td>\n",
              "            <td>cell phone</td>\n",
              "            <td>1</td>\n",
              "            <td>6.994474365251452e-05</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>16061</td>\n",
              "            <td>3678942</td>\n",
              "            <td>last night</td>\n",
              "            <td>1</td>\n",
              "            <td>0.00025680534155110427</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>14990</td>\n",
              "            <td>3594873</td>\n",
              "            <td>staring at</td>\n",
              "            <td>2</td>\n",
              "            <td>0.00011970313622216902</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>4031</td>\n",
              "            <td>1841346</td>\n",
              "            <td>worry about</td>\n",
              "            <td>2</td>\n",
              "            <td>0.00022408963585434174</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>5380</td>\n",
              "            <td>2259900</td>\n",
              "            <td>few weeks</td>\n",
              "            <td>1</td>\n",
              "            <td>0.0003901677721420211</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2206</td>\n",
              "            <td>1223561</td>\n",
              "            <td>difference between</td>\n",
              "            <td>2</td>\n",
              "            <td>0.00012205541315757354</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>11730</td>\n",
              "            <td>3422339</td>\n",
              "            <td>10 minutes</td>\n",
              "            <td>3</td>\n",
              "            <td>0.00010277492291880781</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>11023</td>\n",
              "            <td>3382977</td>\n",
              "            <td>reminds me</td>\n",
              "            <td>1</td>\n",
              "            <td>3.893171377404033e-05</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT *\n",
        "FROM {{feat_2gram_occ05_pmi7_user}}\n",
        "ORDER BY RANDOM()\n",
        "LIMIT 10;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PxCGTdeVXSb"
      },
      "source": [
        "Ahh OK! These things look like standing phrases (hang out, pick up, figure out, woke up, far away, etc.) Those are much more likely than you expect by chance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4s1oxWXVXSb"
      },
      "source": [
        "### 3b) Occurence and PMI-filtering a feat table at the same time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "423iX8RnVXSb"
      },
      "source": [
        "We can extract 2 grams, apply occcurrence threshold and PMI, all in one command, either by ingesting a unfiltered 2-gram feature table (using `--feat_table`), or by extracting it from scratch (using `--add_ngrams -n 2`), see below -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YU7AEMJ9VXSb"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWu6z2IoVXSb",
        "outputId": "fd66e4a9-b6bc-493e-d08f-2b2b2a3a5c72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TopicExtractor: gensim Mallet wrapper unavailable, using Mallet directly.\n",
            "\n",
            "-----\n",
            "DLATK Interface Initiated: 2025-05-06 21:36:41\n",
            "-----\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "query: PRAGMA table_info(msgs)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$2gram$msgs$user_id\n",
            "SQL Query: CREATE TABLE feat$2gram$msgs$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(70), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$2gram$msgs$user_id, column:group_id \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX correl_field$2gram$msgs$user_id ON feat$2gram$msgs$user_id (group_id)\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$2gram$msgs$user_id, column:feat \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX feature$2gram$msgs$user_id ON feat$2gram$msgs$user_id (feat)\n",
            "query: PRAGMA table_info(msgs)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$meta_2gram$msgs$user_id\n",
            "SQL Query: CREATE TABLE feat$meta_2gram$msgs$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(16), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$meta_2gram$msgs$user_id, column:group_id \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$meta_2gram$msgs$user_id, column:feat \n",
            "\n",
            "\n",
            "finding messages for 1000 'user_id's\n",
            "WARNING: The table msgs does not have: a PRIMARY key on message_id. Consider adding.\n",
            "         Please check that all messages have a unique message_id, this can significantly impact all downstream analysis\n",
            "WARNING: The table msgs does not have: an index on user_id. Consider adding.\n",
            " [0%] Inserted 920 total ngram rows covering 1 user_ids\n",
            "Messages Read: 5k\n",
            " [5%] Inserted 854774 total ngram rows covering 50 user_ids\n",
            "Messages Read: 10k\n",
            " [10%] Inserted 1499683 total ngram rows covering 100 user_ids\n",
            "Messages Read: 15k\n",
            " [15%] Inserted 2033874 total ngram rows covering 150 user_ids\n",
            " [20%] Inserted 2374295 total ngram rows covering 200 user_ids\n",
            " [25%] Inserted 2527305 total ngram rows covering 250 user_ids\n",
            "Messages Read: 20k\n",
            " [30%] Inserted 2727151 total ngram rows covering 300 user_ids\n",
            " [35%] Inserted 2845717 total ngram rows covering 350 user_ids\n",
            " [40%] Inserted 3058938 total ngram rows covering 400 user_ids\n",
            " [45%] Inserted 3242314 total ngram rows covering 450 user_ids\n",
            " [50%] Inserted 3380949 total ngram rows covering 500 user_ids\n",
            "Messages Read: 25k\n",
            " [55%] Inserted 3524058 total ngram rows covering 550 user_ids\n",
            " [60%] Inserted 3706277 total ngram rows covering 600 user_ids\n",
            " [65%] Inserted 3836747 total ngram rows covering 650 user_ids\n",
            " [70%] Inserted 4004094 total ngram rows covering 700 user_ids\n",
            " [75%] Inserted 4138590 total ngram rows covering 750 user_ids\n",
            " [80%] Inserted 4291379 total ngram rows covering 800 user_ids\n",
            "Messages Read: 30k\n",
            " [85%] Inserted 4399536 total ngram rows covering 850 user_ids\n",
            " [90%] Inserted 4475568 total ngram rows covering 900 user_ids\n",
            " [95%] Inserted 4562498 total ngram rows covering 950 user_ids\n",
            " [100%] Inserted 4634952 total ngram rows covering 1000 user_ids\n",
            "Done Reading / Inserting.\n",
            "Adding Keys (if goes to keycache, then decrease MAX_TO_DISABLE_KEYS or run myisamchk -n).\n",
            "Done\n",
            "\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            " feat$2gram$msgs$user_id [threshold: 49]\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "    checked 1000000 features\n",
            "SQL Query: DROP TABLE IF EXISTS feat$2gram$msgs$user_id$0_05\n",
            " feat$2gram$msgs$user_id <new table feat$2gram$msgs$user_id$0_05 will have 11214 distinct features.>\n",
            "SQL Query: CREATE TABLE feat$2gram$msgs$user_id$0_05 ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(70), value INTEGER, group_norm DOUBLE)\n",
            "0.1m feature instances written\n",
            "0.2m feature instances written\n",
            "0.3m feature instances written\n",
            "0.4m feature instances written\n",
            "0.5m feature instances written\n",
            "0.6m feature instances written\n",
            "0.7m feature instances written\n",
            "0.8m feature instances written\n",
            "0.9m feature instances written\n",
            "1.0m feature instances written\n",
            "1.1m feature instances written\n",
            "1.2m feature instances written\n",
            "1.3m feature instances written\n",
            "Done inserting.\n",
            "Enabling keys.\n",
            "done.\n",
            "feat$2gram$msgs$user_id$0_05\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "SQL Query: DROP TABLE IF EXISTS feat$2gram$msgs$user_id$0_05$pmi3_0\n",
            " feat$2gram$msgs$user_id$0_05 <new table feat$2gram$msgs$user_id$0_05$pmi3_0 will have 3517 distinct features.>\n",
            "SQL Query: CREATE TABLE feat$2gram$msgs$user_id$0_05$pmi3_0 ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(70), value INTEGER, group_norm DOUBLE)\n",
            "0.1m feature instances written\n",
            "0.2m feature instances written\n",
            "0.3m feature instances written\n",
            "0.4m feature instances written\n",
            "Done inserting.\n",
            "Enabling keys.\n",
            "done.\n",
            "-------\n",
            "Settings:\n",
            "\n",
            "Database - dla_tutorial\n",
            "Corpus - msgs\n",
            "Group ID - user_id\n",
            "Feature table(s) - feat$2gram$msgs$user_id$0_05$pmi3_0\n",
            "-------\n",
            "Interface Runtime: 419.26 seconds\n",
            "DLATK exits with success! A good day indeed  ¯\\_(ツ)_/¯.\n"
          ]
        }
      ],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --add_ngrams -n 2 \\\n",
        "    --feat_occ_filter --set_p_occ 0.05 \\\n",
        "    --feat_colloc_filter --set_pmi_threshold 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thB3zNYoVXSb"
      },
      "source": [
        "This will take time, ~8 minutes. This produces the tables\n",
        "* `feat$2gram$msgs$user_id`\n",
        "* `feat$2gram$msgs$user_id$0_05`\n",
        "* `feat$2gram$msgs$user_id$0_05$pmi3_0`\n",
        "\n",
        "it's like a very nerdy and boring version of the hero's journey (2023 johannes comment: I don't even know that means!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_txe6KKVXSc"
      },
      "source": [
        "### 2c) 3gram table - extraction and filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M34FCRPVXSc"
      },
      "source": [
        "Let's create such a occurrence and PMI filtered table for 3-grams, all in one step using the above command. For this we will use `--feat_colloc_filter` `--set_pmi_threshold 3`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will take a while (~9min)!!"
      ],
      "metadata": {
        "id": "rRY3TNRilhh3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNxqhrbzVXSc"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9bL_XzD7VXSc",
        "outputId": "b52a4c97-f524-4c63-cb5f-d8020acc276d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TopicExtractor: gensim Mallet wrapper unavailable, using Mallet directly.\n",
            "\n",
            "-----\n",
            "DLATK Interface Initiated: 2025-05-06 21:50:06\n",
            "-----\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "query: PRAGMA table_info(msgs)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$3gram$msgs$user_id\n",
            "SQL Query: CREATE TABLE feat$3gram$msgs$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(102), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$3gram$msgs$user_id, column:group_id \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX correl_field$3gram$msgs$user_id ON feat$3gram$msgs$user_id (group_id)\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$3gram$msgs$user_id, column:feat \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX feature$3gram$msgs$user_id ON feat$3gram$msgs$user_id (feat)\n",
            "query: PRAGMA table_info(msgs)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$meta_3gram$msgs$user_id\n",
            "SQL Query: CREATE TABLE feat$meta_3gram$msgs$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(16), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$meta_3gram$msgs$user_id, column:group_id \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$meta_3gram$msgs$user_id, column:feat \n",
            "\n",
            "\n",
            "finding messages for 1000 'user_id's\n",
            "WARNING: The table msgs does not have: a PRIMARY key on message_id. Consider adding.\n",
            "         Please check that all messages have a unique message_id, this can significantly impact all downstream analysis\n",
            "WARNING: The table msgs does not have: an index on user_id. Consider adding.\n",
            " [0%] Inserted 997 total ngram rows covering 1 user_ids\n",
            "Messages Read: 5k\n",
            " [5%] Inserted 1464606 total ngram rows covering 50 user_ids\n",
            "Messages Read: 10k\n",
            " [10%] Inserted 2470650 total ngram rows covering 100 user_ids\n",
            "Messages Read: 15k\n",
            " [15%] Inserted 3264990 total ngram rows covering 150 user_ids\n",
            " [20%] Inserted 3737068 total ngram rows covering 200 user_ids\n",
            " [25%] Inserted 3934160 total ngram rows covering 250 user_ids\n",
            "Messages Read: 20k\n",
            " [30%] Inserted 4203027 total ngram rows covering 300 user_ids\n",
            " [35%] Inserted 4358658 total ngram rows covering 350 user_ids\n",
            " [40%] Inserted 4645087 total ngram rows covering 400 user_ids\n",
            " [45%] Inserted 4885257 total ngram rows covering 450 user_ids\n",
            " [50%] Inserted 5062145 total ngram rows covering 500 user_ids\n",
            "Messages Read: 25k\n",
            " [55%] Inserted 5245461 total ngram rows covering 550 user_ids\n",
            " [60%] Inserted 5495042 total ngram rows covering 600 user_ids\n",
            " [65%] Inserted 5657702 total ngram rows covering 650 user_ids\n",
            " [70%] Inserted 5871674 total ngram rows covering 700 user_ids\n",
            " [75%] Inserted 6043852 total ngram rows covering 750 user_ids\n",
            " [80%] Inserted 6242654 total ngram rows covering 800 user_ids\n",
            "Messages Read: 30k\n",
            " [85%] Inserted 6380390 total ngram rows covering 850 user_ids\n",
            " [90%] Inserted 6471265 total ngram rows covering 900 user_ids\n",
            " [95%] Inserted 6575900 total ngram rows covering 950 user_ids\n",
            " [100%] Inserted 6661533 total ngram rows covering 1000 user_ids\n",
            "Done Reading / Inserting.\n",
            "Adding Keys (if goes to keycache, then decrease MAX_TO_DISABLE_KEYS or run myisamchk -n).\n",
            "Done\n",
            "\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            " feat$3gram$msgs$user_id [threshold: 49]\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "    checked 1000000 features\n",
            "    checked 2000000 features\n",
            "    checked 3000000 features\n",
            "    checked 4000000 features\n",
            "SQL Query: DROP TABLE IF EXISTS feat$3gram$msgs$user_id$0_05\n",
            " feat$3gram$msgs$user_id <new table feat$3gram$msgs$user_id$0_05 will have 4801 distinct features.>\n",
            "SQL Query: CREATE TABLE feat$3gram$msgs$user_id$0_05 ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(102), value INTEGER, group_norm DOUBLE)\n",
            "0.1m feature instances written\n",
            "0.2m feature instances written\n",
            "0.3m feature instances written\n",
            "0.4m feature instances written\n",
            "Done inserting.\n",
            "Enabling keys.\n",
            "done.\n",
            "feat$3gram$msgs$user_id$0_05\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "SQL Query: DROP TABLE IF EXISTS feat$3gram$msgs$user_id$0_05$pmi3_0\n",
            " feat$3gram$msgs$user_id$0_05 <new table feat$3gram$msgs$user_id$0_05$pmi3_0 will have 2576 distinct features.>\n",
            "SQL Query: CREATE TABLE feat$3gram$msgs$user_id$0_05$pmi3_0 ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(102), value INTEGER, group_norm DOUBLE)\n",
            "0.1m feature instances written\n",
            "0.2m feature instances written\n",
            "Done inserting.\n",
            "Enabling keys.\n",
            "done.\n",
            "-------\n",
            "Settings:\n",
            "\n",
            "Database - dla_tutorial\n",
            "Corpus - msgs\n",
            "Group ID - user_id\n",
            "Feature table(s) - feat$3gram$msgs$user_id$0_05$pmi3_0\n",
            "-------\n",
            "Interface Runtime: 589.36 seconds\n",
            "DLATK exits with success! A good day indeed  ¯\\_(ツ)_/¯.\n"
          ]
        }
      ],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --add_ngrams -n 3 \\\n",
        "    --feat_occ_filter --set_p_occ 0.05 \\\n",
        "    --feat_colloc_filter --set_pmi_threshold 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56H848rCVXSc"
      },
      "source": [
        "This produces the following table: `feat$3gram$msgs$user_id$0_05$pmi3_0`\n",
        "\n",
        "How many features does it have in it though?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ue66mtKVXSc"
      },
      "outputs": [],
      "source": [
        "feat_3gram_occ05_pmi3_user = 'feat$3gram$msgs$user_id$0_05$pmi3_0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfPnlvEHVXSc",
        "outputId": "91c54d5a-9d12-4a7f-84a8-d7ac1aae61d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+----------------------+\n",
              "| COUNT(DISTINCT feat) |\n",
              "+----------------------+\n",
              "|         2576         |\n",
              "+----------------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>COUNT(DISTINCT feat)</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>2576</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(DISTINCT feat)\n",
        "FROM {{feat_3gram_occ05_pmi3_user}};"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsNRw0UlVXSc"
      },
      "source": [
        "Ok, ~2.5k, which is totally reasonable. Notice that those are **fewer** than there were 2-grams with the same occurrence filter and PMI settings. A given 3gram is a lot less frequent, which is the main source of filtering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj6-kheEVXSc"
      },
      "source": [
        "## 4) Extracting 1to3grams all together, into a combined table\n",
        "\n",
        "For our downstream correlations and analyses, we want to combine 1grams, 2grams and 3grams into one `1to3gram` table. We do this with the combination of the following two flags: `--add_ngrams -n 1 2 3` and `--combine_feat_tables 1to3gram`.\n",
        "\n",
        "We can then combine this extraction and combination with the same occurrence and PMI filtering. So in one command -\n",
        "\n",
        "```\n",
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --add_ngrams -n 1 2 3 \\\n",
        "    --combine_feat_tables 1to3gram \\\n",
        "    --feat_occ_filter --set_p_occ 0.05 \\\n",
        "    --feat_colloc_filter --set_pmi_threshold 3\n",
        "```\n",
        "\n",
        "The above command produces the following tables:\n",
        "- `feat$1gram$msgs$user_id`\n",
        "- `feat$2gram$msgs$user_id`\n",
        "- `feat$3gram$msgs$user_id`\n",
        "- `feat$1to3gram$msgs$user_id` -- **combined 1 to 3 grams**\n",
        "- `feat$1to3gram$msgs$user_id$0_05` -- combined 1 to3 grams, **filtered to occurrence threshold 0.05**\n",
        "- `feat$1to3gram$msgs$user_id$0_05$pmi3_0` -- combined 1 to3 grams, occurrence threshold 0.05, **plus PMI > 3**\n",
        "\n",
        "Plus 1gram, 2gram and 3gram `meta_` tables, so **9 tables** in total.\n",
        "\n",
        "**Heads up, this will take roughly 20mins!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYsTQMc5VXSc"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5Gix1lTCVXSc",
        "outputId": "2b2ce9f9-4a57-4e3a-e864-fac7d2b35d51",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "TopicExtractor: gensim Mallet wrapper unavailable, using Mallet directly.\n",
            "\n",
            "-----\n",
            "DLATK Interface Initiated: 2025-05-08 18:33:49\n",
            "-----\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "query: PRAGMA table_info(msgs)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$1gram$msgs$user_id\n",
            "SQL Query: CREATE TABLE feat$1gram$msgs$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(36), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$1gram$msgs$user_id, column:group_id \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX correl_field$1gram$msgs$user_id ON feat$1gram$msgs$user_id (group_id)\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$1gram$msgs$user_id, column:feat \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX feature$1gram$msgs$user_id ON feat$1gram$msgs$user_id (feat)\n",
            "query: PRAGMA table_info(msgs)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$meta_1gram$msgs$user_id\n",
            "SQL Query: CREATE TABLE feat$meta_1gram$msgs$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(16), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$meta_1gram$msgs$user_id, column:group_id \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$meta_1gram$msgs$user_id, column:feat \n",
            "\n",
            "\n",
            "finding messages for 1000 'user_id's\n",
            "WARNING: The table msgs does not have: a PRIMARY key on message_id. Consider adding.\n",
            "         Please check that all messages have a unique message_id, this can significantly impact all downstream analysis\n",
            "WARNING: The table msgs does not have: an index on user_id. Consider adding.\n",
            " [0%] Inserted 423 total ngram rows covering 1 user_ids\n",
            "Messages Read: 5k\n",
            " [5%] Inserted 168371 total ngram rows covering 50 user_ids\n",
            "Messages Read: 10k\n",
            " [10%] Inserted 312049 total ngram rows covering 100 user_ids\n",
            "Messages Read: 15k\n",
            " [15%] Inserted 441234 total ngram rows covering 150 user_ids\n",
            " [20%] Inserted 532745 total ngram rows covering 200 user_ids\n",
            " [25%] Inserted 582174 total ngram rows covering 250 user_ids\n",
            "Messages Read: 20k\n",
            " [30%] Inserted 642663 total ngram rows covering 300 user_ids\n",
            " [35%] Inserted 681322 total ngram rows covering 350 user_ids\n",
            " [40%] Inserted 743276 total ngram rows covering 400 user_ids\n",
            " [45%] Inserted 799722 total ngram rows covering 450 user_ids\n",
            " [50%] Inserted 845084 total ngram rows covering 500 user_ids\n",
            "Messages Read: 25k\n",
            " [55%] Inserted 891613 total ngram rows covering 550 user_ids\n",
            " [60%] Inserted 944308 total ngram rows covering 600 user_ids\n",
            " [65%] Inserted 989187 total ngram rows covering 650 user_ids\n",
            " [70%] Inserted 1044189 total ngram rows covering 700 user_ids\n",
            " [75%] Inserted 1088857 total ngram rows covering 750 user_ids\n",
            " [80%] Inserted 1136799 total ngram rows covering 800 user_ids\n",
            "Messages Read: 30k\n",
            " [85%] Inserted 1173547 total ngram rows covering 850 user_ids\n",
            " [90%] Inserted 1203121 total ngram rows covering 900 user_ids\n",
            " [95%] Inserted 1235410 total ngram rows covering 950 user_ids\n",
            " [100%] Inserted 1264232 total ngram rows covering 1000 user_ids\n",
            "Done Reading / Inserting.\n",
            "Adding Keys (if goes to keycache, then decrease MAX_TO_DISABLE_KEYS or run myisamchk -n).\n",
            "Done\n",
            "\n",
            "query: PRAGMA table_info(msgs)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$2gram$msgs$user_id\n",
            "SQL Query: CREATE TABLE feat$2gram$msgs$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(70), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$2gram$msgs$user_id, column:group_id \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX correl_field$2gram$msgs$user_id ON feat$2gram$msgs$user_id (group_id)\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$2gram$msgs$user_id, column:feat \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX feature$2gram$msgs$user_id ON feat$2gram$msgs$user_id (feat)\n",
            "query: PRAGMA table_info(msgs)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$meta_2gram$msgs$user_id\n",
            "SQL Query: CREATE TABLE feat$meta_2gram$msgs$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(16), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$meta_2gram$msgs$user_id, column:group_id \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$meta_2gram$msgs$user_id, column:feat \n",
            "\n",
            "\n",
            "finding messages for 1000 'user_id's\n",
            " [0%] Inserted 920 total ngram rows covering 1 user_ids\n",
            "Messages Read: 5k\n",
            " [5%] Inserted 854774 total ngram rows covering 50 user_ids\n",
            "Messages Read: 10k\n",
            " [10%] Inserted 1499683 total ngram rows covering 100 user_ids\n",
            "Messages Read: 15k\n",
            " [15%] Inserted 2033874 total ngram rows covering 150 user_ids\n",
            " [20%] Inserted 2374295 total ngram rows covering 200 user_ids\n",
            " [25%] Inserted 2527305 total ngram rows covering 250 user_ids\n",
            "Messages Read: 20k\n",
            " [30%] Inserted 2727151 total ngram rows covering 300 user_ids\n",
            " [35%] Inserted 2845717 total ngram rows covering 350 user_ids\n",
            " [40%] Inserted 3058938 total ngram rows covering 400 user_ids\n",
            " [45%] Inserted 3242314 total ngram rows covering 450 user_ids\n",
            " [50%] Inserted 3380949 total ngram rows covering 500 user_ids\n",
            "Messages Read: 25k\n",
            " [55%] Inserted 3524058 total ngram rows covering 550 user_ids\n",
            " [60%] Inserted 3706277 total ngram rows covering 600 user_ids\n",
            " [65%] Inserted 3836747 total ngram rows covering 650 user_ids\n",
            " [70%] Inserted 4004094 total ngram rows covering 700 user_ids\n",
            " [75%] Inserted 4138590 total ngram rows covering 750 user_ids\n",
            " [80%] Inserted 4291379 total ngram rows covering 800 user_ids\n",
            "Messages Read: 30k\n",
            " [85%] Inserted 4399536 total ngram rows covering 850 user_ids\n",
            " [90%] Inserted 4475568 total ngram rows covering 900 user_ids\n",
            " [95%] Inserted 4562498 total ngram rows covering 950 user_ids\n",
            " [100%] Inserted 4634952 total ngram rows covering 1000 user_ids\n",
            "Done Reading / Inserting.\n",
            "Adding Keys (if goes to keycache, then decrease MAX_TO_DISABLE_KEYS or run myisamchk -n).\n",
            "Done\n",
            "\n",
            "query: PRAGMA table_info(msgs)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$3gram$msgs$user_id\n",
            "SQL Query: CREATE TABLE feat$3gram$msgs$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(102), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$3gram$msgs$user_id, column:group_id \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX correl_field$3gram$msgs$user_id ON feat$3gram$msgs$user_id (group_id)\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$3gram$msgs$user_id, column:feat \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX feature$3gram$msgs$user_id ON feat$3gram$msgs$user_id (feat)\n",
            "query: PRAGMA table_info(msgs)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$meta_3gram$msgs$user_id\n",
            "SQL Query: CREATE TABLE feat$meta_3gram$msgs$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(16), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$meta_3gram$msgs$user_id, column:group_id \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$meta_3gram$msgs$user_id, column:feat \n",
            "\n",
            "\n",
            "finding messages for 1000 'user_id's\n",
            " [0%] Inserted 997 total ngram rows covering 1 user_ids\n",
            "Messages Read: 5k\n",
            " [5%] Inserted 1464606 total ngram rows covering 50 user_ids\n",
            "Messages Read: 10k\n",
            " [10%] Inserted 2470650 total ngram rows covering 100 user_ids\n",
            "Messages Read: 15k\n",
            " [15%] Inserted 3264990 total ngram rows covering 150 user_ids\n",
            " [20%] Inserted 3737068 total ngram rows covering 200 user_ids\n",
            " [25%] Inserted 3934160 total ngram rows covering 250 user_ids\n",
            "Messages Read: 20k\n",
            " [30%] Inserted 4203027 total ngram rows covering 300 user_ids\n",
            " [35%] Inserted 4358658 total ngram rows covering 350 user_ids\n",
            " [40%] Inserted 4645087 total ngram rows covering 400 user_ids\n",
            " [45%] Inserted 4885257 total ngram rows covering 450 user_ids\n",
            " [50%] Inserted 5062145 total ngram rows covering 500 user_ids\n",
            "Messages Read: 25k\n",
            " [55%] Inserted 5245461 total ngram rows covering 550 user_ids\n",
            " [60%] Inserted 5495042 total ngram rows covering 600 user_ids\n",
            " [65%] Inserted 5657702 total ngram rows covering 650 user_ids\n",
            " [70%] Inserted 5871674 total ngram rows covering 700 user_ids\n",
            " [75%] Inserted 6043852 total ngram rows covering 750 user_ids\n",
            " [80%] Inserted 6242654 total ngram rows covering 800 user_ids\n",
            "Messages Read: 30k\n",
            " [85%] Inserted 6380390 total ngram rows covering 850 user_ids\n",
            " [90%] Inserted 6471265 total ngram rows covering 900 user_ids\n",
            " [95%] Inserted 6575900 total ngram rows covering 950 user_ids\n",
            " [100%] Inserted 6661533 total ngram rows covering 1000 user_ids\n",
            "Done Reading / Inserting.\n",
            "Adding Keys (if goes to keycache, then decrease MAX_TO_DISABLE_KEYS or run myisamchk -n).\n",
            "Done\n",
            "\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "SQL Query: DROP TABLE IF EXISTS feat$1to3gram$msgs$user_id\n",
            "SQL Query: CREATE TABLE feat$1to3gram$msgs$user_id ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(102), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$1to3gram$msgs$user_id, column:group_id \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX correl_field$1to3gram$msgs$user_id ON feat$1to3gram$msgs$user_id (group_id)\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$1to3gram$msgs$user_id, column:feat \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX feature$1to3gram$msgs$user_id ON feat$1to3gram$msgs$user_id (feat)\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            " feat$1to3gram$msgs$user_id [threshold: 49]\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "    checked 1000000 features\n",
            "    checked 2000000 features\n",
            "    checked 3000000 features\n",
            "    checked 4000000 features\n",
            "    checked 5000000 features\n",
            "SQL Query: DROP TABLE IF EXISTS feat$1to3gram$msgs$user_id$0_05\n",
            " feat$1to3gram$msgs$user_id <new table feat$1to3gram$msgs$user_id$0_05 will have 20766 distinct features.>\n",
            "SQL Query: CREATE TABLE feat$1to3gram$msgs$user_id$0_05 ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(102), value INTEGER, group_norm DOUBLE)\n",
            "0.1m feature instances written\n",
            "0.2m feature instances written\n",
            "0.3m feature instances written\n",
            "0.4m feature instances written\n",
            "0.5m feature instances written\n",
            "0.6m feature instances written\n",
            "0.7m feature instances written\n",
            "0.8m feature instances written\n",
            "0.9m feature instances written\n",
            "1.0m feature instances written\n",
            "1.1m feature instances written\n",
            "1.2m feature instances written\n",
            "1.3m feature instances written\n",
            "1.4m feature instances written\n",
            "1.5m feature instances written\n",
            "1.6m feature instances written\n",
            "1.7m feature instances written\n",
            "1.8m feature instances written\n",
            "1.9m feature instances written\n",
            "2.0m feature instances written\n",
            "2.1m feature instances written\n",
            "2.2m feature instances written\n",
            "2.3m feature instances written\n",
            "2.4m feature instances written\n",
            "2.5m feature instances written\n",
            "Done inserting.\n",
            "Enabling keys.\n",
            "done.\n",
            "feat$1to3gram$msgs$user_id$0_05\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "SQL Query: DROP TABLE IF EXISTS feat$1to3gram$msgs$user_id$0_05$pmi3_0\n",
            " feat$1to3gram$msgs$user_id$0_05 <new table feat$1to3gram$msgs$user_id$0_05$pmi3_0 will have 10844 distinct features.>\n",
            "SQL Query: CREATE TABLE feat$1to3gram$msgs$user_id$0_05$pmi3_0 ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(102), value INTEGER, group_norm DOUBLE)\n",
            "0.1m feature instances written\n",
            "0.2m feature instances written\n",
            "0.3m feature instances written\n",
            "0.4m feature instances written\n",
            "0.5m feature instances written\n",
            "0.6m feature instances written\n",
            "0.7m feature instances written\n",
            "0.8m feature instances written\n",
            "0.9m feature instances written\n",
            "1.0m feature instances written\n",
            "1.1m feature instances written\n",
            "1.2m feature instances written\n",
            "1.3m feature instances written\n",
            "1.4m feature instances written\n",
            "Done inserting.\n",
            "Enabling keys.\n",
            "done.\n",
            "-------\n",
            "Settings:\n",
            "\n",
            "Database - dla_tutorial\n",
            "Corpus - msgs\n",
            "Group ID - user_id\n",
            "Feature table(s) - feat$1to3gram$msgs$user_id$0_05$pmi3_0\n",
            "-------\n",
            "Interface Runtime: 1418.29 seconds\n",
            "DLATK exits with success! A good day indeed  ¯\\_(ツ)_/¯.\n"
          ]
        }
      ],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --add_ngrams -n 1 2 3 \\\n",
        "    --combine_feat_tables 1to3gram \\\n",
        "    --feat_occ_filter --set_p_occ 0.05 \\\n",
        "    --feat_colloc_filter --set_pmi_threshold 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtobntSVVXSc"
      },
      "source": [
        "Alright!! How many features are in the final filtered combo table?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmBt6kGOVXSc"
      },
      "outputs": [],
      "source": [
        "feat_1to3gram_occ05_pmi3_user = 'feat$1to3gram$msgs$user_id$0_05$pmi3_0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DJU2qswbVXSc",
        "outputId": "1d03eebd-8eda-40b5-d5a1-d95e3d4fc782",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/shashanka.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/shashanka.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+----------------------+\n",
              "| COUNT(DISTINCT feat) |\n",
              "+----------------------+\n",
              "|        10844         |\n",
              "+----------------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>COUNT(DISTINCT feat)</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>10844</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(DISTINCT feat)\n",
        "FROM {{feat_1to3gram_occ05_pmi3_user}};"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARinaGP0VXSc"
      },
      "source": [
        "That's about 10,000 features. That's good. Less than our 10,000 1grams + 10,000 2to3 grams = 20,000 1to3grams max cap for user level analyses. We can work with this for correlations now.\n",
        "\n",
        "Yay!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uycKd-5GVXSc"
      },
      "source": [
        "## 👍 **Rules of thumb for thresholds**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1 -  set occurrence threshold\n",
        "\n",
        "Set occurrence threshold so that you  have 3k – 10k 1grams.\n",
        "\n",
        "Good range for **person-level type data**: `--set_p_occ` 0.01 to 0.1, depending on sample size. The larger the sample (e.g., 10k+), the lower (e.g., 0_01) you can set it.\n",
        "\n",
        "Remember: You can always multiply \\[sample size > GFT threshold\\] times threshold to get the threshold number of groups you are picking (48, in the example above) -- and ask yourself: is that a good number for what I'm trying to do? 🤔\n",
        "\n",
        "E.g., 10,000 groups of which 9,000 have 500 words means I can choose a occ thresh of 0.01, that's 90 groups. Enough to run correlations over if a feature exists!\n",
        "\n",
        "#### Good Default -\n",
        "`--feat_occ_filter --set_p_occ 0.05`\n"
      ],
      "metadata": {
        "id": "SS9UzAFOww9E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 -  set PMI, using the occurrence threshold from step 1\n",
        "\n",
        "You want to set the PMI such that you have <10k additional 2-3 grams.  \n",
        "\n",
        "#### Good defaults -\n",
        "\n",
        "For **person-level type data** are PMIs 3 to 6 using `--feat_colloc_filter --set_pmi_threshold 3`"
      ],
      "metadata": {
        "id": "bWTn07m7wyY9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mevE5H9QVXSc"
      },
      "source": [
        "## 5) Making one-gram correlation (\"differential\") word clouds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, in the following we want to individually correlate the user-level frequencies of our 10,000 1to3grams against outcomes like age and gender, one at a time -- and then p-correct our signficance thresholds (done implicitly by default with Benjamini-Hochberg).\n",
        "\n",
        "\n",
        "* We then take the 70 most correlated 1to3grams, and ask DLATK to put them into a **wordcloud in descending order of the magnitude of the correlation coefficient with a given outcome**. We do this **separately for positively and for negatively correlated language features** for a given outcome. So we will get, for example, all the words most positively associated with age, and the words most negatively associated with age -  two word clouds per outcome.\n",
        "\n",
        "\n",
        "* So the main work here will be done by `--correlate` and `--tagcloud --make_wordclouds` into an output folder (which we set here with `--output_name {OUTPUT_FOLDER}/{OUTPUT_NAME}`.\n",
        "\n",
        "\n",
        "* Importantly, the **size of the words** in the wordcloud corresponds to the size of the correlation coefficients. The **coefficient range in the cloud is reported in the filename of the word cloud image files.** These are \"differential word clouds,\" and the method is called \"differential language analysis\" (as in, using correlation cofficients as the primary dimension for visualization-shortlisting).\n",
        "\n",
        "\n",
        "* The **color indexes the relative frequency,** from red (most frequent) to blue (moderate) to grey (rarely used).\n",
        "\n",
        "\n",
        "* Note: these word clouds differ from your \"standard\" lame-o word clouds where size gives frequency. These other kinds of visualizations aren't great because #languageFrequencyStatistics are generally pretty uninformative on their own, many blog posts in need of an image nonwithstanding. You end up just plotting whichever function words you didn't throw out.\n",
        "\n",
        "\n",
        "* Note that if you include the `--rmatrix --csv` flags, you get the usual output files that give you correlation coefficients for every word in an html and a csv file, respectively. With 10,000 language features, these files can get pretty big, but whatever. You could ingest them into R if you wanted, and create other visualizations or use table technology to show correlations, etc.\n",
        "\n",
        "\n",
        "* Here is the legend for these plots, a good thing to have.\n",
        "\n",
        "**A figure template powerpoint with these legends is also linked from the class website, and in the Class GitHub.**"
      ],
      "metadata": {
        "id": "MVrvVVLvxFCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-09/fig1.png\" width=\"300\">"
      ],
      "metadata": {
        "id": "-MKSBUada9jr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnocrIfZVXSc"
      },
      "source": [
        "[Link to PPTX!](https://github.com/CompPsychology/psych290_colab_public/blob/main/notebooks/FIGURE%20TEMPLATE%20-%201to3grams%20and%20topics.pptx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ihPG7mhVXSc"
      },
      "source": [
        "Let's start by just looking at the 1grams most correlated with age and gender. Let's use the occurrence-filtered 1gram table. That's always the most sensical thing to do :: any extra features you correlate suppress your statistical power, as they are taken into account by the Benjamini-Hochberg correction for multiple comparisons.\n",
        "\n",
        "Btw, let's also pipe `> [name_you_want].txt 2>&1` the output away into a text file, so we don't have to hear DLATK tell us about every other word it couldn't find enough examples of, which were significant, etc. 🙄"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNW3JRrpVXSc"
      },
      "outputs": [],
      "source": [
        "#new\n",
        "OUTPUT_NAME = '1grams_age_gender'\n",
        "feat_1gram_occ05_user = 'feat$1gram$msgs$user_id$0_05'\n",
        "\n",
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "outcomes_table = 'outcomes'\n",
        "\n",
        "OUTPUT_FOLDER = 'output_tutorial_9'\n",
        "!rm -rf {OUTPUT_FOLDER}/{OUTPUT_NAME}* # this deletes the output if exists\n",
        "!mkdir -p {OUTPUT_FOLDER} # and this makes the folder!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ng559VzuVXSc"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --correlate --rmatrix --csv --sort \\\n",
        "    --feat_table '{feat_1gram_occ05_user}' \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --outcomes age gender \\\n",
        "    --tagcloud --make_wordclouds \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME} > {OUTPUT_FOLDER}/logs.txt 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3w5YSK-VXSc"
      },
      "source": [
        "The above command has written files to the `output_tutorial_9/` folder with the prefix `1grams_age_gender`. You can check this out in the Files pane in the tab to the left and view the word clouds in Colab by simply clicking them 😀\n",
        "\n",
        "The word clouds are in:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR_8r2EyVXSc"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-09/file_tree_t9.png\" width=\"300\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42nOT8fCVXSc"
      },
      "source": [
        "Have a look at them in Colab.\n",
        "\n",
        "We have already seen the html and csv files when we ran LIWC correlations -- these look the same, just with 1grams  rows, rather than LIWC dictionaries.\n",
        "\n",
        "**Reminder:** You can always download the results to your local machine.\n",
        "\n",
        "Let's look at the word clouds that DLATK put in that subfolder. The position of the words is random between runs.\n",
        "\n",
        "(BTW, if you are wondering how to insert figures into text cells, there's an insert image option on the top of the cell (for example, this cell). We arranged the images using Powerpoint, and took a screenshot)\n",
        "\n",
        "**Gender:**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-09/fig3_gender.png\" width=\"900\">"
      ],
      "metadata": {
        "id": "9Y-Q3O1gWVn9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvddTGxXVXSd"
      },
      "source": [
        "that looks vaguely like what we've seen before in the Eichstaedt et al., 2021. Males orient towards things (the, of), females towards relational and emotional terms. Clouds amplify even small differences: they can amplify stereotypes.\n",
        "\n",
        "**Age:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1BYiVjoVXSc"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-09/fig4_age.png\" width=\"900\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROXaSzdbVXSd"
      },
      "source": [
        "Note that we should probably control for gender (`--controls gender`) when we run age language correlations, and vice versa."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtkaBFZAVXSd"
      },
      "source": [
        "### 5a) 1to3gram correlation (\"differential\") word clouds\n",
        "\n",
        "Let's repeat the exercise, but use our oh-so-carefully crafted 1to3grams table instead.\n",
        "\n",
        "Note that the only thing that changes from the 1grams is the `--feat_table` (`feat$1to3gram$msgs$user_id$0_05$pmi3_0`) we include in the command! (and the `OUTPUT_NAME` (`1to3grams_age_gender`), so that we keep the results separate and tidy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4myMPbcVXSd"
      },
      "outputs": [],
      "source": [
        "OUTPUT_NAME = '1to3grams_age_gender'\n",
        "feat_1to3gram_occ05_pmi3_user = 'feat$1to3gram$msgs$user_id$0_05$pmi3_0'\n",
        "\n",
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "outcomes_table = 'outcomes'\n",
        "\n",
        "OUTPUT_FOLDER = 'output_tutorial_9'\n",
        "!rm -rf {OUTPUT_FOLDER}/{OUTPUT_NAME}* # this deletes the output if exists\n",
        "!mkdir -p {OUTPUT_FOLDER} # and this makes the folder!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnCg8quoVXSd"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --correlate \\\n",
        "    --rmatrix --csv --sort \\\n",
        "    --feat_table '{feat_1to3gram_occ05_pmi3_user}' \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --outcomes age gender \\\n",
        "    --tagcloud --make_wordclouds \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME} > {OUTPUT_FOLDER}/logs.txt 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZos3tWKVXSd"
      },
      "source": [
        "The above command has written files to `output_tutorial_9/` with prefix `1to3grams_age_gender`.\n",
        "We again get html and csv files with all correlations for 10,000 1to3grams, and the word clouds. Let's look at gender.\n",
        "\n",
        "**Gender**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip7DKoe-VXSd"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-09/fig5.png\" width=\"800\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPK_DYVUVXSd"
      },
      "source": [
        "Note the additional_phrases of 2grams and 3grams. As some of the 2grams and 3grams don't seem to add that much (as in, are uninformative: \"a look at,\" \"I felt,\" \"when I\"), one could go back and increase the PMI threshold (say 6), and get a new feature table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgHK-jUCVXSd"
      },
      "source": [
        "#### 👩‍🔬💻 Exercise\n",
        "\n",
        "What would be the most efficient way to create such a table?  \n",
        "**HINT:** what's the last table you could ingest before PMI filtering?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24h3OWBfVXSd"
      },
      "source": [
        "**Answer:**  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rBQxCjg2Ry_W"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkX1FvPSVXSd"
      },
      "source": [
        "### 5b) Differential 1to3gram clouds for occupation\n",
        "\n",
        "Let's look at the words that correlate with the dummies for occupations, using the `--categories_to_binary` flag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQULPh34VXSd"
      },
      "outputs": [],
      "source": [
        "OUTPUT_NAME = '1to3grams_occu'\n",
        "\n",
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "outcomes_table = 'outcomes'\n",
        "feat_1to3gram_occ05_pmi3_user = 'feat$1to3gram$msgs$user_id$0_05$pmi3_0'\n",
        "\n",
        "OUTPUT_FOLDER = 'output_tutorial_9'\n",
        "!rm -rf {OUTPUT_FOLDER}/{OUTPUT_NAME}* # this deletes the output if exists\n",
        "!mkdir -p {OUTPUT_FOLDER} # and this makes the folder!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aUrOf2ZzVXSd"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --correlate \\\n",
        "    --rmatrix --csv --sort \\\n",
        "    --feat_table '{feat_1to3gram_occ05_pmi3_user}' \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --outcomes occu \\\n",
        "    --categories_to_binary occu \\\n",
        "    --tagcloud --make_wordclouds \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME} > {OUTPUT_FOLDER}/logs.txt 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ1tspXzVXSd"
      },
      "source": [
        "This takes \\~6 mins -- it does all the occupations (\\~20) one at a time times 10k-ish language features -- that's 200,000k language correlations. The above command has written files to `output_tutorial_9/1to3grams_age_gender_occu`.\n",
        "\n",
        "There are lot for images produced and mostly 1 for each occupation -- because negative correlations (\"words not used by students\") tend to have much weaker signal that \"words used by students.\" Please take a look in your folder. Do you like one in particular?\n",
        "\n",
        "Here are two positive correlation ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQHhydsOVXSd"
      },
      "source": [
        "**Agriculture**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-09/fig6_ag.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpx-VGf9VXSd"
      },
      "source": [
        "**Technology**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-09/fig7_tech.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vzQ7bxbVXSd"
      },
      "source": [
        "### 5c) Differential 1to3gram clouds for star signs\n",
        "\n",
        "Let's apply this pipeline to see if we can learn something about star signs. We again throw the `--categories_to_binary sign` conversion for `--outcomes sign` (We don't expect great results to manifest.)*\n",
        "\n",
        "\\*or do we? 🤔♉"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AL6jvblmVXSd"
      },
      "outputs": [],
      "source": [
        "OUTPUT_NAME = '1to3grams_sign'\n",
        "\n",
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "outcomes_table = 'outcomes'\n",
        "feat_1to3gram_occ05_pmi3_user = 'feat$1to3gram$msgs$user_id$0_05$pmi3_0'\n",
        "\n",
        "OUTPUT_FOLDER = 'output_tutorial_9'\n",
        "!rm -rf {OUTPUT_FOLDER}/{OUTPUT_NAME}* # this deletes the output if exists\n",
        "!mkdir -p {OUTPUT_FOLDER} # and this makes the folder!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yh5-s09vVXSd"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --correlate \\\n",
        "    --rmatrix --csv --sort \\\n",
        "    --feat_table '{feat_1to3gram_occ05_pmi3_user}' \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --outcomes sign \\\n",
        "    --categories_to_binary sign \\\n",
        "    --tagcloud --make_wordclouds \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME} > {OUTPUT_FOLDER}/logs.txt 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPqH8rrLVXSd"
      },
      "source": [
        "2 minutes for 12 * 10k correlations. The above command has written files to `output_tutorial_9/` with prefix `1to3grams_sign`.\n",
        "\n",
        "Again, there are lot of images produced -- 1 for each sign. Have a look! Here is the wordcloud for `Pisces`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvwa502TVXSd"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-09/fig8.png\" width=\"500\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TyeeCM2VXSd"
      },
      "source": [
        "seems like we picked up spurious signal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Csgo8KzVXSd"
      },
      "source": [
        "## 6) Bonferroni vs Benjamini-Hochberg correction\n",
        "\n",
        "So far we've used the Benjamini-Hochberg correction for multiple comparisons. That largely works alright. When we get the sense that we may be looking at spurious language correlations (here an extra 20 significant among 10,000 features, so a 0.2% false positive fringe), it may be good to ramp up to the more stringent Bonferroni correction.\n",
        "\n",
        "We do that with `--p_correction bonferroni` which overrides the implicit `--p_correction BH`.\n",
        "\n",
        "Let's use Bonferroni for star signs to see if it makes a difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEDatr6IVXSd"
      },
      "outputs": [],
      "source": [
        "OUTPUT_NAME = '1to3grams_sign_BONF'\n",
        "\n",
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "outcomes_table = 'outcomes'\n",
        "feat_1to3gram_occ05_pmi3_user = 'feat$1to3gram$msgs$user_id$0_05$pmi3_0'\n",
        "\n",
        "OUTPUT_FOLDER = 'output_tutorial_9'\n",
        "!rm -rf {OUTPUT_FOLDER}/{OUTPUT_NAME}* # this deletes the output if exists\n",
        "!mkdir -p {OUTPUT_FOLDER} # and this makes the folder!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ9J5wP1VXSd"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --correlate \\\n",
        "    --rmatrix --csv --sort \\\n",
        "    --p_correction bonferroni \\\n",
        "    --feat_table '{feat_1to3gram_occ05_pmi3_user}' \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --categories_to_binary sign \\\n",
        "    --outcomes sign \\\n",
        "    --tagcloud --make_wordclouds \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTct5oqnVXSd"
      },
      "source": [
        "Let's have a look:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRdqxPxfVXSd"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-09/fig9.png\" width=\"400\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwtBfwkYVXSd"
      },
      "source": [
        "This cleared it up. We just learned that Bonferroni correction may be a good idea on this dataset. What we really need is a bigger dataset, with higher statistical power where we can also ramp up the GFT to 1,000, which is more conservative (see the homework!).\n",
        "\n",
        "1to3grams looked a little noisy for both occupations and star signs. We could have ramped up the PMI more. But let's move on.\n",
        "\n",
        "As you can see, calibrating PMI, occurrence filtering, and p_correction is often an iterative process, guided by rules of thumb and inspection of the output (\"are these phrases informative?\")\n",
        "\n",
        "And then of course, people who are born in Pisces-months may just say \"worthy\" and \"my heart\" slightly more, for whatever reason."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijZoagaOVXSd"
      },
      "source": [
        "## 7) Word Clouds with dictionary names as features\n",
        "\n",
        "In case we don't have it, let's extract LIWC2015 features again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4IdgSKKVXSd"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rUJH-xvVXSe"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --add_lex_table -l LIWC2015"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q5EWdLpVXSe"
      },
      "source": [
        "Let's use this word cloud making opportunity to generalize our learning a little bit:\n",
        "\n",
        "What we want to do now is to create wordclouds that have **LIWC dictionary names** in them. For this, we don't have to change anything in the word cloud syntax, other than to give DLATK a `*cat_LIWC2015*` feature table to correlate, rather than a 1gram or 1to3gram table.\n",
        "\n",
        "**Make sure you understand why this is** -- feature tables are abstract. It makes no difference to them if the distinct features in them are words, phrases, names of dictionaries, or topic ids (as we see in the next tutorial), or cats, chickens, or whatever.\n",
        "\n",
        "If we correlate feature table against outcomes and make wordclouds,\n",
        "\n",
        "* DLATK will **correlate** all the distinct **feature group_norms** (here: dictionary categories, like \"POSEMO\" and \"NEGEMO\") it has for all the **groups** against the **outcome**\n",
        "* **visualize** the **shortlisted** strongest positive and negative correlations (separately) (using the strings written in the **feat** column: here, it's POSEMO, etc.),\n",
        "* with the size reflecting correlation magnitude, and color relative frequency (from the values column -- total word count per dictionary).\n",
        "\n",
        "So let's run a correlate for a LIWC feature table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tpFwZoZsVXSe"
      },
      "outputs": [],
      "source": [
        "OUTPUT_NAME = 'LIWC_age_gender_occu'\n",
        "feat_liwc_user = 'feat$cat_LIWC2015$msgs$user_id$1gra'\n",
        "\n",
        "# same as before\n",
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "outcomes_table = 'outcomes'\n",
        "\n",
        "OUTPUT_FOLDER = 'output_tutorial_9'\n",
        "!rm -rf {OUTPUT_FOLDER}/{OUTPUT_NAME}* # this deletes the output if exists\n",
        "!mkdir -p {OUTPUT_FOLDER} # and this makes the folder!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oljHezgEVXSe"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --correlate \\\n",
        "    --rmatrix --csv --sort \\\n",
        "    --feat_table '{feat_liwc_user}' \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --categories_to_binary occu \\\n",
        "    --outcomes age gender occu \\\n",
        "    --tagcloud --make_wordclouds \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxBjTcqIVXSe"
      },
      "source": [
        "Btw, note in the above that we've done all of this at once:\n",
        "\n",
        "    --categories_to_binary occu \\\n",
        "    --outcomes age gender occu \\"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGG_dReoVXSe"
      },
      "source": [
        "The above command has written files to `output_tutorial_9/LIWC_age_gender_occu`.\n",
        "\n",
        "Images were produced -- 2 for age, 2 for gender, ~1 for each occupation. Please take a look.\n",
        "\n",
        "We have seen the HTML output for LIWC before -- **convince yourself** that the table snippet below corresponds to the top features in the positive gender and age correlation clouds (female and older, respectively -- the clouds on the right below)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTljRTjoVXSe"
      },
      "source": [
        "**HTML**\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-09/fig10_a.png\" width=\"900\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-09/fig11_b.png\" width=\"800\">\n"
      ],
      "metadata": {
        "id": "-JL1gicyecsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-09/fig12_c.png\" width=\"800\">"
      ],
      "metadata": {
        "id": "n1sGFDSjegXw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMXYKOSdVXSe"
      },
      "source": [
        "Ok, good, at this point you should have a sense of how different feature tables translate into different word cloud output, and how you get 1to3grams tables ready for these exploratory methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCAmwLLzVXSe"
      },
      "source": [
        "### 7a) LIWC wordclouds next to 1to3gram word clouds\n",
        "\n",
        "For whatever reason, we have found it very helpful to put LIWC wordclouds that give dictionary correlation patterns next to 1to3gram correlation word clouds for the same outcomes. See below for younger, with legend. Particularly if you are more familiar with the LIWC dictionaries, the LIWC clouds give a \"shorthand\" for the patterns you see in the full 1to3gram wordclouds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "331h2tRGVXSe"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-09/fig13.png\" width=\"1000\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hme1dH_ZVXSe"
      },
      "source": [
        "# 💻🤓 FYI"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have put a Powerpoint layouting template for these figures [HERE](https://github.com/CompPsychology/psych290_colab_public/blob/main/notebooks/FIGURE%20TEMPLATE%20-%201to3grams%20and%20topics.pptx) for your use now and forever 💟."
      ],
      "metadata": {
        "id": "wsTQQHTmz-AJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‼️ **Save your database and/or output files** ‼️"
      ],
      "metadata": {
        "id": "bS4qTq9zJdUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's save all this work into as a new database file in your GDrive `sqlite_databases` folder!"
      ],
      "metadata": {
        "id": "qGExUsbZThmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "database = 'dla_tutorial'"
      ],
      "metadata": {
        "id": "F7RlPan-KkYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# copy the database file to your Drive\n",
        "!cp -f \"sqlite_data/{database}.db\" \"/content/drive/MyDrive/sqlite_databases/\"\n",
        "\n",
        "print(f\"✅ Database '{database}.db' has been copied to your Google Drive.\")"
      ],
      "metadata": {
        "id": "Ls5d7ZXwJzxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We generated a lot of output in this tutorial! Here's how you can save it to your Drive if you want to!"
      ],
      "metadata": {
        "id": "_R0h2jyyJfah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_FOLDER = './outputs_tutorial_09'"
      ],
      "metadata": {
        "id": "pZl5sNn-J655"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Copy the database file to your Drive (-r makes it copy the folder and all files/folders inside)\n",
        "!cp -f -r {OUTPUT_FOLDER} \"/content/drive/MyDrive/\"\n",
        "\n",
        "print(f\"✅ '{OUTPUT_FOLDER}' has been copied to your Google Drive.\")"
      ],
      "metadata": {
        "id": "bwD74MqNJzR-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}