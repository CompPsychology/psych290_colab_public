{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CompPsychology/psych290_colab_public/blob/main/notebooks/week-04/W4_Tutorial_06_DLATK_lex_correlation_(dla_tutorial)_withSolutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpcF9v2Vfg01"
      },
      "source": [
        "# W4 Tutorial 6 -- Weighted dictionaries and correlations (DB: dla_tutorial) (2025-03)\n",
        "\n",
        "(c) Johannes Eichstaedt & the World Well-Being Project, 2023.\n",
        "\n",
        "‚úãüèª‚úãüèª NOTE - You need to create a copy of this notebook before you work through it. Click on \"Save a copy in Drive\" option in the File menu, and safe it to your Google Drive.\n",
        "\n",
        "‚úâÔ∏èüêû If you find a bug/something doesn't work, please slack us a screenshot, or email johannes.courses@gmail.com.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial we will extract 1grams (with its meta-table) and LIWC features like last tutorial. In addition to it, we'll also extract weighted dictionaries like **labMT** and **NRC**.\n",
        "\n",
        "Let's get on to it starting with setting up DLATK and copying the `dla_tutorial` database and `dlatk_lexica.db` to your Colab."
      ],
      "metadata": {
        "id": "IK0Jp51ru4YN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Setting up Colab with DLATK and SQLite"
      ],
      "metadata": {
        "id": "-jTQPZT6f1H7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1a) Install DLATK"
      ],
      "metadata": {
        "id": "hQi5BSwuf01Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# assigning the corpus database name\n",
        "database = \"dla_tutorial\""
      ],
      "metadata": {
        "id": "ynPeM3kMf_Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# installing DLATK and necessary packages\n",
        "!git clone -b psych290 https://github.com/dlatk/dlatk.git\n",
        "!pip install -r dlatk/install/requirements.txt\n",
        "!pip install dlatk/\n",
        "!pip install wordcloud langid jupysql"
      ],
      "metadata": {
        "id": "qYiKFmJ-f_eU",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbc9bea5-0ffd-4d76-ec19-dd709bb4b890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dlatk'...\n",
            "remote: Enumerating objects: 6991, done.\u001b[K\n",
            "remote: Counting objects: 100% (1076/1076), done.\u001b[K\n",
            "remote: Compressing objects: 100% (149/149), done.\u001b[K\n",
            "remote: Total 6991 (delta 994), reused 935 (delta 927), pack-reused 5915 (from 1)\u001b[K\n",
            "Receiving objects: 100% (6991/6991), 62.38 MiB | 6.58 MiB/s, done.\n",
            "Resolving deltas: 100% (4947/4947), done.\n",
            "Updating files: 100% (338/338), done.\n",
            "Collecting image<=1.5.33 (from -r dlatk/install/requirements.txt (line 1))\n",
            "  Downloading image-1.5.33.tar.gz (15 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting langid<=1.1.6,>=1.1.4 (from -r dlatk/install/requirements.txt (line 2))\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib<=3.10.0,>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 3)) (3.10.0)\n",
            "Collecting mysqlclient<=2.1.1 (from -r dlatk/install/requirements.txt (line 4))\n",
            "  Downloading mysqlclient-2.1.1.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.1/88.1 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk<=3.9.1,>=3.1 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 5)) (3.9.1)\n",
            "Requirement already satisfied: numpy<=2.0.2 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 6)) (2.0.2)\n",
            "Requirement already satisfied: pandas<=2.2.2,>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 7)) (2.2.2)\n",
            "Requirement already satisfied: patsy<=1.0.1,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<=2.8.2,>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 9)) (2.8.2)\n",
            "Requirement already satisfied: rpy2<=3.5.17 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 10)) (3.5.17)\n",
            "Requirement already satisfied: scikit-learn<=1.6.1 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 11)) (1.6.1)\n",
            "Requirement already satisfied: scipy<=1.14.1,>=0.13.3 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 12)) (1.14.1)\n",
            "Collecting SQLAlchemy<=2.0.39,>=0.9.9 (from -r dlatk/install/requirements.txt (line 13))\n",
            "  Downloading sqlalchemy-2.0.39-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: statsmodels<=0.14.4,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 14)) (0.14.4)\n",
            "Requirement already satisfied: wordcloud<=1.9.4,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from -r dlatk/install/requirements.txt (line 15)) (1.9.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from image<=1.5.33->-r dlatk/install/requirements.txt (line 1)) (11.1.0)\n",
            "Collecting django (from image<=1.5.33->-r dlatk/install/requirements.txt (line 1))\n",
            "  Downloading Django-5.2-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from image<=1.5.33->-r dlatk/install/requirements.txt (line 1)) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10.0,>=1.3.1->-r dlatk/install/requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10.0,>=1.3.1->-r dlatk/install/requirements.txt (line 3)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10.0,>=1.3.1->-r dlatk/install/requirements.txt (line 3)) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10.0,>=1.3.1->-r dlatk/install/requirements.txt (line 3)) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10.0,>=1.3.1->-r dlatk/install/requirements.txt (line 3)) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<=3.10.0,>=1.3.1->-r dlatk/install/requirements.txt (line 3)) (3.2.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<=3.9.1,>=3.1->-r dlatk/install/requirements.txt (line 5)) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<=3.9.1,>=3.1->-r dlatk/install/requirements.txt (line 5)) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<=3.9.1,>=3.1->-r dlatk/install/requirements.txt (line 5)) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk<=3.9.1,>=3.1->-r dlatk/install/requirements.txt (line 5)) (4.67.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<=2.2.2,>=0.17.1->-r dlatk/install/requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<=2.2.2,>=0.17.1->-r dlatk/install/requirements.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: cffi>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from rpy2<=3.5.17->-r dlatk/install/requirements.txt (line 10)) (1.17.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from rpy2<=3.5.17->-r dlatk/install/requirements.txt (line 10)) (3.1.6)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.11/dist-packages (from rpy2<=3.5.17->-r dlatk/install/requirements.txt (line 10)) (5.3.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<=1.6.1->-r dlatk/install/requirements.txt (line 11)) (3.6.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<=2.0.39,>=0.9.9->-r dlatk/install/requirements.txt (line 13)) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<=2.0.39,>=0.9.9->-r dlatk/install/requirements.txt (line 13)) (4.13.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.15.1->rpy2<=3.5.17->-r dlatk/install/requirements.txt (line 10)) (2.22)\n",
            "Collecting asgiref>=3.8.1 (from django->image<=1.5.33->-r dlatk/install/requirements.txt (line 1))\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Requirement already satisfied: sqlparse>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from django->image<=1.5.33->-r dlatk/install/requirements.txt (line 1)) (0.5.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->rpy2<=3.5.17->-r dlatk/install/requirements.txt (line 10)) (3.0.2)\n",
            "Downloading sqlalchemy-2.0.39-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Django-5.2-py3-none-any.whl (8.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m103.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Building wheels for collected packages: image, langid, mysqlclient\n",
            "  Building wheel for image (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for image: filename=image-1.5.33-py2.py3-none-any.whl size=19482 sha256=721917b429d44e8a76309d412e7239b5c2b804d9dd862bcf7aec6ed094291b46\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/40/4f/3c9a8d0f22a1a6f966975a460e5cb509a1e7dc42e2ce5d9a6d\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941171 sha256=8c9cc7f1b3bd1b0a7d654988add6376ba4050947c09c0bbab54750e4d2bb4e95\n",
            "  Stored in directory: /root/.cache/pip/wheels/32/6a/b6/b7eb43a6ad55b139c15c5daa29f3707659cfa6944d3c696f5b\n",
            "  Building wheel for mysqlclient (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mysqlclient: filename=mysqlclient-2.1.1-cp311-cp311-linux_x86_64.whl size=106439 sha256=da2c637d219323157ae35a2157a10bedb651512380332a80f7d20da6421299a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/44/b2/cdf313664c12c8b11f88afd90079823c63369ab9135fc79185\n",
            "Successfully built image langid mysqlclient\n",
            "Installing collected packages: SQLAlchemy, mysqlclient, langid, asgiref, django, image\n",
            "  Attempting uninstall: SQLAlchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.40\n",
            "    Uninstalling SQLAlchemy-2.0.40:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.40\n",
            "Successfully installed SQLAlchemy-2.0.39 asgiref-3.8.1 django-5.2 image-1.5.33 langid-1.1.6 mysqlclient-2.1.1\n",
            "Processing ./dlatk\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk<=3.9.1,>=3.1 in /usr/local/lib/python3.11/dist-packages (from dlatk==1.3.1) (3.9.1)\n",
            "Requirement already satisfied: numpy<=2.0.2 in /usr/local/lib/python3.11/dist-packages (from dlatk==1.3.1) (2.0.2)\n",
            "Requirement already satisfied: pandas<=2.2.2,>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from dlatk==1.3.1) (2.2.2)\n",
            "Requirement already satisfied: patsy<=1.0.1,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from dlatk==1.3.1) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<=2.8.2,>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from dlatk==1.3.1) (2.8.2)\n",
            "Requirement already satisfied: scikit-learn<=1.6.1 in /usr/local/lib/python3.11/dist-packages (from dlatk==1.3.1) (1.6.1)\n",
            "Requirement already satisfied: scipy<=1.14.1,>=0.13.3 in /usr/local/lib/python3.11/dist-packages (from dlatk==1.3.1) (1.14.1)\n",
            "Requirement already satisfied: statsmodels<=0.14.4,>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from dlatk==1.3.1) (0.14.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<=3.9.1,>=3.1->dlatk==1.3.1) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk<=3.9.1,>=3.1->dlatk==1.3.1) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<=3.9.1,>=3.1->dlatk==1.3.1) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk<=3.9.1,>=3.1->dlatk==1.3.1) (4.67.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<=2.2.2,>=0.17.1->dlatk==1.3.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<=2.2.2,>=0.17.1->dlatk==1.3.1) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<=2.8.2,>=2.5.0->dlatk==1.3.1) (1.17.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<=1.6.1->dlatk==1.3.1) (3.6.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from statsmodels<=0.14.4,>=0.5.0->dlatk==1.3.1) (24.2)\n",
            "Building wheels for collected packages: dlatk\n",
            "  Building wheel for dlatk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dlatk: filename=dlatk-1.3.1-py3-none-any.whl size=35635918 sha256=fdf0b71b65dd798f4458610e1eee5eb949fa6cabab25fb02555498518edc252e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fznjrxmn/wheels/cc/c9/65/e1ecc64bac68518c07b286fe86921aa938e11a0c3a87d8ff93\n",
            "Successfully built dlatk\n",
            "Installing collected packages: dlatk\n",
            "Successfully installed dlatk-1.3.1\n",
            "Requirement already satisfied: wordcloud in /usr/local/lib/python3.11/dist-packages (1.9.4)\n",
            "Requirement already satisfied: langid in /usr/local/lib/python3.11/dist-packages (1.1.6)\n",
            "Collecting jupysql\n",
            "  Downloading jupysql-0.11.1-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from wordcloud) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from wordcloud) (11.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from wordcloud) (3.10.0)\n",
            "Requirement already satisfied: prettytable>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from jupysql) (3.16.0)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.11/dist-packages (from jupysql) (2.0.39)\n",
            "Requirement already satisfied: sqlparse in /usr/local/lib/python3.11/dist-packages (from jupysql) (0.5.3)\n",
            "Requirement already satisfied: ipython-genutils>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from jupysql) (0.2.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from jupysql) (3.1.6)\n",
            "Requirement already satisfied: sqlglot>=11.3.7 in /usr/local/lib/python3.11/dist-packages (from jupysql) (25.20.2)\n",
            "Collecting jupysql-plugin>=0.4.2 (from jupysql)\n",
            "  Downloading jupysql_plugin-0.4.5-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting ploomber-core>=0.2.7 (from jupysql)\n",
            "  Downloading ploomber_core-0.2.26-py3-none-any.whl.metadata (527 bytes)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from ploomber-core>=0.2.7->jupysql) (6.0.2)\n",
            "Collecting posthog (from ploomber-core>=0.2.7->jupysql)\n",
            "  Downloading posthog-3.25.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prettytable>=3.12.0->jupysql) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->jupysql) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->wordcloud) (2.8.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy->jupysql) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy->jupysql) (4.13.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.17.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.7 in /usr/local/lib/python3.11/dist-packages (from posthog->ploomber-core>=0.2.7->jupysql) (2.32.3)\n",
            "Collecting monotonic>=1.5 (from posthog->ploomber-core>=0.2.7->jupysql)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog->ploomber-core>=0.2.7->jupysql)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog->ploomber-core>=0.2.7->jupysql) (1.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.7->jupysql) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.7->jupysql) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.7->jupysql) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.7->posthog->ploomber-core>=0.2.7->jupysql) (2025.1.31)\n",
            "Downloading jupysql-0.11.1-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jupysql_plugin-0.4.5-py3-none-any.whl (192 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m192.8/192.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ploomber_core-0.2.26-py3-none-any.whl (22 kB)\n",
            "Downloading posthog-3.25.0-py2.py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Installing collected packages: monotonic, backoff, posthog, ploomber-core, jupysql-plugin, jupysql\n",
            "Successfully installed backoff-2.2.1 jupysql-0.11.1 jupysql-plugin-0.4.5 monotonic-1.6 ploomber-core-0.2.26 posthog-3.25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1b) Mount Google Drive and copy database"
      ],
      "metadata": {
        "id": "0keHaicIgpZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive & copy database to Colab\n",
        "\n",
        "# connects & mounts your Google Drive to this colab space\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# copies {database_name}.db to the sqlite_data folder in this Colab\n",
        "!cp -f \"/content/drive/MyDrive/sqlite_databases/{database}.db\" \"sqlite_data\"\n",
        "\n",
        "# this copies dlatk_lexica.db from your Google Drive to Colab\n",
        "!cp -f \"/content/drive/MyDrive/sqlite_databases/dlatk_lexica.db\" \"sqlite_data\""
      ],
      "metadata": {
        "id": "A0L5GyUHgrho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99e82efc-a5ea-4787-b5cf-a514ef05ff90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1c) Setup database connection"
      ],
      "metadata": {
        "id": "-Q9edWLIgs_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loads the %%sql extension\n",
        "%load_ext sql\n",
        "\n",
        "# connects the extension to the database\n",
        "from sqlalchemy import create_engine\n",
        "tutorial_db_engine = create_engine(f\"sqlite:///sqlite_data/{database}.db?charset=utf8mb4\")\n",
        "dlatk_lexica_engine = create_engine(f\"sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4\")\n",
        "\n",
        "# attaches the dlatk_lexica.db so tutorial_db_engine can query both databases\n",
        "from IPython import get_ipython\n",
        "from sqlalchemy import event\n",
        "\n",
        "# auto‚Äëattach the lexica db whenever tutorial_db_engine connects\n",
        "@event.listens_for(tutorial_db_engine, \"connect\")\n",
        "def _attach_lexica(dbapi_conn, connection_record):\n",
        "    dbapi_conn.execute(\"ATTACH DATABASE 'sqlite_data/dlatk_lexica.db' AS dlatk_lexica;\")\n",
        "\n",
        "%sql tutorial_db_engine\n",
        "\n",
        "#set the output limit to 50\n",
        "%config SqlMagic.displaylimit = 50"
      ],
      "metadata": {
        "id": "ejiyVLi8gwgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1d) (ONLY IF NEEDED: SOFT RELOAD): **If you have a \"database lock\" problem**"
      ],
      "metadata": {
        "id": "Xprqeda3gylp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you face a \"database locked\" issue, restart the session & run this cell to get set back up!\n",
        "\n",
        "database = \"dla_tutorial\"\n",
        "\n",
        "%reload_ext sql\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "tutorial_db_engine = create_engine(f\"sqlite:///sqlite_data/{database}.db?charset=utf8mb4\")\n",
        "dlatk_lexica_engine = create_engine(f\"sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4\")\n",
        "\n",
        "# set the output limit to 50\n",
        "%config SqlMagic.displaylimit = 50\n",
        "\n",
        "from IPython import get_ipython\n",
        "from sqlalchemy import event\n",
        "\n",
        "# auto‚Äëattach the lexica db whenever tutorial_db_engine connects\n",
        "@event.listens_for(tutorial_db_engine, \"connect\")\n",
        "def _attach_lexica(dbapi_conn, connection_record):\n",
        "    dbapi_conn.execute(\"ATTACH DATABASE 'sqlite_data/dlatk_lexica.db' AS dlatk_lexica;\")\n",
        "\n",
        "%sql tutorial_db_engine"
      ],
      "metadata": {
        "id": "tVqs5ZN2g0at"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **(If needed)** re-extract features and dictionaries"
      ],
      "metadata": {
        "id": "zgB4FISevZA6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3trTmK5fg02"
      },
      "source": [
        "This notebook needs 1grams and LIWC features. For the latter, we'll extract `mini_LIWC2015` (containing only `POSEMO`, `NEGEMO`, and `SOCIAL` categories) like last tutorial. If you have them from previous tutorials, you can skip this step. Otherwise, create these feature tables with the DLATK command below. If you run these commands and the tables already exist, the commands will simply replace the existing tables.\n",
        "\n",
        "First comes the 1grams feature table:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gd771wAsfg02"
      },
      "outputs": [],
      "source": [
        "database = \"dla_tutorial\"\n",
        "msgs_table = 'msgs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyVhZ9v7fg02"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --add_ngrams -n 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQlRmpCBfg02"
      },
      "source": [
        "The above command produced table `feat$1gram$msgs$user_id` containing 1grams as features. Then, the `mini_LIWC2015` feature table."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNi0Kbbdfg02"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --add_lex_table -l mini_LIWC2015"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTHkle-Efg02"
      },
      "source": [
        "The above command produced table `feat$cat_mini_LIWC2015$msgs$user_id$1gra` with `mini_LIWC2015` as features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es_tsrNSfg02"
      },
      "source": [
        "## 2) Weighted Dictionaries\n",
        "\n",
        "Alright, time for weighted dictionaries. These weights could originate from word-level annotations (as in the case of labMT), or be inferred through machine learning based on document-level annotations (or some other magic).  \n",
        "\n",
        "All necessary dictionaries are stored in the `dlatk_lexica` database. Let's look at the dictionaries inside the database."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üí°üí° Remember, to switch to switch between databases, we use:\n",
        "```\n",
        "%sql tutorial_db_engine  # for the main database (i.e., dla_tutorial)\n",
        "```\n",
        "\n",
        "or\n",
        "\n",
        "```\n",
        "%sql dlatk_lexica_engine  # for the dlatk lexica database\n",
        "```\n",
        "\n",
        "If you get an error saying \"other database already connected\", try running `%reload_ext sql`. If that doesn't work, resort to Runtime ==> Restart Session."
      ],
      "metadata": {
        "id": "Xg9cYG0qwReN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql dlatk_lexica_engine"
      ],
      "metadata": {
        "id": "3xzNMMgCxCA5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = %sqlcmd tables\n",
        "print(res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM4rMrBtDkfd",
        "outputId": "beef0e07-59f0-4688-b47d-7dde79d51c96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------------------+\n",
            "|         Name         |\n",
            "+----------------------+\n",
            "|       LIWC2015       |\n",
            "|    dd_PastPreFut     |\n",
            "| dd_emnlp14_ageGender |\n",
            "|      dd_permaV3      |\n",
            "| dd_wassa16_affectInt |\n",
            "|      fb2000_cp       |\n",
            "|  fb2000_freq_t50ll   |\n",
            "|        labmt         |\n",
            "|    mini_LIWC2015     |\n",
            "|         nrc          |\n",
            "|       nrc_emot       |\n",
            "|       nrc_sent       |\n",
            "+----------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "üê¨üê¨üê¨\n",
        "USE dlatk_lexica;\n",
        "SHOW TABLES;\n",
        "üê¨üê¨üê¨\n",
        "```"
      ],
      "metadata": {
        "id": "b9jDqVMofg03"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k_xvckQfg03"
      },
      "source": [
        "Among the tables, we have already seen `LIWC2015`, which looks like -"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNWrmoubfg03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "b4affc95-9fee-4f7c-dd19-555f54dcf9f2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+-------+-------------+----------+--------+\n",
              "|   id  |     term    | category | weight |\n",
              "+-------+-------------+----------+--------+\n",
              "| 15743 |   various   |   ADJ    |   1    |\n",
              "|  6457 | world-class |  POWER   |   1    |\n",
              "|  6296 |   poorest   |  POWER   |   1    |\n",
              "|  5519 |   miscar*   |   BIO    |   1    |\n",
              "| 15016 |      ya     |   YOU    |   1    |\n",
              "+-------+-------------+----------+--------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>id</th>\n",
              "            <th>term</th>\n",
              "            <th>category</th>\n",
              "            <th>weight</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>15743</td>\n",
              "            <td>various</td>\n",
              "            <td>ADJ</td>\n",
              "            <td>1</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>6457</td>\n",
              "            <td>world-class</td>\n",
              "            <td>POWER</td>\n",
              "            <td>1</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>6296</td>\n",
              "            <td>poorest</td>\n",
              "            <td>POWER</td>\n",
              "            <td>1</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>5519</td>\n",
              "            <td>miscar*</td>\n",
              "            <td>BIO</td>\n",
              "            <td>1</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>15016</td>\n",
              "            <td>ya</td>\n",
              "            <td>YOU</td>\n",
              "            <td>1</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT *\n",
        "FROM LIWC2015\n",
        "ORDER BY RANDOM()\n",
        "LIMIT 5;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ilutqr93fg03"
      },
      "source": [
        "The column `weight` is 1.0 for all rows in LIWC2015 because it deems all the terms in a category to be equally important (LIWC is unweighted). Let's confirm this with one of the categories."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NoMDP4fufg03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "outputId": "56cae82b-52e6-452a-80a5-39b790e1020d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+-----+-----------+----------+--------+\n",
              "|  id |    term   | category | weight |\n",
              "+-----+-----------+----------+--------+\n",
              "| 309 |   let's   |    WE    |   1    |\n",
              "| 310 |    lets   |    WE    |   1    |\n",
              "| 311 |    our    |    WE    |   1    |\n",
              "| 312 |    ours   |    WE    |   1    |\n",
              "| 313 | ourselves |    WE    |   1    |\n",
              "| 314 |     us    |    WE    |   1    |\n",
              "| 315 |     we    |    WE    |   1    |\n",
              "| 316 |    we'd   |    WE    |   1    |\n",
              "| 317 |   we'll   |    WE    |   1    |\n",
              "| 318 |   we're   |    WE    |   1    |\n",
              "| 319 |   we've   |    WE    |   1    |\n",
              "| 320 |    weve   |    WE    |   1    |\n",
              "+-----+-----------+----------+--------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>id</th>\n",
              "            <th>term</th>\n",
              "            <th>category</th>\n",
              "            <th>weight</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>309</td>\n",
              "            <td>let's</td>\n",
              "            <td>WE</td>\n",
              "            <td>1</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>310</td>\n",
              "            <td>lets</td>\n",
              "            <td>WE</td>\n",
              "            <td>1</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>311</td>\n",
              "            <td>our</td>\n",
              "            <td>WE</td>\n",
              "            <td>1</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>312</td>\n",
              "            <td>ours</td>\n",
              "            <td>WE</td>\n",
              "            <td>1</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>313</td>\n",
              "            <td>ourselves</td>\n",
              "            <td>WE</td>\n",
              "            <td>1</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>314</td>\n",
              "            <td>us</td>\n",
              "            <td>WE</td>\n",
              "            <td>1</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>315</td>\n",
              "            <td>we</td>\n",
              "            <td>WE</td>\n",
              "            <td>1</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>316</td>\n",
              "            <td>we'd</td>\n",
              "            <td>WE</td>\n",
              "            <td>1</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>317</td>\n",
              "            <td>we'll</td>\n",
              "            <td>WE</td>\n",
              "            <td>1</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>318</td>\n",
              "            <td>we're</td>\n",
              "            <td>WE</td>\n",
              "            <td>1</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>319</td>\n",
              "            <td>we've</td>\n",
              "            <td>WE</td>\n",
              "            <td>1</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>320</td>\n",
              "            <td>weve</td>\n",
              "            <td>WE</td>\n",
              "            <td>1</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT *\n",
        "FROM LIWC2015\n",
        "WHERE category = 'WE';"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BW91XTljfg03"
      },
      "source": [
        "The dictionaries **LabMT** and **NRC**, on the other hand, do not consider all terms to be equally important and so weigh different terms differently based on their importance within the category.  \n",
        "\n",
        "[**LabMT**](https://www.nature.com/articles/srep02625)  \n",
        "The labMT word list was created by combining the ~10k words most frequently appearing in four sources: Twitter, the New York Times, Google Books, and music lyrics, and then scoring the words for sentiment on Amazon‚Äôs Mechanical Turk (annotations range from 1 to 9). The table `dlatk_lexica.labmt` contains this lexicon/dictionary, with the 3.7k words remaining that have valences <4 and >6. This follows the original authors' guidelines on how to use the dictionary.\n",
        "\n",
        "[**NRC**](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)  \n",
        "The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The annotations were manually done by crowdsourcing. The table `dlatk_lexica.nrc` contains just the positive/negative sentiments from NRC.\n",
        "\n",
        "Let us look at few records of `labmt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75UTmqjpfg03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "ab2fb39d-b175-4743-cc02-59d9daeeb7ee"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+-------+-----------+----------+--------+\n",
              "|   id  |    term   | category | weight |\n",
              "+-------+-----------+----------+--------+\n",
              "|  2590 |   writes  | valence  |  6.02  |\n",
              "|  2267 | proceeded | valence  |  6.12  |\n",
              "| 10027 |  beating  | valence  |  2.56  |\n",
              "|   18  |   smiled  | valence  |  8.08  |\n",
              "|  2175 |    wed    | valence  |  6.16  |\n",
              "+-------+-----------+----------+--------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>id</th>\n",
              "            <th>term</th>\n",
              "            <th>category</th>\n",
              "            <th>weight</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>2590</td>\n",
              "            <td>writes</td>\n",
              "            <td>valence</td>\n",
              "            <td>6.02</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2267</td>\n",
              "            <td>proceeded</td>\n",
              "            <td>valence</td>\n",
              "            <td>6.12</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>10027</td>\n",
              "            <td>beating</td>\n",
              "            <td>valence</td>\n",
              "            <td>2.56</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>18</td>\n",
              "            <td>smiled</td>\n",
              "            <td>valence</td>\n",
              "            <td>8.08</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2175</td>\n",
              "            <td>wed</td>\n",
              "            <td>valence</td>\n",
              "            <td>6.16</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT *\n",
        "FROM labmt\n",
        "ORDER BY RANDOM()\n",
        "LIMIT 5;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW3bmf1ofg03"
      },
      "source": [
        "There is only one category in `labmt` -- `valence`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VjDYuPnfg03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "a29b9b1c-05b1-4ee4-f1ca-3285ac7f923a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+----------+\n",
              "| category |\n",
              "+----------+\n",
              "| valence  |\n",
              "+----------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>category</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>valence</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT DISTINCT(category)\n",
        "FROM labmt;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOHFdJR_fg03"
      },
      "source": [
        "#### üë©‚Äçüî¨üíª Exercise\n",
        "\n",
        "All the terms/words belong to this one category - `valence`. Can you see how many words/terms we have?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpAyEVn-fg03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "cd89e6ea-63b8-40b2-ce74-6e4b3aedb8fe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+-----------+\n",
              "| num_terms |\n",
              "+-----------+\n",
              "|    3731   |\n",
              "+-----------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>num_terms</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>3731</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(DISTINCT term) AS num_terms\n",
        "FROM labmt;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPKy2-zEfg03"
      },
      "source": [
        "That is `3731` terms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JWDG77cfg03"
      },
      "source": [
        "#### üë©‚Äçüî¨üíª Exercise\n",
        "\n",
        "Now, what are the the minimum and maximum weights in this dictionary?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arJfAhRVfg03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "c083cd07-e22c-4673-f4a0-dc05e7d181c9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+------------+------------+\n",
              "| min_weight | max_weight |\n",
              "+------------+------------+\n",
              "|    1.3     |    8.5     |\n",
              "+------------+------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>min_weight</th>\n",
              "            <th>max_weight</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>1.3</td>\n",
              "            <td>8.5</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT MIN(weight) AS min_weight, MAX(weight) AS max_weight\n",
        "FROM labmt;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNIqkF7Vfg03"
      },
      "source": [
        "#### üë©‚Äçüî¨üíª Exercise\n",
        "\n",
        "Also, confirm if there are any terms with weight >4 and <6?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vGDfhO9fg03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "7277a28d-283f-4e39-fd1b-e1f922c79de2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+-----------+\n",
              "| num_terms |\n",
              "+-----------+\n",
              "|     0     |\n",
              "+-----------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>num_terms</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>0</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(*) AS num_terms\n",
        "FROM labmt\n",
        "WHERE (weight > 4 ) AND (weight < 6);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuq8uxdkfg03"
      },
      "source": [
        "Now that we have explored `labmt`, let's check `nrc` - which is a sentiment dictionary. Let's start by looking at some random records."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yH1U8pfWfg03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "353f0d63-37d7-4398-923b-f1ddafbe0095"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+-------+-----------+----------+--------+\n",
              "|   id  |    term   | category | weight |\n",
              "+-------+-----------+----------+--------+\n",
              "| 26139 | @wweajlee |   SENT   |  0.19  |\n",
              "| 53269 |   limbs   |   SENT   | -0.081 |\n",
              "| 97430 |   ledge   | neg_sent | -0.081 |\n",
              "| 86153 |    cave   | pos_sent | 0.055  |\n",
              "| 40804 |     91    |   SENT   | 0.008  |\n",
              "+-------+-----------+----------+--------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>id</th>\n",
              "            <th>term</th>\n",
              "            <th>category</th>\n",
              "            <th>weight</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>26139</td>\n",
              "            <td>@wweajlee</td>\n",
              "            <td>SENT</td>\n",
              "            <td>0.19</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>53269</td>\n",
              "            <td>limbs</td>\n",
              "            <td>SENT</td>\n",
              "            <td>-0.081</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>97430</td>\n",
              "            <td>ledge</td>\n",
              "            <td>neg_sent</td>\n",
              "            <td>-0.081</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>86153</td>\n",
              "            <td>cave</td>\n",
              "            <td>pos_sent</td>\n",
              "            <td>0.055</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>40804</td>\n",
              "            <td>91</td>\n",
              "            <td>SENT</td>\n",
              "            <td>0.008</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT *\n",
        "FROM nrc\n",
        "ORDER BY RANDOM()\n",
        "LIMIT 5;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0Tn8X2Rfg03"
      },
      "source": [
        "We see that nrc also has negative weights -- those words subtract from weights with positive weights in the same category.\n",
        "\n",
        "If those words dominate, the group_norms will end up being negative once it's extracted. That works fine in DLATK -- correlations etc. will still work (but maybe flipped, because there will be negative `group_norm`s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf6D6FcHfg03"
      },
      "source": [
        "#### üë©‚Äçüî¨üíª Exercise\n",
        "\n",
        "Can you check the number of unique categories?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gqvz_E7fg03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "a424bf71-63dd-4f42-87ad-fecdbb4b7b20"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+----------------+\n",
              "| num_categories |\n",
              "+----------------+\n",
              "|       3        |\n",
              "+----------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>num_categories</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>3</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(DISTINCT category) AS num_categories\n",
        "FROM nrc;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIj6dt-Yfg03"
      },
      "source": [
        "#### üë©‚Äçüî¨üíª Exercise\n",
        "\n",
        "Also, check the number of types (distinct tokens) in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5t7uKnX-fg03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "9be0968a-0e3a-4c89-9027-02f77ca6518c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+-----------+\n",
              "| num_terms |\n",
              "+-----------+\n",
              "|   54128   |\n",
              "+-----------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>num_terms</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>54128</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(distinct(term)) AS num_terms\n",
        "FROM nrc;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei1dCTYAfg04"
      },
      "source": [
        "That's a huge vocabulary! Another sign that these weights were learned though machine learning, and not through word annotations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmq8kf78fg04"
      },
      "source": [
        "### 2a) Extracting Weighted Dictionaries\n",
        "\n",
        "Let's now extract a weighted dictionary. The extraction process is the same, except now DLATK should multiply the occurrence of words in the dictionary by the weight of the words. The command for DLATK to extract a weighted dictionary needs `--weighted_lexicon` to tell DLATK to handle weights properly. Without it, it would ignore the weights, and the `_w` would be missing from the feat table name."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (i) Extracting `labmt`\n",
        "\n",
        "Let's extract `labmt`. Again, the order of DLATK flags does not matter. the `\\` just allows for line breaks to make it easier to look over the command.\n",
        "\n",
        "It's the same as when we extracted something like `mini_LIWC`, it just has `--weighted_lexicon` as an additional flag."
      ],
      "metadata": {
        "id": "zfvn7HOhhL5X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ySGtE8cfg04"
      },
      "outputs": [],
      "source": [
        "database = \"dla_tutorial\"\n",
        "msgs_table = \"msgs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyYgHlAtfg07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a04f7a20-b2cc-49aa-c7d8-7278d4bf74e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TopicExtractor: gensim Mallet wrapper unavailable, using Mallet directly.\n",
            "\n",
            "-----\n",
            "DLATK Interface Initiated: 2025-04-17 22:10:00\n",
            "-----\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "query: PRAGMA table_info(msgs)\n",
            "SQL Query: DROP TABLE IF EXISTS feat$cat_labmt_w$msgs$user_id$1gra\n",
            "SQL Query: CREATE TABLE feat$cat_labmt_w$msgs$user_id$1gra ( id INTEGER PRIMARY KEY, group_id INTEGER, feat VARCHAR(10), value INTEGER, group_norm DOUBLE)\n",
            "\n",
            "\n",
            "Creating index correl_field on table:feat$cat_labmt_w$msgs$user_id$1gra, column:group_id \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX correl_field$cat_labmt_w$msgs$user_id$1gra ON feat$cat_labmt_w$msgs$user_id$1gra (group_id)\n",
            "\n",
            "\n",
            "Creating index feature on table:feat$cat_labmt_w$msgs$user_id$1gra, column:feat \n",
            "\n",
            "\n",
            "SQL Query: CREATE INDEX feature$cat_labmt_w$msgs$user_id$1gra ON feat$cat_labmt_w$msgs$user_id$1gra (feat)\n",
            "WORD TABLE feat$1gram$msgs$user_id\n",
            "10 out of 1000 group Id's processed; 0.01 complete\n",
            "20 out of 1000 group Id's processed; 0.02 complete\n",
            "30 out of 1000 group Id's processed; 0.03 complete\n",
            "40 out of 1000 group Id's processed; 0.04 complete\n",
            "50 out of 1000 group Id's processed; 0.05 complete\n",
            "60 out of 1000 group Id's processed; 0.06 complete\n",
            "70 out of 1000 group Id's processed; 0.07 complete\n",
            "80 out of 1000 group Id's processed; 0.08 complete\n",
            "90 out of 1000 group Id's processed; 0.09 complete\n",
            "100 out of 1000 group Id's processed; 0.10 complete\n",
            "110 out of 1000 group Id's processed; 0.11 complete\n",
            "120 out of 1000 group Id's processed; 0.12 complete\n",
            "130 out of 1000 group Id's processed; 0.13 complete\n",
            "140 out of 1000 group Id's processed; 0.14 complete\n",
            "150 out of 1000 group Id's processed; 0.15 complete\n",
            "160 out of 1000 group Id's processed; 0.16 complete\n",
            "170 out of 1000 group Id's processed; 0.17 complete\n",
            "180 out of 1000 group Id's processed; 0.18 complete\n",
            "190 out of 1000 group Id's processed; 0.19 complete\n",
            "200 out of 1000 group Id's processed; 0.20 complete\n",
            "210 out of 1000 group Id's processed; 0.21 complete\n",
            "220 out of 1000 group Id's processed; 0.22 complete\n",
            "230 out of 1000 group Id's processed; 0.23 complete\n",
            "240 out of 1000 group Id's processed; 0.24 complete\n",
            "250 out of 1000 group Id's processed; 0.25 complete\n",
            "260 out of 1000 group Id's processed; 0.26 complete\n",
            "270 out of 1000 group Id's processed; 0.27 complete\n",
            "280 out of 1000 group Id's processed; 0.28 complete\n",
            "290 out of 1000 group Id's processed; 0.29 complete\n",
            "300 out of 1000 group Id's processed; 0.30 complete\n",
            "310 out of 1000 group Id's processed; 0.31 complete\n",
            "320 out of 1000 group Id's processed; 0.32 complete\n",
            "330 out of 1000 group Id's processed; 0.33 complete\n",
            "340 out of 1000 group Id's processed; 0.34 complete\n",
            "350 out of 1000 group Id's processed; 0.35 complete\n",
            "360 out of 1000 group Id's processed; 0.36 complete\n",
            "370 out of 1000 group Id's processed; 0.37 complete\n",
            "380 out of 1000 group Id's processed; 0.38 complete\n",
            "390 out of 1000 group Id's processed; 0.39 complete\n",
            "400 out of 1000 group Id's processed; 0.40 complete\n",
            "410 out of 1000 group Id's processed; 0.41 complete\n",
            "420 out of 1000 group Id's processed; 0.42 complete\n",
            "430 out of 1000 group Id's processed; 0.43 complete\n",
            "440 out of 1000 group Id's processed; 0.44 complete\n",
            "450 out of 1000 group Id's processed; 0.45 complete\n",
            "460 out of 1000 group Id's processed; 0.46 complete\n",
            "470 out of 1000 group Id's processed; 0.47 complete\n",
            "480 out of 1000 group Id's processed; 0.48 complete\n",
            "490 out of 1000 group Id's processed; 0.49 complete\n",
            "500 out of 1000 group Id's processed; 0.50 complete\n",
            "510 out of 1000 group Id's processed; 0.51 complete\n",
            "520 out of 1000 group Id's processed; 0.52 complete\n",
            "530 out of 1000 group Id's processed; 0.53 complete\n",
            "540 out of 1000 group Id's processed; 0.54 complete\n",
            "550 out of 1000 group Id's processed; 0.55 complete\n",
            "560 out of 1000 group Id's processed; 0.56 complete\n",
            "570 out of 1000 group Id's processed; 0.57 complete\n",
            "580 out of 1000 group Id's processed; 0.58 complete\n",
            "590 out of 1000 group Id's processed; 0.59 complete\n",
            "600 out of 1000 group Id's processed; 0.60 complete\n",
            "610 out of 1000 group Id's processed; 0.61 complete\n",
            "620 out of 1000 group Id's processed; 0.62 complete\n",
            "630 out of 1000 group Id's processed; 0.63 complete\n",
            "640 out of 1000 group Id's processed; 0.64 complete\n",
            "650 out of 1000 group Id's processed; 0.65 complete\n",
            "660 out of 1000 group Id's processed; 0.66 complete\n",
            "670 out of 1000 group Id's processed; 0.67 complete\n",
            "680 out of 1000 group Id's processed; 0.68 complete\n",
            "690 out of 1000 group Id's processed; 0.69 complete\n",
            "700 out of 1000 group Id's processed; 0.70 complete\n",
            "710 out of 1000 group Id's processed; 0.71 complete\n",
            "720 out of 1000 group Id's processed; 0.72 complete\n",
            "730 out of 1000 group Id's processed; 0.73 complete\n",
            "740 out of 1000 group Id's processed; 0.74 complete\n",
            "750 out of 1000 group Id's processed; 0.75 complete\n",
            "760 out of 1000 group Id's processed; 0.76 complete\n",
            "770 out of 1000 group Id's processed; 0.77 complete\n",
            "780 out of 1000 group Id's processed; 0.78 complete\n",
            "790 out of 1000 group Id's processed; 0.79 complete\n",
            "800 out of 1000 group Id's processed; 0.80 complete\n",
            "810 out of 1000 group Id's processed; 0.81 complete\n",
            "820 out of 1000 group Id's processed; 0.82 complete\n",
            "830 out of 1000 group Id's processed; 0.83 complete\n",
            "840 out of 1000 group Id's processed; 0.84 complete\n",
            "850 out of 1000 group Id's processed; 0.85 complete\n",
            "860 out of 1000 group Id's processed; 0.86 complete\n",
            "870 out of 1000 group Id's processed; 0.87 complete\n",
            "880 out of 1000 group Id's processed; 0.88 complete\n",
            "890 out of 1000 group Id's processed; 0.89 complete\n",
            "900 out of 1000 group Id's processed; 0.90 complete\n",
            "910 out of 1000 group Id's processed; 0.91 complete\n",
            "920 out of 1000 group Id's processed; 0.92 complete\n",
            "930 out of 1000 group Id's processed; 0.93 complete\n",
            "940 out of 1000 group Id's processed; 0.94 complete\n",
            "950 out of 1000 group Id's processed; 0.95 complete\n",
            "960 out of 1000 group Id's processed; 0.96 complete\n",
            "970 out of 1000 group Id's processed; 0.97 complete\n",
            "980 out of 1000 group Id's processed; 0.98 complete\n",
            "990 out of 1000 group Id's processed; 0.99 complete\n",
            "1000 out of 1000 group Id's processed; 1.00 complete\n",
            "1000 out of 1000 group Id's processed; 1.00 complete\n",
            "-------\n",
            "Settings:\n",
            "\n",
            "Database - dla_tutorial\n",
            "Corpus - msgs\n",
            "Group ID - user_id\n",
            "Feature table(s) - feat$cat_labmt_w$msgs$user_id$1gra\n",
            "-------\n",
            "Interface Runtime: 4.62 seconds\n",
            "DLATK exits with success! A good day indeed  ¬Ø\\_(„ÉÑ)_/¬Ø.\n"
          ]
        }
      ],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --weighted_lexicon \\\n",
        "    --add_lex_table -l labmt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5jukai2fg07"
      },
      "source": [
        "The above command produced a feature table `feat$cat_labmt_w$msgs$user_id$1gra`. Note that now the feature table name contains `...cat_labmt_w...` where the `_w` means that it was created with `--weighted_lexicon`. So if had forgotten to activate the weighted extraction, `_w` will not be in the name.\n",
        "\n",
        "Before we analyse the feature table, let's move to our `dla_tutorial` database."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "üê¨üê¨üê¨\n",
        "USE {database};\n",
        "üê¨üê¨üê¨\n",
        "```"
      ],
      "metadata": {
        "id": "adHFKeEyfg07"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql tutorial_db_engine"
      ],
      "metadata": {
        "id": "BRKDuv4Jztvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wCWH3-sLfg07"
      },
      "outputs": [],
      "source": [
        "feat_labmt_user = \"feat$cat_labmt_w$msgs$user_id$1gra\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZuSGAuCfg07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "48293af4-ed22-4aff-b6d7-cdeba2579c57"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+------+----------+---------+-------+--------------------+\n",
              "|  id  | group_id |   feat  | value |     group_norm     |\n",
              "+------+----------+---------+-------+--------------------+\n",
              "| 689  | 3478510  | valence |  241  | 1.4100778967867575 |\n",
              "| 857  | 3570813  | valence |  4097 | 1.2980608345112832 |\n",
              "| 1209 | 3808913  | valence |  239  | 1.391739526411658  |\n",
              "| 1121 | 3745004  | valence |  3924 | 1.2079569009748576 |\n",
              "|  35  |  734023  | valence |  204  | 1.2232298136645954 |\n",
              "+------+----------+---------+-------+--------------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>id</th>\n",
              "            <th>group_id</th>\n",
              "            <th>feat</th>\n",
              "            <th>value</th>\n",
              "            <th>group_norm</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>689</td>\n",
              "            <td>3478510</td>\n",
              "            <td>valence</td>\n",
              "            <td>241</td>\n",
              "            <td>1.4100778967867575</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>857</td>\n",
              "            <td>3570813</td>\n",
              "            <td>valence</td>\n",
              "            <td>4097</td>\n",
              "            <td>1.2980608345112832</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1209</td>\n",
              "            <td>3808913</td>\n",
              "            <td>valence</td>\n",
              "            <td>239</td>\n",
              "            <td>1.391739526411658</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1121</td>\n",
              "            <td>3745004</td>\n",
              "            <td>valence</td>\n",
              "            <td>3924</td>\n",
              "            <td>1.2079569009748576</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>35</td>\n",
              "            <td>734023</td>\n",
              "            <td>valence</td>\n",
              "            <td>204</td>\n",
              "            <td>1.2232298136645954</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT *\n",
        "FROM {{feat_labmt_user}}\n",
        "ORDER BY RANDOM()\n",
        "LIMIT 5;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAXg26O_fg07"
      },
      "source": [
        "The `_intercept` feature in `feat` column is a DLATK dummy variable. It is there to make sure a row appears for users who did not use any words from category dictionary. Basically, it ensures that if you run `SELECT COUNT(DISTINCT(group_id))` on a feature table, you will always get the number of `group_id`s it was extracted over."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePMxfyV0fg07"
      },
      "source": [
        "#### üë©‚Äçüî¨üíª Exercise\n",
        "\n",
        "Before we go forward, can you quickly find the top 10 most frequent words contained in the `labmt`? Ignoring weights, just in terms of unweighted mentions -- but do also output the weight (valence) for them, together with their word count.\n",
        "\n",
        "Please skip this exercise if it's holding you up for too long -- lots to get to!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCrtZrt5fg07"
      },
      "outputs": [],
      "source": [
        "feat_1gram_user = 'feat$1gram$msgs$user_id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dk8eGEF3fg07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "outputId": "377016b1-0e0f-4825-ba3d-c7fad7509d9e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+------+--------+--------+\n",
              "| feat | counts | weight |\n",
              "+------+--------+--------+\n",
              "|  my  |  985   |  6.16  |\n",
              "| all  |  983   |  6.22  |\n",
              "|  me  |  965   |  6.58  |\n",
              "| not  |  962   |  3.86  |\n",
              "| like |  949   |  7.22  |\n",
              "|  up  |  933   |  6.14  |\n",
              "| you  |  928   |  6.24  |\n",
              "| more |  905   |  6.24  |\n",
              "| will |  898   |  6.02  |\n",
              "|  we  |  895   |  6.38  |\n",
              "+------+--------+--------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>feat</th>\n",
              "            <th>counts</th>\n",
              "            <th>weight</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>my</td>\n",
              "            <td>985</td>\n",
              "            <td>6.16</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>all</td>\n",
              "            <td>983</td>\n",
              "            <td>6.22</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>me</td>\n",
              "            <td>965</td>\n",
              "            <td>6.58</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>not</td>\n",
              "            <td>962</td>\n",
              "            <td>3.86</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>like</td>\n",
              "            <td>949</td>\n",
              "            <td>7.22</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>up</td>\n",
              "            <td>933</td>\n",
              "            <td>6.14</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>you</td>\n",
              "            <td>928</td>\n",
              "            <td>6.24</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>more</td>\n",
              "            <td>905</td>\n",
              "            <td>6.24</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>will</td>\n",
              "            <td>898</td>\n",
              "            <td>6.02</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>we</td>\n",
              "            <td>895</td>\n",
              "            <td>6.38</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT feat, counts, weight\n",
        "FROM (SELECT feat, COUNT(value) AS counts\n",
        "      FROM {{feat_1gram_user}}\n",
        "      GROUP BY feat) AS word_counts, dlatk_lexica.labmt\n",
        "WHERE feat = term\n",
        "ORDER BY counts DESC\n",
        "LIMIT 10;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7F2inLffg07"
      },
      "source": [
        "As you see, function words are remaining in labmt with valences (weights) **just** above 6.\n",
        "\n",
        "What could possibly go wrong??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mojh2KaZfg07"
      },
      "source": [
        "#### (ii) Extracting `nrc`\n",
        "\n",
        "Let's repeat this for dictionary `NRC`. We use the same command but change the dictionary name. It does not matter to DLATK _how_ the weights came about (through word annotation or some machine learning model), only that the dictionary _has_ weights, and that it should use them (`--weighted_lexicon`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUx9FpHKfg08"
      },
      "outputs": [],
      "source": [
        "database = \"dla_tutorial\"\n",
        "msgs_table = \"msgs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvaBQgBufg08"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --weighted_lexicon \\\n",
        "    --add_lex_table -l nrc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3dGXcU2fg08"
      },
      "source": [
        "The above command produced a feature table `feat$cat_nrc_w$msgs$user_id$1gra` and again we see the `_w` in the name telling us the extraction was done with a weighted lexicon/dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sknsszhfg08"
      },
      "outputs": [],
      "source": [
        "feat_nrc_user = 'feat$cat_nrc_w$msgs$user_id$1gra'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "082Hy8sJfg08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "fc222672-572c-4b7b-9915-870487b1fa7c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+------+----------+------------+-------+-----------------------+\n",
              "|  id  | group_id |    feat    | value |       group_norm      |\n",
              "+------+----------+------------+-------+-----------------------+\n",
              "|  93  |  832265  |    SENT    | 40232 | -0.015619862794127084 |\n",
              "| 1572 | 3527113  | _intercept |   1   |          1.0          |\n",
              "| 1567 | 3525639  |  pos_sent  |  504  |  0.11579310344827577  |\n",
              "| 2395 | 3802286  |  neg_sent  | 16678 |  -0.22115482114630317 |\n",
              "| 2609 | 3858126  |    SENT    |  9853 |  0.019410895660203133 |\n",
              "+------+----------+------------+-------+-----------------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>id</th>\n",
              "            <th>group_id</th>\n",
              "            <th>feat</th>\n",
              "            <th>value</th>\n",
              "            <th>group_norm</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>93</td>\n",
              "            <td>832265</td>\n",
              "            <td>SENT</td>\n",
              "            <td>40232</td>\n",
              "            <td>-0.015619862794127084</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1572</td>\n",
              "            <td>3527113</td>\n",
              "            <td>_intercept</td>\n",
              "            <td>1</td>\n",
              "            <td>1.0</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1567</td>\n",
              "            <td>3525639</td>\n",
              "            <td>pos_sent</td>\n",
              "            <td>504</td>\n",
              "            <td>0.11579310344827577</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2395</td>\n",
              "            <td>3802286</td>\n",
              "            <td>neg_sent</td>\n",
              "            <td>16678</td>\n",
              "            <td>-0.22115482114630317</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2609</td>\n",
              "            <td>3858126</td>\n",
              "            <td>SENT</td>\n",
              "            <td>9853</td>\n",
              "            <td>0.019410895660203133</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT *\n",
        "FROM {{feat_nrc_user}}\n",
        "ORDER BY RANDOM()\n",
        "LIMIT 5;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzZ7sZK4fg08"
      },
      "source": [
        "Again, the value `_intercept` in `feat` column is to make sure a row appears for users who did not use any words from the dictionary (see above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rj3WALuZfg08"
      },
      "source": [
        "#### üë©‚Äçüî¨üíª Exercise\n",
        "\n",
        "Can you check for users who did not use any words from the `nrc` set of dictionaries (`SENT`, `POS_SENT`, `NEG_SENT`)?\n",
        "\n",
        "**Hint:** How would they appear in the NRC feat table?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4r0pluPfg08"
      },
      "outputs": [],
      "source": [
        "feat_nrc_user = 'feat$cat_nrc_w$msgs$user_id$1gra'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVTbSyCIfg08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 58
        },
        "outputId": "97dffd23-dfb3-43dd-ed19-3e7fd082db13"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+----------+\n",
              "| group_id |\n",
              "+----------+\n",
              "+----------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>group_id</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT group_id\n",
        "FROM {{feat_nrc_user}}\n",
        "GROUP BY group_id\n",
        "HAVING COUNT(*) = 1;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yhln6UCfg08"
      },
      "source": [
        "So this means that all users had at least 1 word that appeared in NRC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFrbbaAOfg08"
      },
      "source": [
        "### 2b) Words per group_id threshold\n",
        "\n",
        "### Group Frequency Threshold -- Ensuring minimum number of words per user\n",
        "\n",
        "Before we get to correlations, we need to talk about something important.\n",
        "\n",
        "Your 401(k) tax planning.\n",
        "\n",
        "When we do language analyses like correlations, we want to make sure users have at least (some) minimum number of words across their blog posts. If a user only has 5 words, their (relative frequency = group_norm) dictionary statistics won't be particularly meaningful, and might throw off the correlations. We don't want to lump them in with users who have 1,000 words.\n",
        "\n",
        "Generally, we often default to drop users (or groups, more generally) who have less than a predetermined (threshold) number of words (tokens) in the data set.\n",
        "\n",
        "In DLATK that's implemented as a cut-off (threshold), `--group_freq_thresh`, or the group frequency threshold -- minimum word count per group. With a (`--correl_field user_id`), that means minimum number of tokens/words for a user to be included in the correlation. `--group_freq_thresh` is meant to be used along with `--correlate`.\n",
        "\n",
        "**DLATK defaults?**\n",
        "- When we don't include it, DLATK will default to a `--group_freq_thresh` of 500, which is a good minimal threshold in our experience.\n",
        "- When we run message-level correlations, we want to include all messages, generally, even very short ones: If we want to override the threshold, we can always say `--group_freq_thresh 0` (include every group, regardless of how many tokens).\n",
        "\n",
        "In our dla_tutorial data set, if we limit to users with 500 words or more, 978 out of 1,000 make the cut. We verify this below.\n",
        "\n",
        "This is something we have to keep in mind when we work in R -- when we run correlations there between outcomes and features, as we will in the next tutorial, we want to make sure we also shortlist to the same 978 users.\n",
        "\n",
        "The best way to keep track is **just make word count another group-level outcome in the outcomes table**. So, let's get word counts from all users, and merge it as a column onto the outcome table.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rvWcXG1MhXYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (i) Word Counting Way (1)\n",
        "\n",
        "Let's remind ourselves, here is the first way -- summing over the values in the 1gram table:"
      ],
      "metadata": {
        "id": "wCUh-BGUhe5H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg89BV0sfg08"
      },
      "outputs": [],
      "source": [
        "feat_1gram_user = 'feat$1gram$msgs$user_id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJgt3VRnfg08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "outputId": "725c5f1d-3cb6-4d83-9fbf-f19b47f07075"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+----------+------------+\n",
              "| group_id | word_count |\n",
              "+----------+------------+\n",
              "|  942828  |   365311   |\n",
              "|  664485  |   262052   |\n",
              "| 1234212  |   259708   |\n",
              "|  979795  |   135594   |\n",
              "| 1807720  |   130719   |\n",
              "|  518116  |   111002   |\n",
              "| 2314011  |   104240   |\n",
              "| 2238828  |   97572    |\n",
              "| 1488330  |   95536    |\n",
              "| 1826527  |   95372    |\n",
              "+----------+------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>group_id</th>\n",
              "            <th>word_count</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>942828</td>\n",
              "            <td>365311</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>664485</td>\n",
              "            <td>262052</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1234212</td>\n",
              "            <td>259708</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>979795</td>\n",
              "            <td>135594</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1807720</td>\n",
              "            <td>130719</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>518116</td>\n",
              "            <td>111002</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2314011</td>\n",
              "            <td>104240</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2238828</td>\n",
              "            <td>97572</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1488330</td>\n",
              "            <td>95536</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1826527</td>\n",
              "            <td>95372</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT group_id, SUM(value) AS word_count\n",
        "FROM {{feat_1gram_user}}\n",
        "GROUP BY group_id\n",
        "ORDER BY word_count DESC\n",
        "LIMIT 10;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAg-Rj_Sfg08"
      },
      "source": [
        "#### (ii) Word Counting Way (2) (easiest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ek2t7OIfg08"
      },
      "source": [
        "Here is the second way: just using the meta-table that was also automatically extracted during 1gram extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ohfrA3Vfg08"
      },
      "outputs": [],
      "source": [
        "meta_1gram_user = 'feat$meta_1gram$msgs$user_id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kpkqt0qrfg08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 271
        },
        "outputId": "d07e07b1-459b-4480-df13-171784894611"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+----------+------------+\n",
              "| group_id | word_count |\n",
              "+----------+------------+\n",
              "|  942828  |  365311.0  |\n",
              "|  664485  |  262052.0  |\n",
              "| 1234212  |  259708.0  |\n",
              "|  979795  |  135594.0  |\n",
              "| 1807720  |  130719.0  |\n",
              "|  518116  |  111002.0  |\n",
              "| 2314011  |  104240.0  |\n",
              "| 2238828  |  97572.0   |\n",
              "| 1488330  |  95536.0   |\n",
              "| 1826527  |  95372.0   |\n",
              "+----------+------------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>group_id</th>\n",
              "            <th>word_count</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>942828</td>\n",
              "            <td>365311.0</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>664485</td>\n",
              "            <td>262052.0</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1234212</td>\n",
              "            <td>259708.0</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>979795</td>\n",
              "            <td>135594.0</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1807720</td>\n",
              "            <td>130719.0</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>518116</td>\n",
              "            <td>111002.0</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2314011</td>\n",
              "            <td>104240.0</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>2238828</td>\n",
              "            <td>97572.0</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1488330</td>\n",
              "            <td>95536.0</td>\n",
              "        </tr>\n",
              "        <tr>\n",
              "            <td>1826527</td>\n",
              "            <td>95372.0</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT group_id, group_norm AS word_count\n",
        "FROM {{meta_1gram_user}}\n",
        "WHERE feat = \"_total1grams\"\n",
        "ORDER BY word_count DESC\n",
        "LIMIT 10;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKKwBouufg08"
      },
      "source": [
        "As you can see, these are identical.  \n",
        "\n",
        "### 2c) Let's add wordcounts to our outcome table\n",
        "\n",
        "Let's add a column to the outcome table `outcomes` and store these counts there. It will put the column at the end. It will default to NULL entries, which is the `NA` of SQL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2gaPUowfg08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fdd2b8f3-821b-4250-b4aa-f99175a4a4de"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "++\n",
              "||\n",
              "++\n",
              "++"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "ALTER TABLE outcomes ADD COLUMN wordcount INT NULL;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTIxMYfEfg08"
      },
      "source": [
        "Now we use `UPDATE table SET Var = something` to update."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "\n",
        "UPDATE outcomes AS a\n",
        "SET wordcount = (\n",
        "                SELECT b.group_norm\n",
        "                FROM {{meta_1gram_user}} AS b\n",
        "                WHERE b.group_id = a.user_id AND b.feat = '_total1grams'\n",
        "                )\n",
        "WHERE a.user_id IN (SELECT group_id FROM {{meta_1gram_user}} WHERE feat = '_total1grams');"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "uZIlLvecLsVc",
        "outputId": "ba0182c8-1918-42a3-fb87-00fc17e52f02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1000 rows affected."
            ],
            "text/html": [
              "<span style=\"color: green\">1000 rows affected.</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "++\n",
              "||\n",
              "++\n",
              "++"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In SQLite, you have to join by using a subquery in the `UPDATE table SET` syntax. But in MySQL you can simply:\n",
        "\n",
        "```\n",
        "üê¨üê¨üê¨\n",
        "UPDATE outcomes AS a, {{meta_1gram_user}} AS b\n",
        "SET wordcount = group_norm\n",
        "WHERE a.user_id = b.group_id AND feat = \"_total1grams\";\n",
        "üê¨üê¨üê¨\n",
        "```"
      ],
      "metadata": {
        "id": "xM07sXyqfg08"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: you're altering the `outcomes` table here. If you want a fresh copy you can always get it from `csvToSQLite` (showed in mini tutorial 5B)!"
      ],
      "metadata": {
        "id": "Ql64pv5o1z_p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZGzG159fg08"
      },
      "source": [
        "Alrighty! Let's see how many users have at least 500 words in that new column.\n",
        "\n",
        "**NOTE** 500-1000 words is the rule of thumb threshold for number of words per user to get decent dictionary language variables. Less if you use dictionaries that ingest giant vocabularies (>10,000), like NRC. Those really can make every word... count."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRP8RY59fg08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "edef12cd-6dc9-4f65-b80d-eeb3159d8292"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Running query in 'sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4'"
            ],
            "text/html": [
              "<span style=\"None\">Running query in &#x27;sqlite:///sqlite_data/dla_tutorial.db?charset=utf8mb4&#x27;</span>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "+---------+\n",
              "| n_users |\n",
              "+---------+\n",
              "|   978   |\n",
              "+---------+"
            ],
            "text/html": [
              "<table>\n",
              "    <thead>\n",
              "        <tr>\n",
              "            <th>n_users</th>\n",
              "        </tr>\n",
              "    </thead>\n",
              "    <tbody>\n",
              "        <tr>\n",
              "            <td>978</td>\n",
              "        </tr>\n",
              "    </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(*) AS n_users\n",
        "FROM outcomes\n",
        "WHERE wordcount >= 500;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3YvxAIVfg09"
      },
      "source": [
        "Bingo, OK, the world makes sense. It's nice to have all this important user-level stuff in the same table, especially as we think about pulling these tables into R."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2gBVNZzfg09"
      },
      "source": [
        "## 3) Language Correlations With Feature Tables (Dictionaries)\n",
        "\n",
        "Ok, now we've spent a lot staring at and looking into feature tables. The time has come to do something with them -- we'll run our first language correlations against \"outcomes\" in the table `blog_outcomes` -- such as age and gender for users.  The DLATK command to do this will have the argument `--correlate`.\n",
        "\n",
        "The basic idea is that `--correlate` correlates:\n",
        "- **Language** in a feature table specified with `--feat_table`\n",
        "- **Outcomes** in the outcome table specified with `--outcome_table`. Within the outcome table, which can have many columns. We specify column names/variables we wish to designate as **outcomes** (to correlate against) with `--outcomes`\n",
        "\n",
        "We'll give it a few extra flags to give output of a few different kinds. Following are all the arguments we need:\n",
        "- `--feat_table` -- Feature table with language features. This can be any one of the feature tables we have extracted so far including -- 1grams, LIWC, labmt, NRC\n",
        "- `--outcome_table` -- Table where our extra-linquistic data lives. For us this is the _blog_outcomes_ table.\n",
        "- `--outcomes` -- Column names within outcome table (specified in `--outcome_table`) for variables we want to correlate language with. This is age, gender, sign etc for us as per our `blog_outcomes` table.\n",
        "- `--rmatrix` -- Produces a correlation matrix in HTML format with color coding so it is easier to read.\n",
        "- `--csv` -- Produces a correlation matrix in csv format so we can open it up in excel, R, Python etc.\n",
        "- `--sort` -- Appends a table to the HTML or csv with correlations sorted by effect size.\n",
        "- `--output_name` Specifies what the output files should be called. Let us start the name of the output with the feature table so we can run multiple times with different feature tables.\n",
        "\n",
        "this is a good output_name convention: {feature-type}\\_{outcomes}\\_CONTROLS\\_{controls-if-any}\n",
        "\n",
        "finally, from now on, let us ALWAYS specify the group_freq_thresh explicitly -- it will reduce the odds that we forget, ever, what it is set to, and that that's appropriate for what we are doing.\n",
        "\n",
        "- `--group_freq_thresh` 500 -- limit to 500 words or more per group.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Lets run the correlations for `mini_LIWC2015` with these arguments. But before that, let's create a folder for all the outputs that we'll generate in this tutorial. (Then you can download the folder or save it in Google Drive!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uo8t7Z1Bfg09"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "outcomes_table = 'outcomes'\n",
        "feat_miniliwc_user = 'feat$cat_mini_LIWC2015$msgs$user_id$1gra'\n",
        "\n",
        "OUTPUT_FOLDER = './outputs_tutorial_06'\n",
        "OUTPUT_NAME = 'mini_liwc_age_gender'\n",
        "!mkdir -p {OUTPUT_FOLDER} # this makes a folder, the -p does it even if it exists already"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b77MFligfg09"
      },
      "source": [
        "**Note:** make sure to put `'` quotes around your feat table name, even if you insert it cleverly with a Python variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYMMxZcEfg09"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --correlate \\\n",
        "    --rmatrix --csv --sort \\\n",
        "    --feat_table '{feat_miniliwc_user}' \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --outcomes age gender \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVybk1Rnfg09"
      },
      "source": [
        "Please scroll through the output and confirm that you see the row that tells you that `978` groups were retained -- it's the same `978` that we manually checked above.\n",
        "\n",
        "‚ö†Ô∏è It is wise to look for the **number of groups** on every DLATK run. It will save you countless headaches!\n",
        "\n",
        "The above command produced files in the home directory. All of them are prefixed by `mini_liwc_age_gender` which we specified in the command with `--output_name`. Let us look at what we have. The `*` is a wild-card that allows us to print any file or directory that begins with `mini_liwc_age_gender`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gEFXiRQpfg09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b51b7cf-8a7b-4454-8138-ecc6860fdbbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 7.6K Apr 17 22:54 ./outputs_tutorial_06/mini_liwc_age_gender.csv\n",
            "-rw-r--r-- 1 root root  16K Apr 17 22:54 ./outputs_tutorial_06/mini_liwc_age_gender.html\n"
          ]
        }
      ],
      "source": [
        "!ls -lh {OUTPUT_FOLDER}/{OUTPUT_NAME}*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9LRAtjYfg09"
      },
      "source": [
        "There are 2 files -- html & csv.  \n",
        "\n",
        "You can go back to the Colab file tree (on the left), click refresh the button in the tree, and download the files there.\n",
        "\n",
        "üí° Some files are small enough & formatted correctly to be opened inside Colab's file preview feature. Try double-clicking on the file and you will either get a little popout window of the csv/image/file or you'll get an error! Otherwise, download it and open on your laptop :)\n",
        "\n",
        "\n",
        "The csv file contains the same information as in the html but is readable by Excel / R. When you open the html file, it will look like this:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Fig¬†1](https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-06/fig1.png)\n"
      ],
      "metadata": {
        "id": "UQPszBJ7HhWf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDGlbFBgfg09"
      },
      "source": [
        "When you open the csv file in excel, it will look like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Fig¬†2](https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-06/fig2.png)\n"
      ],
      "metadata": {
        "id": "dgF_C1hqHusa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_Ep5nRqfg09"
      },
      "source": [
        "We see that `gender` is correlated 0.248 with `SOCIAL`, and 0.142 with `POSEMO`. As women are coded as 1, this means women use more POSEMO and SOCIAL langauge.\n",
        "\n",
        "The output also contains `p values` (already adjusted for multiple comparisons) -- in this case, for three types of features (`NEGEMO`, `POSEMO`, `SOCIAL`), `N` contains the numbers of groups this was run over (978 again!). `CI_l` is the lower end of the 95% Confidence Interval, `CI_u` the upper bound. `freq` is the total sum(value) for the dictionaries -- the total number of dictionary words seen across the 978 users.\n",
        "\n",
        "So in APA reporting, LIWC's `NEGEMO` and age are correlated at `r = -.24 [-.29, -.18], p < .001`. That's what you would report, with a note in the methods section that all the p's are corrected for multiple comparison using the Benjamini-Hochberg False Discovery Rate adjustment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnIKXVTCfg09"
      },
      "source": [
        "### 3a) Weighted Dictionary Correlations\n",
        "\n",
        "Weighted dicationary correlations are exactly the same as unweighted dictionary correlations. We just supply the feature table that was created with weighted lexicon (`--weighted_lexicon`) to the correlate command, and that's it.\n",
        "\n",
        "That's the beauty of the feature tables -- they are so nicely interchangable.\n",
        "\n",
        "Let's correlate `labmt` with `age` and `gender`. Note the `labmt` feature table below. Of course, `labmt` feature table needs to be extracted first. (Just a reminder: Before we can extract the `labmt` feature table, we have to extract the 1grams feature table, which we have already done, of course.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iH-_Yfbfg09"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "outcomes_table = 'outcomes'\n",
        "feat_labmt_user = 'feat$cat_labmt_w$msgs$user_id$1gra'\n",
        "\n",
        "OUTPUT_FOLDER = './outputs_tutorial_06'\n",
        "OUTPUT_NAME = 'labmt_age_gender'\n",
        "!mkdir -p {OUTPUT_FOLDER}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QzQMqLc_fg09"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --correlate \\\n",
        "    --rmatrix --csv --sort \\\n",
        "    --feat_table '{feat_labmt_user}' \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --outcomes age gender \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vnMGafBfg09"
      },
      "source": [
        "Similar to the previous exercise, the above command produced files in the home directory. All of them are prefixed by `labmt_age_gender` which we specified in the command with `--output_name`. Let us look at what we have. The `*` is a wild-card that allows us to print any file or directory that begins with `labmt_age_gender`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftIwJJnvfg09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3b7a68d-e325-4f9c-8b08-d7b7622097f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 6.8K Apr 17 22:55 ./outputs_tutorial_06/labmt_age_gender.csv\n",
            "-rw-r--r-- 1 root root  13K Apr 17 22:55 ./outputs_tutorial_06/labmt_age_gender.html\n"
          ]
        }
      ],
      "source": [
        "!ls -lh {OUTPUT_FOLDER}/{OUTPUT_NAME}*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkm4I5_wfg09"
      },
      "source": [
        "Open the files with your Jupyter browser and take a look!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McY2wXP8fg09"
      },
      "source": [
        "#### üë©‚Äçüî¨üíª Exercise\n",
        "\n",
        "Can you repeat this with `nrc` and take a look at the files? Remember to rename the `--output_name` accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03qi4sx8fg09"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "outcomes_table = 'outcomes'\n",
        "feat_nrc_user = 'feat$cat_nrc_w$msgs$user_id$1gra'\n",
        "\n",
        "OUTPUT_FOLDER = './outputs_tutorial_06'\n",
        "OUTPUT_NAME = 'nrc_age_gender'\n",
        "!mkdir -p {OUTPUT_FOLDER}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgmVpXM6fg09"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --correlate \\\n",
        "    --rmatrix --csv --sort \\\n",
        "    --feat_table '{feat_nrc_user}' \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --outcomes age gender \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPTUMXbnfg09"
      },
      "source": [
        "### 3b) Categorical Variables as Outcomes\n",
        "\n",
        "Categorical data is data that takes only a limited number of values. For example, if people responded to a survey about which brand of car they own, the result would be categorical (because the answers would be things like Honda, Toyota, Ford, None, etc.). Responses fall into a fixed set of categories.  \n",
        "\n",
        "[One-hot encoding](https://en.wikipedia.org/wiki/One-hot) is a widespread approach to encoding categorical variables when the number of possible values of the categorical variable is small enough.\n",
        "\n",
        "For example, let us say our categorical variable is _color_ and it takes only 3 values -- red, blue, green. Our one-hot encoding for them can look like:\n",
        "\n",
        "| color | encoding | isRed | isBlue | isGreen |\n",
        "| ----- | -------- | --------- | --------- | --------- |\n",
        "| red   | 1,0,0 | 1 | 0 | 0 |\n",
        "| blue  | 0,1,0 | 0 | 1 | 0 |\n",
        "| green | 0,0,1 | 0 | 0 | 1 |\n",
        "\n",
        "\n",
        "Things to note:\n",
        "- This is a binary encoding -- there are only 1s and 0s in the encoding.\n",
        "- There is always just one binary 1 in the encoding and rest are zero.\n",
        "- Position of the binary 1 in the encoding tells us the category it maps to. This mapping is decided by us.\n",
        "- The length of our encoding, in characters, is equal to the number of values our variable will take. In this case 3.\n",
        "\n",
        "**This is the important bit:** DLATK will use one-hot encoding for us if we specify `--categories_to_binary`. The results will automatically be in terms of our categories, which is very convenient.  \n",
        "\n",
        "Let's say we want to correlate \"positive emotions against the different occupations\" -- answering the question if one occupation uses more POSEMO langauge than the others.\n",
        "\n",
        "Let us use the categorical column `occu` (occupation) from `blog_outcomes` to correlate with langauge we extracted with _mini_LIWC2015_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfOKQjYdfg0-"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "outcomes_table = 'outcomes'\n",
        "feat_miniliwc_user = 'feat$cat_mini_LIWC2015$msgs$user_id$1gra'\n",
        "\n",
        "OUTPUT_FOLDER = './outputs_tutorial_06'\n",
        "OUTPUT_NAME = 'mini_liwc_occu'\n",
        "!mkdir -p {OUTPUT_FOLDER}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKanJnxTfg0-"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --correlate \\\n",
        "    --rmatrix --csv --sort \\\n",
        "    --feat_table '{feat_miniliwc_user}' \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --categories_to_binary occu \\\n",
        "    --outcomes occu \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4ZfE7lafg0-"
      },
      "source": [
        "The above command produced files in the output directory. All of them are prefixed by `mini_liwc_occu` which we specified in the command with `--output_name`. Let us look at what we have. The `*` is a wild-card that allows us to print any file or directory that begins with `mini_liwc_occu`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgXkavUXfg0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7c5e3a8-7083-435b-a2d4-081c07346ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root  33K Apr 17 22:58 ./outputs_tutorial_06/mini_liwc_occu.csv\n",
            "-rw-r--r-- 1 root root 151K Apr 17 22:58 ./outputs_tutorial_06/mini_liwc_occu.html\n"
          ]
        }
      ],
      "source": [
        "!ls -lh {OUTPUT_FOLDER}/{OUTPUT_NAME}*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VjaGCBKfg0-"
      },
      "source": [
        "#### üë©‚Äçüî¨üíª Exercise\n",
        "\n",
        "Look at the output html file (see it in the `outputs_tutorial_06` folder). Do students express more Negative Emotion? üò∂üò∂üò∂"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hl2-uYOIfg0-"
      },
      "source": [
        "## 4) Statistical (Covariate) Controls\n",
        "\n",
        "As you know from your statsy courses, often we would like to know the correlation for our variables of interest in the presence of other variables -- to isolate the effect of some outcome variable adjusting for some control variable (alternative phrases: controlling for, explaining over and above).\n",
        "\n",
        "In DLATK, we specify statistical controls the argument `--controls`. The columns are also expected in the `--outcome_table` (there is no additional `--control_table` expected or anything like that).   \n",
        "\n",
        "For example we can run mini_liwc correlations for `age` controlling for `gender`, which means we are interested in `age` but while controlling for `gender`.\n",
        "\n",
        "**NOTE:** Given that gender has the strongest language associations of all variables, we almost always want to control for gender."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdtFuAW0fg0-"
      },
      "outputs": [],
      "source": [
        "database = \"dla_tutorial\"\n",
        "msgs_table = 'msgs'\n",
        "outcomes_table = 'outcomes'\n",
        "feat_miniliwc_user = 'feat$cat_mini_LIWC2015$msgs$user_id$1gra'\n",
        "\n",
        "OUTPUT_FOLDER = './outputs_tutorial_06'\n",
        "OUTPUT_NAME = 'mini_liwc_age_CTRL_gender'\n",
        "!mkdir -p {OUTPUT_FOLDER}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p38o15-5fg0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046ba278-2e62-4e44-c2a4-b1d14e3933af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TopicExtractor: gensim Mallet wrapper unavailable, using Mallet directly.\n",
            "\n",
            "-----\n",
            "DLATK Interface Initiated: 2025-04-17 23:00:57\n",
            "-----\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "WARNING: The table outcomes does not have: a PRIMARY key on user_id. Consider adding.\n",
            "Loading Outcomes and Getting Groups for: {'age', 'gender'}\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "Yielding data over ['age'], adjusting for: ['gender'].\n",
            "Yielding norms with zeros (978 groups * 4 feats).\n",
            "                                 OLS Regression Results                                \n",
            "=======================================================================================\n",
            "Dep. Variable:                    age   R-squared (uncentered):                   0.000\n",
            "Model:                            OLS   Adj. R-squared (uncentered):             -0.001\n",
            "Method:                 Least Squares   F-statistic:                             0.4216\n",
            "Date:                Thu, 17 Apr 2025   Prob (F-statistic):                       0.516\n",
            "Time:                        23:00:58   Log-Likelihood:                         -1387.5\n",
            "No. Observations:                 978   AIC:                                      2777.\n",
            "Df Residuals:                     977   BIC:                                      2782.\n",
            "Df Model:                           1                                                  \n",
            "Covariance Type:            nonrobust                                                  \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "gender         0.0208      0.032      0.649      0.516      -0.042       0.084\n",
            "==============================================================================\n",
            "Omnibus:                      112.898   Durbin-Watson:                   2.044\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.154\n",
            "Skew:                           0.947   Prob(JB):                     1.50e-33\n",
            "Kurtosis:                       3.353   Cond. No.                         1.00\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] R¬≤ is computed without centering (uncentered) since the model does not contain a constant.\n",
            "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "OLS threw ValueError: [exog contains inf or nans]\n",
            " feature '_intercept' with outcome 'age' results not included\n",
            "unable to correlate feature '_intercept' with 'age'\n",
            "Namespace(lexinterface=False, toinitfile=None, frominitfile='', mysqlconfigfile='', dbengine='sqlite', corpdb='dla_tutorial', corptable='msgs', correl_field='user_id', message_field='message', messageid_field='message_id', date_field='updated_time', lexicondb='dlatk_lexica', encoding='utf8mb4', useunicode=True, language='en', feattable='feat$cat_mini_LIWC2015$msgs$user_id$1gra', n=[1], metafeats=True, lextable='', wordTable=None, colloc_table='test_collocs', colloc_column='feat', feature_type_name=None, gzipcsv='', categories='', feat_blacklist='', feat_whitelist='', valuefunc=<function main.<locals>.<lambda> at 0xf69286e0180>, lexvaluefunc=<function main.<locals>.<lambda> at 0xf69286e02c0>, pocc=0.01, pmi=3.0, minfeatsum=0, topicfile='', numtopicwords=15, topiclexicon='', topiclist='', topiclexmethod='csv_lik', weightedlexicon=False, numbins=None, flexiplotfile='', groupidrange=None, masktable=None, embmodel='bert-base-uncased', embclass=None, tokenizermodel=None, embaggs=['mean'], emblayeraggs=['concatenate'], transwordaggs=['mean'], emblayers=[-2], embnocontext=False, embtablename=None, batchsize=32, embkeepmsg=False, listfeattables=False, showtables=False, describetables=False, viewtables=False, createrandsample=None, createcopiedtable=None, extension=None, top_messages=False, outcometable='outcomes', outcomefields=['age'], outcomecontrols=['gender'], cattobinfields=[], cattointfields=[], outcomeinteraction=[], fold_column=None, test_folds=None, featnames=['honor'], groupfreqthresh=500, outputdir='.', outputname='./outputs_tutorial_06/mini_liwc_age_CTRL_gender', maxtcwords=100, showfeatfreqs=True, tcfilter=True, featlabelmaptable='', featlabelmaplex='', bracketlabels='', compTagcloud=False, compTCsample1=[], compTCsample2=[], csv=True, pickle=False, sort=True, whitelist=False, blacklist=False, spearman=False, logisticReg=False, cohensd=False, IDP=False, auc=False, zScoreGroup=False, p_correction_method='BH', bonferroni=True, nvalue=True, confint=True, freq=True, tagcloudcolorscheme='multi', cleancloud=False, weightedsample='', low_variance_thresh=0.0, minabseffectsize=0.0, interactions=False, bootstrapp=0, maxP=0.05, groupswhere='', mediation=False, mediationboot=False, mediationbootnum=1000, outcomepathstarts=[], outcomemediators=[], feat_as_path_start=False, feat_as_outcome=False, feat_as_control=False, no_features=False, mediationcsv=False, mediationsummary=True, mediation_style='baron', adapttable=None, adaptcolumns=None, model='ridgecv', turn_off_backoff_model=False, combmodels=['ridgecv'], sparse=False, folds=5, outlier_to_mean=False, picklefile='', allcontrolsonly=True, nolang=False, controlcombosizes=[], res_controls=False, pred_csv=False, prob_csv=False, weightedeval=None, standardize=True, featureselection='', featureselectionstring='', train_bootstraps=None, train_bootstraps_ns=None, addngrams=False, addngramsfromtok=False, use_collocs=False, include_sub_collocs=False, colloc_pmi_thresh=3.0, addcharngrams=False, lowercaseonly=True, addlextable=False, addcorplextable=False, addphrasetable=False, addpostable=False, pos_ngram=False, addldafeattable=None, printtokenizedlines=None, printjoinedfeaturelines=None, addtopiclexfromtopicfile=False, addtimexdiff=False, addpostimexdiff=False, addwnnopos=False, addwnpos=False, addfkscore=False, addpnames=None, embaddfeat=False, lexicon_normalization=False, multicategory_normalization=False, addtokenized=False, addsenttokenized=False, addsentperrow=False, addparses=False, addsegmented=False, segmentationModel='ctb', addtweettok=False, addtweetpos=False, addldamsgs=None, addoutcomefeats=False, langfilter=[], lightenglishfilter=False, cleanmessages=False, deduplicate=False, spamfilter=None, addmessageid=None, ldamsgtbl='messages_en_lda$msgs_en_tok_a30', createdists=False, addner=False, ttestfeats=False, featoccfilter=False, combinefeattables=None, addfeatnorms=False, addstdfeats=False, featcollocfilter=False, createcollocscores=False, featcorrelfilter=False, maketopiclabelmap=False, tfidf=False, featgroupoutcomes=False, aggregategroup=False, archetypesimilarity=None, interpolategroup=None, printcsv=None, printfreqcsv=None, printnumgroups=False, densifytable=None, correlate=True, rmatrix=True, combormatrix=False, topicdupefilter=False, tagcloud=False, topictc=False, topictagcloudcolorscheme='blue', corptopictc=False, makewordclouds=False, maketopicwordclouds=False, makealltopicwordclouds=False, keepduplicates=False, useFeatTableFeats=False, outcomeWithOutcome=False, outcomeWithOutcomeOnly=False, outputInteractionTerms=False, interactionDdla=None, ddlaSignificance=0.001, ddlaFiles=None, ddlaTagcloud=False, multir=False, trainregression=False, testregression=False, combotestregression=False, predictregression=False, controladjustreg=False, testcombregression=[], predictrtofeats=None, predictRtoOutcomeTable=None, predictalltofeats=None, fsparams=False, kbest=None, pcacomp=None, adaptationfactors=None, factorselectiontype='rfe', numoffactors=None, pairedfactors=False, report=False, factoraddition=False, factoradaptation=False, residualizedfactoradaptation=False, trainclassifiers=False, testclassifiers=False, combotestclassifiers=False, predictclassifiers=False, roc=False, predictctofeats=None, predictprobstofeats=None, predictCtoOutcomeTable=None, regrToLex=None, classToLex=None, stratifyfolds=False, prefixsupergroups=False, trainclasstoreg=False, testclasstoreg=False, predictclasstoreg=False, reducertolexicon=None, supertopics=None, reducedlexicon=None, fitreducer=False, n_components=None, transformdrtofeats=None, cca=0, penaltyFeats=None, penaltyOutcomes=None, ccaOutcomesVsControls=False, ccaPermute=0, predictCcaCompsFromModel=False, newSQLtable=None, usexfeats=False, usexcontrols=False, savemodels=False, loadmodels=False, barplot=False, scatterplot=False, featflexibin=False, skipbinstep=False, preservebintable=False, descplot=False, loessplot='', estimate_lda_topics=False, mallet_path='/opt/mallet/bin/mallet', save_lda_files=None, lda_lexicon_name=None, no_lda_lexicon=False, num_topics=100, num_lda_threads=4, num_stopwords=50, extra_lda_stopwords=None, no_lda_stopping=False, lda_alpha=5.0, lda_beta=0.01, lda_iterations=1000, integrationmethod='', featureselectionparams=None)\n",
            "\n",
            "age:\n",
            "[('NEGEMO',\n",
            "  (np.float64(-0.23531636112941576), np.float64(3.6808055843603076e-13), 978,\n",
            "   (...), 140323)),\n",
            " ('POSEMO',\n",
            "  (np.float64(-0.18683153475807607), np.float64(1.1211909573023429e-08), 978,\n",
            "   (...), 232923)),\n",
            " ('SOCIAL',\n",
            "  (np.float64(-0.05827335614359282), np.float64(0.10332209421947297), 978,\n",
            "   (...), 595011)),\n",
            " ('_intercept', (nan, np.float64(1.0), 978, (...), 0))]\n",
            "\n",
            "2 features significant at p < 0.05\n",
            "Generating Correlation Matrix.\n",
            " print to file: ./outputs_tutorial_06/mini_liwc_age_CTRL_gender.html\n",
            "Generating Correlation Matrix.\n",
            " print to file: ./outputs_tutorial_06/mini_liwc_age_CTRL_gender.csv\n",
            "-------\n",
            "Settings:\n",
            "\n",
            "Database - dla_tutorial\n",
            "Corpus - msgs\n",
            "Group ID - user_id\n",
            "Feature table(s) - feat$cat_mini_LIWC2015$msgs$user_id$1gra\n",
            "Outcome table - outcomes\n",
            "Outcome(s) - age\n",
            "Control(s) - gender\n",
            "-------\n",
            "Interface Runtime: 0.63 seconds\n",
            "DLATK exits with success! A good day indeed  ¬Ø\\_(„ÉÑ)_/¬Ø.\n"
          ]
        }
      ],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --correlate \\\n",
        "    --rmatrix --csv --sort \\\n",
        "    --feat_table '{feat_miniliwc_user}' \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --outcomes age \\\n",
        "    --controls gender \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO5YnhEBfg0-"
      },
      "source": [
        "Because we are using covariate controls the DLATK output got a little fancier, as we're calling on regression packages in the background. You can largely ignore this.\n",
        "\n",
        "The above command produced files in the output directory. All of them are prefixed by `mini_liwc_age_CTRL_gender` which we specified in the command with `--output_name`. Let us look at what we have. `*` wild-card allows us to print any file or directory that begins with `mini_liwc_age_CTRL_gender`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdt9HG6gfg0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24e22c98-7084-4f3c-f0cc-f394f29dfd76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 6.9K Apr 17 23:00 ./outputs_tutorial_06/mini_liwc_age_CTRL_gender.csv\n",
            "-rw-r--r-- 1 root root  12K Apr 17 23:00 ./outputs_tutorial_06/mini_liwc_age_CTRL_gender.html\n"
          ]
        }
      ],
      "source": [
        "!ls -lh {OUTPUT_FOLDER}/{OUTPUT_NAME}*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVwwV1-Nfg0-"
      },
      "source": [
        "When you open the html file, it will look like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Fig¬†3](https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-06/fig3.png)\n"
      ],
      "metadata": {
        "id": "Ty3SpQGZH2K1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUc-uJvTfg0-"
      },
      "source": [
        "And the csv file looks like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Fig¬†4](https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-06/fig4.png)\n"
      ],
      "metadata": {
        "id": "zmcuid2LH86l"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMRvbJl7fg0-"
      },
      "source": [
        "So this tells us that older users' blog posts contain less POSEMO and NEGEMO words -- the langauge gets less emotional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mKXtpu1fg0-"
      },
      "source": [
        "### 4a) LabMT -- correlate `age`, controlling `gender`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üë©‚Äçüî¨üíª Exercise\n",
        "\n",
        "Can you repeat the above process for `labmt` now? Note the `--controls gender` in the command, and also that gender is no longer among the `--outcomes`. Remember to rename the `output_name` appropriately. What's the trend with labMT for age, controlling for gender? Does the valence go up or down with age?"
      ],
      "metadata": {
        "id": "ZnFAobCTiwMN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWJ6h_o9fg0-"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "outcomes_table = 'outcomes'\n",
        "feat_labmt_user = 'feat$cat_labmt_w$msgs$user_id$1gra'\n",
        "\n",
        "OUTPUT_FOLDER = './outputs_tutorial_06'\n",
        "OUTPUT_NAME = 'labmt_age_CTRL_gender'\n",
        "!mkdir -p {OUTPUT_FOLDER}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_nzc80zfg0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e70737c0-4552-444c-b3d1-548cc30b48c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "TopicExtractor: gensim Mallet wrapper unavailable, using Mallet directly.\n",
            "\n",
            "-----\n",
            "DLATK Interface Initiated: 2025-04-17 23:02:26\n",
            "-----\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "WARNING: The table outcomes does not have: a PRIMARY key on user_id. Consider adding.\n",
            "Loading Outcomes and Getting Groups for: {'gender', 'age'}\n",
            "Connecting to SQLite database: /content/sqlite_data/dla_tutorial\n",
            "Yielding data over ['age'], adjusting for: ['gender'].\n",
            "Yielding norms with zeros (978 groups * 2 feats).\n",
            "                                 OLS Regression Results                                \n",
            "=======================================================================================\n",
            "Dep. Variable:                    age   R-squared (uncentered):                   0.000\n",
            "Model:                            OLS   Adj. R-squared (uncentered):             -0.001\n",
            "Method:                 Least Squares   F-statistic:                             0.4216\n",
            "Date:                Thu, 17 Apr 2025   Prob (F-statistic):                       0.516\n",
            "Time:                        23:02:26   Log-Likelihood:                         -1387.5\n",
            "No. Observations:                 978   AIC:                                      2777.\n",
            "Df Residuals:                     977   BIC:                                      2782.\n",
            "Df Model:                           1                                                  \n",
            "Covariance Type:            nonrobust                                                  \n",
            "==============================================================================\n",
            "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
            "------------------------------------------------------------------------------\n",
            "gender         0.0208      0.032      0.649      0.516      -0.042       0.084\n",
            "==============================================================================\n",
            "Omnibus:                      112.898   Durbin-Watson:                   2.044\n",
            "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              151.154\n",
            "Skew:                           0.947   Prob(JB):                     1.50e-33\n",
            "Kurtosis:                       3.353   Cond. No.                         1.00\n",
            "==============================================================================\n",
            "\n",
            "Notes:\n",
            "[1] R¬≤ is computed without centering (uncentered) since the model does not contain a constant.\n",
            "[2] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
            "OLS threw ValueError: [exog contains inf or nans]\n",
            " feature '_intercept' with outcome 'age' results not included\n",
            "Namespace(lexinterface=False, toinitfile=None, frominitfile='', mysqlconfigfile='', dbengine='sqlite', corpdb='dla_tutorial', corptable='msgs', correl_field='user_id', message_field='message', messageid_field='message_id', date_field='updated_time', lexicondb='dlatk_lexica', encoding='utf8mb4', useunicode=True, language='en', feattable='feat$cat_labmt_w$msgs$user_id$1gra', n=[1], metafeats=True, lextable='', wordTable=None, colloc_table='test_collocs', colloc_column='feat', feature_type_name=None, gzipcsv='', categories='', feat_blacklist='', feat_whitelist='', valuefunc=<function main.<locals>.<lambda> at 0x11781a768180>, lexvaluefunc=<function main.<locals>.<lambda> at 0x11781a7682c0>, pocc=0.01, pmi=3.0, minfeatsum=0, topicfile='', numtopicwords=15, topiclexicon='', topiclist='', topiclexmethod='csv_lik', weightedlexicon=False, numbins=None, flexiplotfile='', groupidrange=None, masktable=None, embmodel='bert-base-uncased', embclass=None, tokenizermodel=None, embaggs=['mean'], emblayeraggs=['concatenate'], transwordaggs=['mean'], emblayers=[-2], embnocontext=False, embtablename=None, batchsize=32, embkeepmsg=False, listfeattables=False, showtables=False, describetables=False, viewtables=False, createrandsample=None, createcopiedtable=None, extension=None, top_messages=False, outcometable='outcomes', outcomefields=['age'], outcomecontrols=['gender'], cattobinfields=[], cattointfields=[], outcomeinteraction=[], fold_column=None, test_folds=None, featnames=['honor'], groupfreqthresh=500, outputdir='.', outputname='./outputs_tutorial_06/labmt_age_CTRL_gender', maxtcwords=100, showfeatfreqs=True, tcfilter=True, featlabelmaptable='', featlabelmaplex='', bracketlabels='', compTagcloud=False, compTCsample1=[], compTCsample2=[], csv=True, pickle=False, sort=True, whitelist=False, blacklist=False, spearman=False, logisticReg=False, cohensd=False, IDP=False, auc=False, zScoreGroup=False, p_correction_method='BH', bonferroni=True, nvalue=True, confint=True, freq=True, tagcloudcolorscheme='multi', cleancloud=False, weightedsample='', low_variance_thresh=0.0, minabseffectsize=0.0, interactions=False, bootstrapp=0, maxP=0.05, groupswhere='', mediation=False, mediationboot=False, mediationbootnum=1000, outcomepathstarts=[], outcomemediators=[], feat_as_path_start=False, feat_as_outcome=False, feat_as_control=False, no_features=False, mediationcsv=False, mediationsummary=True, mediation_style='baron', adapttable=None, adaptcolumns=None, model='ridgecv', turn_off_backoff_model=False, combmodels=['ridgecv'], sparse=False, folds=5, outlier_to_mean=False, picklefile='', allcontrolsonly=True, nolang=False, controlcombosizes=[], res_controls=False, pred_csv=False, prob_csv=False, weightedeval=None, standardize=True, featureselection='', featureselectionstring='', train_bootstraps=None, train_bootstraps_ns=None, addngrams=False, addngramsfromtok=False, use_collocs=False, include_sub_collocs=False, colloc_pmi_thresh=3.0, addcharngrams=False, lowercaseonly=True, addlextable=False, addcorplextable=False, addphrasetable=False, addpostable=False, pos_ngram=False, addldafeattable=None, printtokenizedlines=None, printjoinedfeaturelines=None, addtopiclexfromtopicfile=False, addtimexdiff=False, addpostimexdiff=False, addwnnopos=False, addwnpos=False, addfkscore=False, addpnames=None, embaddfeat=False, lexicon_normalization=False, multicategory_normalization=False, addtokenized=False, addsenttokenized=False, addsentperrow=False, addparses=False, addsegmented=False, segmentationModel='ctb', addtweettok=False, addtweetpos=False, addldamsgs=None, addoutcomefeats=False, langfilter=[], lightenglishfilter=False, cleanmessages=False, deduplicate=False, spamfilter=None, addmessageid=None, ldamsgtbl='messages_en_lda$msgs_en_tok_a30', createdists=False, addner=False, ttestfeats=False, featoccfilter=False, combinefeattables=None, addfeatnorms=False, addstdfeats=False, featcollocfilter=False, createcollocscores=False, featcorrelfilter=False, maketopiclabelmap=False, tfidf=False, featgroupoutcomes=False, aggregategroup=False, archetypesimilarity=None, interpolategroup=None, printcsv=None, printfreqcsv=None, printnumgroups=False, densifytable=None, correlate=True, rmatrix=True, combormatrix=False, topicdupefilter=False, tagcloud=False, topictc=False, topictagcloudcolorscheme='blue', corptopictc=False, makewordclouds=False, maketopicwordclouds=False, makealltopicwordclouds=False, keepduplicates=False, useFeatTableFeats=False, outcomeWithOutcome=False, outcomeWithOutcomeOnly=False, outputInteractionTerms=False, interactionDdla=None, ddlaSignificance=0.001, ddlaFiles=None, ddlaTagcloud=False, multir=False, trainregression=False, testregression=False, combotestregression=False, predictregression=False, controladjustreg=False, testcombregression=[], predictrtofeats=None, predictRtoOutcomeTable=None, predictalltofeats=None, fsparams=False, kbest=None, pcacomp=None, adaptationfactors=None, factorselectiontype='rfe', numoffactors=None, pairedfactors=False, report=False, factoraddition=False, factoradaptation=False, residualizedfactoradaptation=False, trainclassifiers=False, testclassifiers=False, combotestclassifiers=False, predictclassifiers=False, roc=False, predictctofeats=None, predictprobstofeats=None, predictCtoOutcomeTable=None, regrToLex=None, classToLex=None, stratifyfolds=False, prefixsupergroups=False, trainclasstoreg=False, testclasstoreg=False, predictclasstoreg=False, reducertolexicon=None, supertopics=None, reducedlexicon=None, fitreducer=False, n_components=None, transformdrtofeats=None, cca=0, penaltyFeats=None, penaltyOutcomes=None, ccaOutcomesVsControls=False, ccaPermute=0, predictCcaCompsFromModel=False, newSQLtable=None, usexfeats=False, usexcontrols=False, savemodels=False, loadmodels=False, barplot=False, scatterplot=False, featflexibin=False, skipbinstep=False, preservebintable=False, descplot=False, loessplot='', estimate_lda_topics=False, mallet_path='/opt/mallet/bin/mallet', save_lda_files=None, lda_lexicon_name=None, no_lda_lexicon=False, num_topics=100, num_lda_threads=4, num_stopwords=50, extra_lda_stopwords=None, no_lda_stopping=False, lda_alpha=5.0, lda_beta=0.01, lda_iterations=1000, integrationmethod='', featureselectionparams=None)\n",
            "\n",
            "age:\n",
            "[('valence',\n",
            "  (np.float64(-0.09960642183638473), np.float64(0.004106443664337416), 978,\n",
            "   (...), 1660990)),\n",
            " ('_intercept',\n",
            "  (np.float64(0.02076858768879442), np.float64(0.5162946538747435), 978, (...),\n",
            "   978))]\n",
            "\n",
            "1 features significant at p < 0.05\n",
            "Generating Correlation Matrix.\n",
            " print to file: ./outputs_tutorial_06/labmt_age_CTRL_gender.html\n",
            "Generating Correlation Matrix.\n",
            " print to file: ./outputs_tutorial_06/labmt_age_CTRL_gender.csv\n",
            "-------\n",
            "Settings:\n",
            "\n",
            "Database - dla_tutorial\n",
            "Corpus - msgs\n",
            "Group ID - user_id\n",
            "Feature table(s) - feat$cat_labmt_w$msgs$user_id$1gra\n",
            "Outcome table - outcomes\n",
            "Outcome(s) - age\n",
            "Control(s) - gender\n",
            "-------\n",
            "Interface Runtime: 0.52 seconds\n",
            "DLATK exits with success! A good day indeed  ¬Ø\\_(„ÉÑ)_/¬Ø.\n"
          ]
        }
      ],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --correlate \\\n",
        "    --rmatrix --csv --sort \\\n",
        "    --feat_table '{feat_labmt_user}' \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --outcomes age \\\n",
        "    --controls gender \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_sEv-Vvfg0-"
      },
      "source": [
        "Like the commands earlier, the above command produced files in the output directory, prefixed by `labmt_age_CTRL_gender` which we specified in the command with `--output_name`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvVtVPZifg0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1471533-edf2-4e4a-aff4-e515e6a17539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 6.6K Apr 17 23:02 ./outputs_tutorial_06/labmt_age_CTRL_gender.csv\n",
            "-rw-r--r-- 1 root root  10K Apr 17 23:02 ./outputs_tutorial_06/labmt_age_CTRL_gender.html\n"
          ]
        }
      ],
      "source": [
        "!ls -lh {OUTPUT_FOLDER}/{OUTPUT_NAME}*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsZjx-Yafg0-"
      },
      "source": [
        "There are 2 files -- html & csv. Look at the html file, or the console output. Valence goes down with age."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvLOYQ8Jfg0-"
      },
      "source": [
        "### 4b) **FYI: what to do if the DLATK output gets very long**\n",
        "\n",
        "Let's say you want to correlate mini_liwc against all occupations. This will work nicely with the 1-hot-encoding, but it will create endless output.\n",
        "\n",
        "At the end of the command, you can pipe (\"forward\") DLATK's output into a text file. This will make DLATK show you a lot less output in the actual console. You do this by just adding\n",
        "```\n",
        "> somefilename 2>&1\n",
        "```\n",
        "\n",
        "at the end of the DLATK command. This is a basic, command line Linux trick. Run it once like it is below, and then remove the pipe -- you can see the difference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RVDUVoKfg0-"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "outcomes_table = 'outcomes'\n",
        "feat_miniliwc_user = 'feat$cat_mini_LIWC2015$msgs$user_id$1gra'\n",
        "\n",
        "OUTPUT_FOLDER = './outputs_tutorial_06'\n",
        "OUTPUT_NAME = 'mini_liwc_occu_CTRL_gender'\n",
        "!mkdir -p {OUTPUT_FOLDER}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5jahaImfg0_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --correlate \\\n",
        "    --rmatrix --csv --sort \\\n",
        "    --feat_table '{feat_miniliwc_user}' \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --categories_to_binary occu \\\n",
        "    --outcomes occu \\\n",
        "    --controls gender \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME} > {OUTPUT_FOLDER}/logs.text 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w32KGb7Kfg0_"
      },
      "source": [
        "## 5) Unpacking dictionary correlations\n",
        "\n",
        "### How do the words within them correlate? `--whitelist` flag.\n",
        "\n",
        "We have already seen that it's important to know which words drive a dictionary. Now that we are correlating dictionaries, wouldn't it be nice to know how the words within them correlate with an outcome?\n",
        "\n",
        "How to correlate _ALL_ 1grams  we will talk about in a later tutorial. We correlate 1gram features just like we would correlate dictionaries -- just by giving DLATK a 1gram `--feat` table for its `--correlate` command.\n",
        "\n",
        "But for today, all we want to correlate is the words that are contained in a particular `category` in a `DLATK_lexica` table -- LIWC'S POSEMO, for example.\n",
        "\n",
        "We do this by adding the flags `--whitelist --lex_table 'LIWC2015' --categories 'POSEMO'` to the 1gram correlation.\n",
        "\n",
        "Let's see how the top words within POSEMO correlate with gender.\n",
        "\n",
        "Let's also pipe the output away to not suffer through DLATK telling us about all the 1grams it's correlating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoDS1JAmfg0_"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "outcomes_table = 'outcomes'\n",
        "feat_1gram_user = 'feat$1gram$msgs$user_id'\n",
        "\n",
        "OUTPUT_FOLDER = './outputs_tutorial_06'\n",
        "OUTPUT_NAME = '1gram_age_gender_FILTER_POSEMO'\n",
        "!mkdir -p {OUTPUT_FOLDER}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5mwFXOffg0_"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --correlate --csv \\\n",
        "    --feat_table '{feat_1gram_user}' \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --outcomes age gender \\\n",
        "    --whitelist --categories POSEMO --lex_table LIWC2015 \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME} > {OUTPUT_FOLDER}/logs.text 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqRo2LEIfg0_"
      },
      "source": [
        "The output in the files is long -- let's actually use the CSV's this time, and sort by the frequency, descending. That way we get the most frequent words with their correlations.  \n",
        "\n",
        "Here is what this looks like in Excel / Google Sheets / Open Office, etc:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Fig¬†5](https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-06/fig5.png)"
      ],
      "metadata": {
        "id": "X7qntjEBIFq7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQAs9hcpfg0_"
      },
      "source": [
        "Let's sort by column M descending through a filter, and add a cell formula that gives us the betas with CI's. We have shared an excel sheet with this tutorial, so you can copy the formatting and the cell formala.\n",
        "\n",
        "ü§ìü§ìü§ì Using conditional formatting that colors cells based on numerical value (as shown below) is...very pretty and makes it easy to eyeball large sets of values."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Fig¬†6](https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-06/fig6.png)"
      ],
      "metadata": {
        "id": "5NF3reGuIL2n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jms-3Cp0fg0_"
      },
      "source": [
        "And here is what this would look like in APA style for your supplement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8aRhy5UFfg0_"
      },
      "source": [
        "Table S1\n",
        "\n",
        "_The most frequent words in the LIWC positive emotion dictionary, and their association with gender_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Fig¬†7](https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-06/fig7.png)"
      ],
      "metadata": {
        "id": "wtL2zO1aIQor"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wrav06u7fg0_"
      },
      "source": [
        "As you can see, the association between LIWC and female gender is probably entirely driven through the word `love.`\n",
        "\n",
        "What are the odds??"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpFU9-oZfg0_"
      },
      "source": [
        "### Quick preview: 1gram word correlation differential wordclouds!\n",
        "\n",
        "BTW, as a preview: all you need to get the 1gram correlations as wordcloud images is to also add the flags `--tagcloud --make_wordclouds`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soWnrL76fg0_"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "outcomes_table = 'outcomes'\n",
        "feat_1gram_user = 'feat$1gram$msgs$user_id'\n",
        "\n",
        "OUTPUT_FOLDER = './outputs_tutorial_06'\n",
        "OUTPUT_NAME = '1gram_age_gender_FILTER_POSEMO'\n",
        "!mkdir -p {OUTPUT_FOLDER}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuoIuJctfg0_",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --correlate --csv \\\n",
        "    --feat_table '{feat_1gram_user}' \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --outcomes age gender \\\n",
        "    --whitelist --categories POSEMO --lex_table LIWC2015 \\\n",
        "    --tagcloud --make_wordclouds \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME} > {OUTPUT_FOLDER}/logs.txt 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "OgGi-xp3MZ4Y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SjzqUeNfg0_"
      },
      "source": [
        "You can browse the word clouds through the Colab file tree (it may take a moment to load)!\n",
        "\n",
        "üí° Double-click to open the file and a side panel on the right side of the Colab notebook will pop up with the image!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Fig¬†13](https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-06/fig13.png)"
      ],
      "metadata": {
        "id": "XrbDOH8YIVJ4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BmV6AKHfg0_"
      },
      "source": [
        "Here are all the LIWC POSEMO tokens significantly positively correlated with gender (controlling for multiple comparisons). The file name tells us that the coefficients of the tokens range from r = .12 (yay) to r = .21 (love). Words are sized by beta coefficient, colors gives frequence (from grey to blue to red being the most frequent)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Fig¬†9](https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-06/fig9.png)"
      ],
      "metadata": {
        "id": "68CVzvYMIhWO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y_eaT-GEfg0_"
      },
      "source": [
        "### Finding the messages that have the highest dictionary scores. `--top_messages` flag."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syzyKNKhfg0_"
      },
      "source": [
        "DLATK also provides a way to check which messages score highest in a dictionary/lexicon. For example, we want to find the top 5 messages for every dictionary in LIWC2015 based on their proportion. This can be done using the `--top_messages n` flag.\n",
        "\n",
        "To to do this, we need to have 1gram and then the dictionary features extracted at the **message level.**\n",
        "\n",
        "So, let's extract the 1gram and LIWC2015 features at the message level."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiSrcPSmfg0_"
      },
      "source": [
        "Message level 1grams:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFJn-p7Ffg0_"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üö®üö®üö® If you've already extracted message-level features (we did in HW3!), you don't need to extract again. This takes around 30 minutes!"
      ],
      "metadata": {
        "id": "di4osY6YDNzq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_OBmWwGfg0_"
      },
      "outputs": [],
      "source": [
        "# Read above!\n",
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field message_id \\\n",
        "    --add_ngrams -n 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auF_5oc4fg0_"
      },
      "source": [
        "Message level LIWC:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_WvVXkrfg0_"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIPlLo2Lfg1A"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field message_id \\\n",
        "    --add_lex_table -l LIWC2015"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkMDt6ojfg1A"
      },
      "source": [
        "Now that we have extracted the features, let's check the top-5 messages for every dictionary in LIWC using `--top_messages 5 --lex_table LIWC2015`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbWv6aWzfg1A"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "feat_liwc_msg = 'feat$cat_LIWC2015$msgs$message_id$1gra'\n",
        "\n",
        "OUTPUT_FOLDER = './outputs_tutorial_06'\n",
        "OUTPUT_NAME = 'liwc_5'\n",
        "!mkdir -p {OUTPUT_FOLDER}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0q-dfl0fg1A"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field message_id \\\n",
        "    --feat_table '{feat_liwc_msg}' \\\n",
        "    --top_messages 5 --lex_table LIWC2015 \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ui9kfunIfg1A"
      },
      "source": [
        "This produces a CSV file with the top-n messages in the output folder, prefixed with `liwc_5` which we specified with `--output_name`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8x_ixDTqfg1A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ba4bb0f-2e48-452c-c187-0472098017db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 65K Apr 17 23:21 ./outputs_tutorial_06/liwc_5_topmsgs.csv\n"
          ]
        }
      ],
      "source": [
        "!ls -lh {OUTPUT_FOLDER}/{OUTPUT_NAME}*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUYKjsnafg1A"
      },
      "source": [
        "Here are the top-5 messages for `POSEMO` from the CSV. And you can see why these messages have scored high on the dictionary--they are pure POSEMO. (The csv file also includes the words contained in every dictionary, as shown below)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Fig¬†10](https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-06/fig10.png)"
      ],
      "metadata": {
        "id": "WOAk1c2AImqr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stl-fLY4fg1A"
      },
      "source": [
        "Ok, this was a lot -- but this is important. We always want to know what's happening within our dictionaries.\n",
        "\n",
        "Top messages per dictionary, and top words that drive a dictionary should both go into the supplement of a language analysis paper."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuIwRHFcfg1A"
      },
      "source": [
        "## FYI: Pure Outcome Cross-Correlation with DLATK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xay7mpTOfg1A"
      },
      "source": [
        "We'll soon be in R, but just so you know, DLATK has a nifty function to make cross-correlation tables that just cross-correlate outcome columns, no language. Helpful for when you first get your data and want to just throw a cross-correlation table into your Jupyter/DLATK workflow.\n",
        "\n",
        "The argument for DLATK is:\n",
        "`--outcome_with_outcome_only` -- Says that we are ignoring language and are only looking at the outcomes.\n",
        "\n",
        "When computing outcome cross-correlation, we don't specify feature table `--feat_table`, because we don't need that to compute outcome cross-correlations. Everything else remains the same and the command looks like below. Note that we change the output location so we can keep our previous `mini_LIWC` results and not overwrite it.\n",
        "\n",
        "It's equivalent to calling `cor(df$age, df$gender)` in R.\n",
        "\n",
        "Note that this ignores group_freq_thresh -- you can see in the output that it ran over 1,000 groups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2rhzRMXfg1A"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "outcomes_table = 'outcomes'\n",
        "\n",
        "OUTPUT_FOLDER = './outputs_tutorial_06'\n",
        "OUTPUT_NAME = 'outcome_correlations_age_gender'\n",
        "!mkdir -p {OUTPUT_FOLDER}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2YJll0Bfg1A"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --outcomes age gender \\\n",
        "    --outcome_with_outcome_only \\\n",
        "    --rmatrix --csv --sort \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME} > {OUTPUT_FOLDER}/logs.txt 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Fig¬†11](https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-06/fig11.png)"
      ],
      "metadata": {
        "id": "1ZUW0GExIqEo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOdZuL1Afg1A"
      },
      "source": [
        "When you open the csv file in excel, it will look like this:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Fig¬†12](https://raw.githubusercontent.com/CompPsychology/psych290_images/main/images/tutorial-06/fig12.png)"
      ],
      "metadata": {
        "id": "vF4dysFLIumb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5xrYfKMFfg1A"
      },
      "source": [
        "We see that age and gender are correlated 0.013 (basically they don't), and that it was run over all `N = 1000` groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5A2nIcAfg1A"
      },
      "source": [
        "Ok, kewl."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ÄºÔ∏è **Save your database and/or output files** ‚ÄºÔ∏è"
      ],
      "metadata": {
        "id": "bS4qTq9zJdUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "database = 'dla_tutorial'"
      ],
      "metadata": {
        "id": "F7RlPan-KkYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# copy the database file to your Drive\n",
        "!cp -f \"sqlite_data/{database}.db\" \"/content/drive/MyDrive/sqlite_databases/\"\n",
        "\n",
        "print(f\"‚úÖ Database '{database}.db' has been copied to your Google Drive.\")"
      ],
      "metadata": {
        "id": "Ls5d7ZXwJzxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We generated a lot of output in this tutorial! Here's how you can save it to your Drive if you want to!"
      ],
      "metadata": {
        "id": "_R0h2jyyJfah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_FOLDER = './outputs_tutorial_06'"
      ],
      "metadata": {
        "id": "pZl5sNn-J655"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Copy the database file to your Drive (-r makes it copy the folder and all files/folders inside)\n",
        "!cp -f -r {OUTPUT_FOLDER} \"/content/drive/MyDrive/\"\n",
        "\n",
        "print(f\"‚úÖ '{OUTPUT_FOLDER}' has been copied to your Google Drive.\")"
      ],
      "metadata": {
        "id": "bwD74MqNJzR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yay! Done üòé"
      ],
      "metadata": {
        "id": "iriSrQXyLCiA"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}