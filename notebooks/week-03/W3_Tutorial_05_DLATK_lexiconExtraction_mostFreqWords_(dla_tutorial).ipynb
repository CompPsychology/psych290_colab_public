{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CompPsychology/psych290_colab_public/blob/main/notebooks/week-03/W3_Tutorial_05_DLATK_lexiconExtraction_mostFreqWords_(dla_tutorial).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T51xSd8noO1Z"
      },
      "source": [
        "# W3 Tutorial 5 -- Lexicon tables & feature extraction (DB: dla_tutorial) (2025-03)\n",
        "\n",
        "(c) Johannes Eichstaedt & the World Well-Being Project, 2023.\n",
        "\n",
        "✋🏻✋🏻 NOTE - You need to create a copy of this notebook before you work through it. Click on \"Save a copy in Drive\" option in the File menu, and safe it to your Google Drive.\n",
        "\n",
        "✉️🐞 If you find a bug/something doesn't work, please slack us a screenshot, or email johannes.courses@gmail.com."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Setting up Colab with DLATK and SQLite\n",
        "\n",
        "If colab asks you about this not being authored by Google, say \"Run anyway.\""
      ],
      "metadata": {
        "id": "3ywzkuSISLDv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1a to 1c) Streamlined: Setting up Colab with DLATK and your data"
      ],
      "metadata": {
        "id": "Ybc1hfhQTOtu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# assigning the database name\n",
        "database = \"dla_tutorial\""
      ],
      "metadata": {
        "id": "PUOpupo9vIWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### 1a) Install\n",
        "\n",
        "# installing DLATK and necessary packages\n",
        "!git clone -b psych290 https://github.com/dlatk/dlatk.git\n",
        "!pip install -r dlatk/install/requirements.txt\n",
        "!pip install dlatk/\n",
        "!pip install wordcloud langid jupysql\n",
        "\n",
        "########### 1b) Download data and insert into SQLite database\n",
        "\n",
        "# this download the csvs we need for this tutorial\n",
        "!git clone https://github.com/CompPsychology/psych290_data.git\n",
        "\n",
        "# load the required package -- similar to library() function in R\n",
        "import os\n",
        "from dlatk.tools.importmethods import csvToSQLite\n",
        "\n",
        "# store the complete path to the database -- sqlite_data/[database_name].db\n",
        "database_path = os.path.join(\"sqlite_data\", database)\n",
        "\n",
        "msgs = \"psych290_data/dla_tutorial/msgs.csv\"\n",
        "csvToSQLite(msgs, database_path, \"msgs\")\n",
        "\n",
        "outcomes = \"psych290_data/dla_tutorial/blog_outcomes.csv\"\n",
        "csvToSQLite(outcomes, database_path, \"outcomes\")\n",
        "\n",
        "############# 1c) Setup database connection\n",
        "\n",
        "# loads the %%sql extension\n",
        "%load_ext sql\n",
        "\n",
        "# connects the extension to the database\n",
        "from sqlalchemy import create_engine\n",
        "tutorial_db_engine =  create_engine(f\"sqlite:///sqlite_data/{database}.db?charset=utf8mb4\")\n",
        "%sql tutorial_db_engine\n",
        "\n",
        "# set the output limit to 50\n",
        "%config SqlMagic.displaylimit = 50\n",
        "\n",
        "## PRINT FINISHED\n",
        "print(\" ******* LOAD FINISHED ¯\\_(ツ)_/ *******\")"
      ],
      "metadata": {
        "id": "2Wdsn3m1os6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1d) Get **`dlatk_lexica.db`** in your Google Drive!\n"
      ],
      "metadata": {
        "id": "xrkx_SLVWvbd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some dictionaries are not stored in the GitHub download of DLATK because they are not publically shared.\n",
        "\n",
        "So, you'll have to upload a database with it to Colab when you want to work with it. We can store it in your Google Drive and access it easily.\n",
        "\n",
        "**Follow these steps** 📋 (*only need to do this once!*)\n",
        "\n",
        "1.  In your Google Drive make a folder called `sqlite_databases`\n",
        "2.  Open [this shared folder](https://drive.google.com/drive/folders/1nxX0Qf6vd1hnNX9ywqwVsvNLoYWo62zA). Please feel free to request access.\n",
        "3.  Make a copy of `dlatk_lexica.db` and put it in `sqlite_databases` in your Google Drive, so `MyDrive\\sqlite_databases`.\n",
        "\n",
        "\n",
        "⚠️ Make sure the file is `dlatk_lexica.db` and stored in `sqlite_databases` -- that's where the code below expects it."
      ],
      "metadata": {
        "id": "cVh4cXfFszPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1e) Mount Google Drive & configure the database in SQLite\n",
        "\n",
        "Now that you have the right copy of dlatk_lexica.db stored in your Google Drive, let's connect your Drive to this Colab!\n",
        "\n",
        "Google will ask you to allow this notebook to access your Drive--click yes and follow prompts to login and allow."
      ],
      "metadata": {
        "id": "x_L_7YjkjqZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Google Drive &  copy database to Colab\n",
        "\n",
        "# this connects & mounts your Google Drive to this colab space\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# this copies dlatk_lexica.db from your Google Drive to Colab\n",
        "!cp -f \"/content/drive/MyDrive/sqlite_databases/dlatk_lexica.db\" \"sqlite_data\""
      ],
      "metadata": {
        "id": "esOMHN8AW1Q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code block below enables SQLite to use `dlatk_lexica.db` and `dla_tutorial.db` in the same SQL connection!"
      ],
      "metadata": {
        "id": "9Gf5zspTEUhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# attaches the dlatk_lexica.db so tutorial_db_engine can query both databases\n",
        "\n",
        "from IPython import get_ipython\n",
        "from sqlalchemy import event\n",
        "\n",
        "# auto‑attach the lexica db whenever tutorial_db_engine connects\n",
        "@event.listens_for(tutorial_db_engine, \"connect\")\n",
        "def _attach_lexica(dbapi_conn, connection_record):\n",
        "    dbapi_conn.execute(\"ATTACH DATABASE 'sqlite_data/dlatk_lexica.db' AS dlatk_lexica;\")"
      ],
      "metadata": {
        "id": "gijQ5YuL1fG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "‼️ Note: In mini tutorial 5B, we go in depth on how we use Google Drive to store databases."
      ],
      "metadata": {
        "id": "qfR9I0awibOq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (ONLY IF NEEDED: SOFT RELOAD) **If you have a \"database lock\" problem**"
      ],
      "metadata": {
        "id": "Dw1lMui1L3ZE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, go to Runtime => Restart Session. Wait for that to complete. Your colab files will be preserved during this, including the DLATK install you did earlier.\n",
        "\n",
        "Second, execute this cell:"
      ],
      "metadata": {
        "id": "TIsqp_phnS5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you face a \"database locked\" issue, restart the session & run this cell to get set back up!\n",
        "\n",
        "database = \"dla_tutorial\"\n",
        "\n",
        "%reload_ext sql\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "tutorial_db_engine = create_engine(f\"sqlite:///sqlite_data/{database}.db?charset=utf8mb4\")\n",
        "dlatk_lexica_engine = create_engine(f\"sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4\")\n",
        "\n",
        "# set the output limit to 50\n",
        "%config SqlMagic.displaylimit = 50\n",
        "\n",
        "from IPython import get_ipython\n",
        "from sqlalchemy import event\n",
        "\n",
        "# auto‑attach the lexica db whenever tutorial_db_engine connects\n",
        "@event.listens_for(tutorial_db_engine, \"connect\")\n",
        "def _attach_lexica(dbapi_conn, connection_record):\n",
        "    dbapi_conn.execute(\"ATTACH DATABASE 'sqlite_data/dlatk_lexica.db' AS dlatk_lexica;\")\n",
        "\n",
        "%sql tutorial_db_engine"
      ],
      "metadata": {
        "id": "xS6Nd8QLL-yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Re-extract features"
      ],
      "metadata": {
        "id": "ZrRiO1t9vodS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's quickly re-extract 1grams from the `dla_tutorial` messages table!\n",
        "\n",
        "This will take 2.5ish minutes. Go make yourself a tea?"
      ],
      "metadata": {
        "id": "nxwslmVtv5wO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#database = \"dla_tutorial\" #we set that at the top! you can set it again if you like...\n",
        "msgs_table = \"msgs\""
      ],
      "metadata": {
        "id": "yytKVDGEwO6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!dlatkInterface.py \\\n",
        "  --corpdb {database} \\\n",
        "  --corptable {msgs_table} \\\n",
        "  --correl_field user_id \\\n",
        "  --add_ngrams -n 1"
      ],
      "metadata": {
        "id": "6GY-LaXEv483"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTx-YCcfoO1c"
      },
      "source": [
        "Good!\n",
        "\n",
        "Let's extract our first dictionaries (which needs the 1gram table in place -- that's why we re-extracted). But before we do that, let's learn about dictionaries (= lexicons)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzTun0PsoO1d"
      },
      "source": [
        "## 3) Getting to know dictionary (= lexicon) tables\n",
        "\n",
        "We will use the `LIWC2015` set of dictionaries, which are stored in the database `dlatk_lexica`.\n",
        "\n",
        "Now we set up a connection between SQLite and `dlatk_lexica.db`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# connection between sqlite and dlatk_lexica\n",
        "dlatk_lexica_engine = create_engine(f\"sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4\")"
      ],
      "metadata": {
        "id": "UkrnjQ1GqEcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activates dlatk_lexica db\n",
        "%sql dlatk_lexica_engine"
      ],
      "metadata": {
        "id": "EN-oghuuLlaX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "💡 The `%sql dlatk_lexica_engine` would be roughly 🐬🐬🐬`USE database` in MySQL. So, run `%sql tutorial_db_engine` to swtich back to the `dla_tutorial` database."
      ],
      "metadata": {
        "id": "lZcMsdLMqjv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "⚠️ Note: if you get an error saying \"other database already connected\", try running `%reload_ext sql`. If that doesn't work, resort to Runtime ==> Restart Session 🙂"
      ],
      "metadata": {
        "id": "ofdrfCxjns9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see all the dictionaries in `dlatk_lexica` run this:"
      ],
      "metadata": {
        "id": "UUG5mfDfL_tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sqlcmd tables"
      ],
      "metadata": {
        "id": "TiXGu7hfqHwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what's in the LIWC2015 table first!! That's a new table type we should know about."
      ],
      "metadata": {
        "id": "hntt1A37tBo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%sql PRAGMA table_info(LIWC2015)"
      ],
      "metadata": {
        "id": "FI0UcZCWyMbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vb-l9B9oO1d"
      },
      "source": [
        "In 🐬🐬🐬 MySQL it looks more confusing, as the category column is an `enum` data type. It just means that category can be one of the values from the list. It's a glorified `VARCHAR`. Let's look into the table now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdNfoiZDoO1d"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT *\n",
        "FROM LIWC2015\n",
        "LIMIT 10;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDMLLOxBoO1d"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT term, COUNT(*) as occ\n",
        "FROM LIWC2015\n",
        "GROUP BY term\n",
        "ORDER BY occ DESC\n",
        "LIMIT 20;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_yu4yE0oO1e"
      },
      "source": [
        "Ah, Ok, we are getting a sense. so it lists `terms` that are in `category`s, and these terms can have `weight`s (which are all 1 for LIWC -- it's an unweighted dictionary)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0JAgaJ7oO1e"
      },
      "source": [
        "#### 👩‍🔬💻 Exercise\n",
        "\n",
        "So, can you write the command to check the number of words in each category?\n",
        "\n",
        "⚠️ If you're using the `dla_tutorial` database, make sure to give the location of the LIWC2015 table specifically as residing in `dlatk_lexica` (e.g., `SELECT * FROM dlatk_lexica.LIWC2015`). Otherwise you have to switch into the dlatk_lexica database by running `%sql dlatk_lexica_engine` (🐬🐬🐬 `use dlatk_lexica` in MySQL)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BvJEaNOoO1e"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Gavg0YIoO1e"
      },
      "source": [
        "Nice!! That's looking like a good summary. Alright! Let's look at what's in the `POSEMO` dictionary. It says below there are 622 words in there.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVlJhh5GoO1e"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(*)\n",
        "FROM LIWC2015\n",
        "WHERE category = \"POSEMO\";"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAiualNZoO1e"
      },
      "source": [
        "Let's get a few random samples by executing the below query a few times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI8Vw9ccoO1e"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT *\n",
        "FROM LIWC2015\n",
        "WHERE category = \"POSEMO\"\n",
        "ORDER BY RANDOM()\n",
        "LIMIT 10;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zm-buTKOoO1f"
      },
      "source": [
        "Alrighty! 622 words, some of those stemmed (with `*` asterixes). DLATK will know what to do with it when we extract the dictionary -- it will match all the words that start with this stem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pi8FsaoHoO1f"
      },
      "source": [
        "#### 👩‍🔬💻 Exercise\n",
        "\n",
        "Also, let's check some terms are mentioned in more than one dictionary category.\n",
        "\n",
        "⚠️ Make sure to use right database!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9_w40gZoO1f"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10xqldjvoO1f"
      },
      "source": [
        "Alright, seem's like some pronouns (+ stems) are in multiple dictionaries. But let's see what categories the word `perfected` (which is a content word) is in."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtgSNGwdoO1f"
      },
      "source": [
        "#### 👩‍🔬💻 Exercise\n",
        "\n",
        "Can you write the command to check the categories that `perfected` is in?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jULikazVoO1f"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sN5Gw4ZoO1f"
      },
      "source": [
        "Interesting...\n",
        "\n",
        "This tells us something about LIWC -- the same word can show up all over the place.\n",
        "\n",
        "FYI, some of these categories are nested: every word in `POSEMO` is also in the `AFFECT` dictionary, all `ACHIEVE` words are in `DRIVE`, etc. (see page 4 here in the [LIWC manual](https://repositories.lib.utexas.edu/bitstream/handle/2152/31333/LIWC2015_LanguageManual.pdf)).\n",
        "\n",
        "While we are at it, let's look at the word \"love\", which does all sorts of damage when measured with dictionaries [(see here for a discussion](https://static1.squarespace.com/static/53d29678e4b04e06965e9423/t/5f2ad5ec2985cb6e8e2e6548/1596642799449/PNAS-2020-subjectiveWellBeing.pdf), third page, under Highly Frequent Words)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmRJ1jY-oO1f"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT term, category\n",
        "FROM LIWC2015\n",
        "WHERE term = \"love\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwi_GJSOoO1f"
      },
      "source": [
        "Good to know where it shows up!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXq0fTUVoO1g"
      },
      "source": [
        "## 4) Dictionary extraction\n",
        "\n",
        "Alright, we've gotten a sense of how dictionary tables work. Let's extract dictionaries. Basically, here is the logic\n",
        "\n",
        "* For every user (i.e., group_id)\n",
        "    * For every dictionary\n",
        "        * Count how many words that he/she uses are in the dictionary\n",
        "        * Divide that number by the total number of words that he/she has written\n",
        "\n",
        "This will be in a feature table which will tell us, for example, that `5.3%` of the words by user `11111` are `POSEMO` words (that is, match `terms` in the POSEMO `category` (dictionary) in the LIWC2015 table (set of dictionaries)).\n",
        "\n",
        "Ok, let's tell DLATK to do this for all users with a mini LIWC2015 (`mini_LIWC2015`) set of dictionaries that's a little easier to explore. It just contains `POSEMO`, `NEGEMO` and `SOCIAL` words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "126_YuYfoO1g"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT category, COUNT(*) AS terms_in_category\n",
        "FROM mini_LIWC2015\n",
        "GROUP BY category;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQN-COQnoO1g"
      },
      "source": [
        "### 4a) DLATK command: Dictionary extraction\n",
        "\n",
        "⚠️ Before extracting dictionary features, we need to make sure that we have the 1gram feature table as dictionary extraction depends on it. Else, dictionary extraction throws an error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcjkYyYpoO1g"
      },
      "source": [
        "Other than the first three, that we always need for a DLATK command, the dictionary extraction flags are\n",
        "\n",
        "`--add_lex_table -l mini_LIWC2015`,\n",
        "\n",
        "where the string after -l gives the name of the dictionary table in the `dlatk_lexica` database we want to extract. In this case, it's `mini_LIWC2015`\n",
        "\n",
        "⚠️ note: long flags get two dashes (`--add_lex_table`), short single letter flags and parameters are given with one dash (`-l mini_LIWC2015`, or `-n 1`)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "database = \"dla_tutorial\"\n",
        "msgs_table = \"msgs\""
      ],
      "metadata": {
        "id": "o9IVsm3gvEo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0jK799OoO1g"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field user_id \\\n",
        "    --add_lex_table -l mini_LIWC2015"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmBhYehKoO1g"
      },
      "source": [
        "That was quick. So first of all, let's look at the new feature table that was created here:\n",
        "\n",
        "`feat$cat_mini_LIWC2015$msgs$user_id$1gra`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LruGL9wXoO1g"
      },
      "source": [
        "## 5) Understanding dictionary feature tables\n",
        "\n",
        "Let's look at the columns in the feature table `feat$cat_mini_LIWC2015$msgs$user_id$1gra`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we point our handy `%sql` extension to the dla_tutorial database again.\n",
        "\n",
        "⚠️ Remember, with this database switching, you may run into an error like \"other database already being used!\", then you just `run %reload_ext sql`!"
      ],
      "metadata": {
        "id": "cE1rKv04rsL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %reload_ext sql # if needed!"
      ],
      "metadata": {
        "id": "n1F2Ygy0oXxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# switch into tutorial_db_engine (= \"dla_tutorial.db\")\n",
        "%sql tutorial_db_engine"
      ],
      "metadata": {
        "id": "3SRBE1wWrzGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Lmfja4DoO1g"
      },
      "outputs": [],
      "source": [
        "feat_mini_liwc_usr = 'feat$cat_mini_LIWC2015$msgs$user_id$1gra'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "\n",
        "PRAGMA table_info( {{feat_mini_liwc_usr}} )"
      ],
      "metadata": {
        "id": "6bz_E7qNvZWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "🐬🐬🐬\n",
        "SHOW COLUMNS FROM {feat_mini_liwc_usr};\n",
        "🐬🐬🐬\n",
        "```"
      ],
      "metadata": {
        "id": "a3wxFGdDoO1g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQuWFUuHoO1g"
      },
      "source": [
        "Feature tables can be confusing. The thing to note about them is  \n",
        "* their columns/fields always have the same names (`id`, `group_id`, `feat`, `value`, `group_norm`).\n",
        "* the **name** of the feature table tells us how the table came about, and what exactly is in it.\n",
        "\n",
        "In the name of this table, the `cat_` prefix means that it was created with `--add_lex_table`.\n",
        "\n",
        "Let's look at the contents now. The offset command in conjunction with the limit skips the first 100 rows, and then shows you 10. It's a way to avoid always looking at the same darn 10 rows at the top of the table (instead you are now looking at rows 101-110)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNS0iXVMoO1h"
      },
      "outputs": [],
      "source": [
        "feat_mini_liwc_usr = 'feat$cat_mini_LIWC2015$msgs$user_id$1gra'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zqeOBjDoO1h"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT *\n",
        "FROM {{feat_mini_liwc_usr}}\n",
        "LIMIT 10\n",
        "OFFSET 100;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KvUvczEoO1h"
      },
      "source": [
        "In table `feat$cat_mini_LIWC2015$msgs$user_id$1gra`,\n",
        "- **group_id** -- `user_id` from `msgs`, represents a user\n",
        "- **feat** -- One of several categories (dictionaries) from our lexicon table, `mini_LIWC2015`\n",
        "- **value** -- Number of times a word in the category mentioned in `feat` is used by user in `group_id`\n",
        "- **group_norm** -- Proportion of words used by user in `group_id` that belong to category in `feat`\n",
        "\n",
        "### 5a) How the feature table name connects to its contents\n",
        "\n",
        "This is now the second time that we are encountering a feature table -- in the previous tutorial, we had a 1gram table. Now we have a dictionary table.\n",
        "\n",
        "For the dictionary table `feat$cat_mini_LIWC2015$msgs$user_id$1gra`\n",
        "\n",
        "| this column in the feature table... | ...contains this (as recorded in table name)|\n",
        "|------|------|\n",
        "| feat | **categories in _mini_LIWC2015_** |\n",
        "| group_id | user_id |\n",
        "\n",
        "Compare this to the 1gram table `feat$1gram$msgs$user_id$16to16`\n",
        "\n",
        "| this column in the feature table... | ...contains this (as recorded in table name)|\n",
        "|------|------|\n",
        "| feat | **1gram** |\n",
        "| group_id | user_id |\n",
        "\n",
        "For example, for a particular user_id:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyvdDQ4JoO1h"
      },
      "outputs": [],
      "source": [
        "feat_mini_liwc_usr = 'feat$cat_mini_LIWC2015$msgs$user_id$1gra'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkiOiUHooO1h"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT *\n",
        "FROM {{feat_mini_liwc_usr}}\n",
        "WHERE group_id = 911744;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRZ0pEIZoO1h"
      },
      "source": [
        "User `911744` has used:\n",
        "- 28 words in LIWC category `POSEMO`\n",
        "- 86 words in LIWC category `SOCIAL`\n",
        "- 8 words in LIWC category `NEGEMO`\n",
        "- `_intercept`: **you can ignore this, if you like**. It's a dummy for every `group_id` that DLATK adds to make sure that every `group_id` shows up in this table, even if a particular `group_id` did not use any words in the `feat` categories (here: POSEMO, SOCIAL, or NEGEMO). This is to take into account the sparse encoding of feature tables-- group_ids would not get feature rows for dictionaries they did not mention, and so would not appear in the table otherwise.\n",
        "\n",
        "Having this `_intercept` in here makes sure that if you run `count(distinct(group_id))` on a lexicon table,  you'll always get the total number of `group_id`'s that were in the message table.\n",
        "\n",
        "At this point, a LIWC-minded person might say this person expresses more social words than negemo words, for example (but that makes strong assumptions about the recall of the dictionaries -- that they really hit all the social and negemo expressions that are out there in the world)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITTcWmxWoO1h"
      },
      "source": [
        "#### 👩‍🔬💻 Exercise\n",
        "\n",
        "Do the group_norms (ignoring `_intercept`) sum to 1 in a lex table? Why or why not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gwNI4TjCoO1i"
      },
      "outputs": [],
      "source": [
        "feat_mini_liwc_usr = 'feat$cat_mini_LIWC2015$msgs$user_id$1gra '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-qpDaaUoO1i"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1l8Sv2uqoO1i"
      },
      "source": [
        "As shown above, the `group_norm` without `_intercept` don't sum up to one, because all 1-grams need not occur in at least one dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kekEnXqloO1i"
      },
      "outputs": [],
      "source": [
        "feat_1gram_usr = 'feat$1gram$msgs$user_id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ks4gkEZ_oO1i"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT group_id, SUM(value) AS tokens\n",
        "FROM {{feat_1gram_usr}}\n",
        "GROUP BY group_id\n",
        "ORDER BY group_id ASC;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cx-tj5y9oO1i"
      },
      "outputs": [],
      "source": [
        "feat_mini_liwc_usr = 'feat$cat_mini_LIWC2015$msgs$user_id$1gra '"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kN8lOcbMoO1i"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT group_id, SUM(value) AS tokens\n",
        "FROM {{feat_mini_liwc_usr}}\n",
        "GROUP BY group_id\n",
        "ORDER BY group_id ASC;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "So1Vc7MAoO1i"
      },
      "source": [
        "\n",
        "Let's look at users who have the largest proportions of `POSEMO`. For proportions we have to look at the\n",
        "`group_norm` field."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVHVWHgsoO1i"
      },
      "source": [
        "#### 👩‍🔬💻 Exercise\n",
        "\n",
        "Can you now guess the SQL spell to show which users have the largest proportion of LIWC `POSEMO` words?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQ8vAwQuoO1i"
      },
      "outputs": [],
      "source": [
        "feat_mini_liwc_usr = 'feat$cat_mini_LIWC2015$msgs$user_id$1gra'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCBZK0eCoO1j"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yM6fjZeAoO1j"
      },
      "source": [
        "Now, let's get the average `group_norm` for each LIWC category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_tewwWyoO1j"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT feat, AVG(group_norm)\n",
        "FROM {{feat_mini_liwc_usr}}\n",
        "GROUP BY feat\n",
        "limit 10;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IP9QecoSoO1j"
      },
      "source": [
        "`POSEMO` is a bit less than twice as frequent than `NEGEMO` overall. And `SOCIAL` is the largest.\n",
        "\n",
        "BTW, here is a way to get more statistics, all at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IIBg4U2oO1j"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT feat, AVG(group_norm), MIN(group_norm), MAX(group_norm), count(distinct(group_id)) AS included_users\n",
        "FROM {{feat_mini_liwc_usr}}\n",
        "GROUP BY feat;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSWwOQbXoO1j"
      },
      "source": [
        "it's good to include the \"included_users\" column to remind ourselves that we are doing \"dumb\" group_norm averages -- generally an acceptable strategy when it comes to dictionaries with decent coverage. Apparently there were two users who have never used a NEGEMO word in the dataset -- good for theeeeeem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_TWZ1suoO1j"
      },
      "source": [
        "## 6) Which words are in a dictionary(s), and drive its frequency?\n",
        "\n",
        "As discussed in the lectures, we need to know what's going on inside a dictionary, we can't really take it at face value. For that we have to know which words drive its occurrence.\n",
        "\n",
        "For that, we have to combine two pieces of information:\n",
        "\n",
        "* Table `feat$1gram$msgs$user_id` contains the number of occurrences of tokens (words) for users\n",
        "* Table `mini_LIWC2015` contains which words belong to a LIWC category.\n",
        "\n",
        "Let us merge the two on words (`feat` in the 1gram table, `term` in the lexicon table) so we can find out for each category, which words occurred the most.\n",
        "\n",
        "This will show us which words are dominant in a category for our corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBjRJ4oaoO1j"
      },
      "source": [
        "### 6a) Step 1: make a word count table -- counts per words\n",
        "\n",
        "The word counts in `feat$1gram$msgs$user_id` are per user (that's the group_id), and the meta_tables (if you thought of those) too report statistics *per user*. We want total counts per words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 👩‍🔬💻 Exercise\n",
        "\n",
        "Can you make a new table `word_counts` that will have those?\n"
      ],
      "metadata": {
        "id": "GlnJnyVO2km9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BiDyBeIfoO1j"
      },
      "outputs": [],
      "source": [
        "feat_1gram_usr = 'feat$1gram$msgs$user_id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LtNRaBboO1j"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ1oiVDpoO1k"
      },
      "source": [
        "Great, this gives us a table with overall word counts. Let's just check what are the most frequent ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArIKGYiYoO1k"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT *\n",
        "FROM word_counts\n",
        "ORDER BY count DESC\n",
        "LIMIT 10;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuKKOglloO1k"
      },
      "source": [
        "### 6b) Step 2: merge on the dictionary table\n",
        "\n",
        "Alright! These words make sense. Now, let's filter this table down to only the words in the `mini_LIWC2015` dict.\n",
        "\n",
        "The `a.*` below is clever -- it tells SQL to show us all columns contained in the first table (and then whatever we want from table b -- `b.*` would also be an option.) Make sure you understand the above query. `where a.word = b.term` with two tables is the same as an inner join (it's an **Implicit Inner Join** to be precise)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pqEqd4_xoO1k"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT a.*, b.category\n",
        "FROM word_counts AS a, dlatk_lexica.mini_LIWC2015 AS b\n",
        "WHERE a.word = b.term\n",
        "ORDER BY count DESC\n",
        "LIMIT 10;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1IY4CC4oO1k"
      },
      "source": [
        "🤔 that took a weebit long... I wonder why that is? do you know what we could have done to make it faster?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "\n",
        "CREATE INDEX idx_word_counts_word\n",
        "ON word_counts(word);"
      ],
      "metadata": {
        "id": "W7AC56rhxDTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "🐬🐬🐬\n",
        "ALTER TABLE word_counts ADD index (word);\n",
        "🐬🐬🐬\n",
        "```"
      ],
      "metadata": {
        "id": "FPJKUUOzoO1k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IxkTVP3oO1k"
      },
      "source": [
        "Now run the cell again that does the implicit join above the one that created the index just now! Faster?? Seeeee-- indices! Like magic, except less exciting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfSoZPTpoO1k"
      },
      "source": [
        "Let's save the join as a new table by wrapping a `create table ` around it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vcxjZqYoO1k"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "DROP TABLE IF EXISTS miniLiwc_wordcounts;\n",
        "\n",
        "CREATE TABLE miniLiwc_wordcounts AS SELECT a.*, b.category\n",
        "                                    FROM word_counts AS a, dlatk_lexica.mini_LIWC2015 AS b\n",
        "                                    WHERE a.word = b.term\n",
        "                                    ORDER BY count DESC;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7voSRpqoO1k"
      },
      "source": [
        "### 6c) Investigating different categories (sub-dictionaries) within the (dlatk_lexica) dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kypS_Ee8oO1k"
      },
      "source": [
        "Ok, cool! So now let's only look at the `POSEMO` category -- what words occur the most in it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6eCn8_-oO1l"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT *\n",
        "FROM miniLiwc_wordcounts\n",
        "WHERE category = \"POSEMO\"\n",
        "ORDER BY count DESC\n",
        "limit 10;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eegZZ1myoO1l"
      },
      "source": [
        "Turns out, `well`, `good`, `love` are all bad words in that they are highly ambiguous. [see here](https://static1.squarespace.com/static/53d29678e4b04e06965e9423/t/5f2ad5ec2985cb6e8e2e6548/1596642799449/PNAS-2020-subjectiveWellBeing.pdf)\n",
        "\n",
        "Can you mod the query above to show you the POSEMO words only used once?\n",
        "\n",
        "Let's check in on the other two as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TFrboFzxoO1l"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT *\n",
        "FROM miniLiwc_wordcounts\n",
        "WHERE category = 'NEGEMO'\n",
        "ORDER BY count DESC\n",
        "LIMIT 10;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24FEUrkBoO1l"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT *\n",
        "FROM miniLiwc_wordcounts\n",
        "WHERE category = 'SOCIAL'\n",
        "ORDER BY count DESC\n",
        "LIMIT 10;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hz6rmX6oO1l"
      },
      "source": [
        "Who would have guessed that `SOCIAL` is full of function word pronouns!? It's basically just a pronoun dictionary, the rest won't matter. Good we checked!!\n",
        "\n",
        "Bahh! Ignorance is bliss, except, of course, it isn't."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BDuM-5KoO1l"
      },
      "source": [
        "## 7) Message-level extraction: Blog posts that have POSEMO words\n",
        "\n",
        "During the lectures, we saw that to judge the precision of a dictionary, we need to look at (1) of those messages containing dictionary words (2) how many actually express the concept conveyed by the dictionary. Specifically (2)/(1) is \"precision.\"\n",
        "\n",
        "Lets find some blogs with POSEMO words in it -- so that we can look at them, with our eyes, and see if they really are positive emotion-y.\n",
        "\n",
        "For that, we again want to match the words in the POSEMO category against a feature table that contains words -- so 1grams. But now, we don't want to group blog posts from users, we want to get the words in the specific blog posts, so we can fish out blog posts that have POSEMO words in them.\n",
        "\n",
        "DLATK will come to the rescue -- through the clever use of the `correl_field` (=group-by_field) -- we will tell it to extract 1-grams with `message_id` as our `correl_field`. That will get us a `feat$1gram$msgs$message_id` feature table.\n",
        "\n",
        "Then we find `message_id` of `feat` in `feat$1gram$msgs$message_id` that belong `term` in `category` POSEMO in `mini_LIWC2015`.\n",
        "\n",
        "This will take a ~25 minutes!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVa08BaeoO1l"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "  --corpdb {database} \\\n",
        "  --corptable {msgs_table} \\\n",
        "  --correl_field message_id \\\n",
        "  --add_ngrams -n 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErtNE7_koO1l"
      },
      "source": [
        "Just to remind ourselves again --\n",
        "* all feature tables have the same columns (`id`, `group_id`, `feat`, `value`, `group_norm`).\n",
        "* the **name** of the feature table tells us how the table came about, and what exactly is in it.\n",
        "\n",
        "### 7a) A feature table for messages!\n",
        "\n",
        "This is now the third time that we are encountering a feature table.\n",
        "\n",
        "Our new table is: `feat$1gram$msgs$message_id`\n",
        "\n",
        "| this column in the feature table... | ...contains this (as recorded in table name)|\n",
        "|------|------|\n",
        "| feat | 1gram |\n",
        "| group_id | message_id |\n",
        "\n",
        "**Note that the group_id field now is no longer users, but messages (here: blog posts. That's what's in the message table).**\n",
        "\n",
        "Previously we had, just for comparison:\n",
        "\n",
        "The dictionary table `feat$cat_mini_LIWC2015$msgs$user_id$1gra`\n",
        "\n",
        "| this column in the feature table... | ...contains this (as recorded in table name)|\n",
        "|------|------|\n",
        "| feat | **categories in _mini_LIWC2015_** |\n",
        "| group_id | user_id |\n",
        "\n",
        "And the 1gram table `feat$1gram$msgs$user_id`\n",
        "\n",
        "| this column in the feature table... | ...contains this (as recorded in table name)|\n",
        "|------|------|\n",
        "| feat | **1gram** |\n",
        "| group_id | user_id |\n",
        "\n",
        "Hopefully you are getting the idea. It's an abstract data structure 🌈\n",
        "\n",
        "Let's sanity check the new table. How many messages does it contain features for?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc1FVb4NoO1l"
      },
      "outputs": [],
      "source": [
        "feat_1gram_msg = 'feat$1gram$msgs$message_id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7ykD8zCoO1l"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(DISTINCT(group_id)) AS num_msgs\n",
        "FROM {{feat_1gram_msg}};"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBtSOoQqoO1m"
      },
      "source": [
        "So yeah, it's a big table! Because it has 31,000 messages times all their 1gram counts, for a total of how many rows?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNYYHj_WoO1m"
      },
      "outputs": [],
      "source": [
        "feat_1gram_msg = 'feat$1gram$msgs$message_id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6mk-fj0oO1m"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT COUNT(*) AS num_rows\n",
        "FROM {{feat_1gram_msg}};"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t_lDqxkoO1m"
      },
      "source": [
        "3.7m rows -- we are not in Excel Land anymore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EazUQFSNoO1m"
      },
      "source": [
        "Let's keep swimming though.\n",
        "\n",
        "Now we want to create a query that pulls out the `message_id`s that have 1grams that match the `term` column for POSEMO words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G4fDhR9OoO1m"
      },
      "outputs": [],
      "source": [
        "feat_1gram_msg = 'feat$1gram$msgs$message_id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y64L2OWpoO1m"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT group_id AS message_id, feat\n",
        "FROM feat$1gram$msgs$message_id\n",
        "WHERE feat IN (SELECT term\n",
        "               FROM dlatk_lexica.mini_LIWC2015\n",
        "               WHERE category = 'POSEMO')\n",
        "LIMIT 10;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fP6iWoW0oO1m"
      },
      "source": [
        "So this tells us that message 2523 had the feature \"(:\" in it, which is in the POSEMO dictionary. Hopefully you get the idea. So let's make ourselves a table that preserves this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYWrQAScoO1m"
      },
      "outputs": [],
      "source": [
        "feat_1gram_msg = 'feat$1gram$msgs$message_id'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVbAhx9toO1m"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "DROP TABLE IF EXISTS posemo_blogs;\n",
        "\n",
        "CREATE TABLE posemo_blogs AS SELECT group_id AS message_id, feat\n",
        "                              FROM {{feat_1gram_msg}}\n",
        "                              WHERE feat IN (SELECT term\n",
        "                                             FROM dlatk_lexica.mini_LIWC2015\n",
        "                                             WHERE category = 'POSEMO');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He0ixB4AoO1m"
      },
      "source": [
        "and let's pull some of those messages for a random sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y4sbtofCoO1m"
      },
      "outputs": [],
      "source": [
        "%%sql\n",
        "\n",
        "SELECT a.feat AS posemo_word, b.*\n",
        "FROM posemo_blogs AS a, msgs AS b\n",
        "WHERE a.message_id = b.message_id\n",
        "ORDER BY RANDOM()\n",
        "LIMIT 3;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHTIRtt6oO1m"
      },
      "source": [
        "Now we could annotate this to see if the `POSEMO` token really designates positive emotion for a random sample."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8) ‼️ Save your database ‼️\n",
        "\n",
        "The homework assumes that you have extracted feature tables. So let's write a copy of your database to Google Drive! 📝\n",
        "\n",
        "Google will ask you to allow this notebook to access your Drive--click yes and follow prompts to login and allow!"
      ],
      "metadata": {
        "id": "e9ARUuiJwsmU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "database = \"dla_tutorial\""
      ],
      "metadata": {
        "id": "DOrFgM0VE3CQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save your database in Google Drive\n",
        "\n",
        "# mount Google Drive (if you haven't already)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# copy the database file to your Drive (`-f` forces it to write over the old database with any changes)\n",
        "!cp -f \"sqlite_data/{database}.db\" \"/content/drive/MyDrive/sqlite_databases/\"\n",
        "\n",
        "print(f\"Database has been copied to your Google Drive with success!\")"
      ],
      "metadata": {
        "id": "tfxZnJW0EP8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now your database is saved in your Google Drive! We can double check it's there by running this:"
      ],
      "metadata": {
        "id": "76d0t7iMJWki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lh \"/content/drive/MyDrive/sqlite_databases\""
      ],
      "metadata": {
        "id": "M02fKknZFLYP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "🧐 Next up, check out the mini tutorial for more practice on saving databases to Google Drive!"
      ],
      "metadata": {
        "id": "6S2NfTJF22Sk"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}