{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CompPsychology/psych290_colab_public/blob/main/notebooks/week-08/W8_Tutorial_12_The_Everything_Tutorial_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e6bbd60",
      "metadata": {
        "id": "3e6bbd60"
      },
      "source": [
        "# W8 Tutorial 12 -- The Everything Tutorial (DB: `dla_tutorial`) (2025-05)\n",
        "\n",
        "✋🏻✋🏻 NOTE - You need to create a copy of this notebook before you work through it. Click on \"Save a copy in Drive\" option in the File menu, and safe it to your Google Drive.\n",
        "\n",
        "✉️🐞 If you find a bug/something doesn't work, please slack us a screenshot, or email johannes.courses@gmail.com.\n",
        "\n",
        "(c) Johannes Eichstaedt & the World Well-Being Project, 2023.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial is designed to do the critical bits of most previous tutorials, so that you can apply them to your own dataset. It is a summary (or cheatsheet) of the important DLATK commands you need for your journey in text analysis. Thus, it's also a way to summarize the entire course. 🙂\n"
      ],
      "metadata": {
        "id": "wMGfaoGnMeIy"
      },
      "id": "wMGfaoGnMeIy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview"
      ],
      "metadata": {
        "id": "ALesYX8cX7Mj"
      },
      "id": "ALesYX8cX7Mj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tutorial mainly has 3 sections.\n",
        "\n",
        "* **Feature extraction** - which talks about both n-gram and lexicon feature extraction.\n",
        "* **Feature correlation** - where we correlate the extracted features.\n",
        "* **Topic modeling** - where we model the topics ourselves and store them as lexicons.\n",
        "\n",
        "You can access the individual sections and subsections in the table of contents on the left!\n",
        "\n",
        "\n",
        "💡 **Most important DLATK flags in each section**\n",
        "\n",
        "* *1. Feature extraction* (`--corpdb`, `--corptable`, `--correl_field`)\n",
        "\n",
        "    * *1.1. N-gram extraction* (`--add_ngrams -n`, `--feat_occ_filter --set_p_occ`, `--feat_colloc_filter --set_pmi_threshold`, `--combine_feat_tables`)\n",
        "    \n",
        "    * *1.2. Topic extraction* (`--add_lex_table -l`, `--weighted_lexicon`)\n",
        "    \n",
        "* *2. Feature correlation*\n",
        "\n",
        "    * *2.1. Correlation with tokens* (`--correlate`, `--outcomes`, `--controls`, `--categories_to_binary`, `--tagcloud --make_wordclouds`)\n",
        "    \n",
        "    * *2.2. Correlation with topic features* (`--topic_tagcloud --make_topic_wordclouds`, `--topic_lexicon`)\n",
        "    \n",
        "* *3. Topic modeling* (`--estimate_lda_topics`, `--num_stopwords`, `--num_topics`, `--lda_alpha`, `--lexicondb`, `--lda_lexicon_name`, `--make_all_topic_wordclouds`)"
      ],
      "metadata": {
        "id": "PPXEm9J2WvLg"
      },
      "id": "PPXEm9J2WvLg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's setup Colab as usual!"
      ],
      "metadata": {
        "id": "Qm7gH1SZYc0x"
      },
      "id": "Qm7gH1SZYc0x"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up Colab with DLATK and SQLite"
      ],
      "metadata": {
        "id": "eMUESGfY0CQK"
      },
      "id": "eMUESGfY0CQK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a) Install DLATK"
      ],
      "metadata": {
        "id": "zVf7Sw7zroJ5"
      },
      "id": "zVf7Sw7zroJ5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "☝🏻 Don't forget to click `Restart` if it prompts you to!"
      ],
      "metadata": {
        "id": "XuJNfnzyRXdK"
      },
      "id": "XuJNfnzyRXdK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-JeYt4fVXSR",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# installing DLATK and necessary packages\n",
        "!git clone -b psych290 https://github.com/dlatk/dlatk.git\n",
        "!pip install -r dlatk/install/requirements.txt\n",
        "!pip install dlatk/\n",
        "!pip install wordcloud langid jupysql gensim==4.3"
      ],
      "id": "3-JeYt4fVXSR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b) Mount Google Drive and copy databases"
      ],
      "metadata": {
        "id": "ORz4GM0ArvYS"
      },
      "id": "ORz4GM0ArvYS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "💡 **Uploading your own data**: if you're working with new data (CSVs), see how to upload them [with DLATK in Tutorial 5B](https://github.com/CompPsychology/psych290_colab_public/blob/main/notebooks/week-03/W3_Tutorial_05B_mini_tutorial_saving_SQLite_in_GoogleDrive_(dla_tutorial).ipynb) or [with R in Tutorial 7](https://github.com/CompPsychology/psych290_colab_public/blob/main/notebooks/week-05/W5_Tutorial_07_R_dataImport_metaTablePlots_(csv).ipynb)!\n",
        "\n",
        "For now, `dla_tutorial` should be saved in your Google Drive already 😎"
      ],
      "metadata": {
        "id": "gExftU6iZmQb"
      },
      "id": "gExftU6iZmQb"
    },
    {
      "cell_type": "code",
      "source": [
        "database = \"dla_tutorial\""
      ],
      "metadata": {
        "id": "hsoYPwaGdDKI"
      },
      "execution_count": null,
      "outputs": [],
      "id": "hsoYPwaGdDKI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive & copy to Colab\n",
        "\n",
        "# connects & mounts your Google Drive to this colab space\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# this copies dlatk_lexica.db from your Google Drive to Colab\n",
        "!cp -f \"/content/drive/MyDrive/sqlite_databases/dlatk_lexica.db\" \"sqlite_data\"\n",
        "\n",
        "# this copies {database}.db from your Google Drive to Colab\n",
        "!cp -f \"/content/drive/MyDrive/sqlite_databases/{database}.db\" \"sqlite_data\""
      ],
      "metadata": {
        "id": "I7XWXktOrzWP"
      },
      "execution_count": null,
      "outputs": [],
      "id": "I7XWXktOrzWP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c) Setup database connection"
      ],
      "metadata": {
        "id": "oCj9VppSr5Nz"
      },
      "id": "oCj9VppSr5Nz"
    },
    {
      "cell_type": "code",
      "source": [
        "# loads the %%sql extension\n",
        "%load_ext sql\n",
        "\n",
        "# connects the extension to the database - mounts both databases as engines\n",
        "from sqlalchemy import create_engine\n",
        "tutorial_db_engine = create_engine(f\"sqlite:///sqlite_data/{database}.db?charset=utf8mb4\")\n",
        "dlatk_lexica_engine = create_engine(f\"sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4\")\n",
        "\n",
        "# attaches the dlatk_lexica.db so tutorial_db_engine can query both databases\n",
        "from IPython import get_ipython\n",
        "from sqlalchemy import event\n",
        "\n",
        "# auto‑attach the lexica db whenever tutorial_db_engine connects\n",
        "@event.listens_for(tutorial_db_engine, \"connect\")\n",
        "def _attach_lexica(dbapi_conn, connection_record):\n",
        "    dbapi_conn.execute(\"ATTACH DATABASE 'sqlite_data/dlatk_lexica.db' AS dlatk_lexica;\")\n",
        "\n",
        "%sql tutorial_db_engine\n",
        "\n",
        "#set the output limit to 50\n",
        "%config SqlMagic.displaylimit = 50"
      ],
      "metadata": {
        "id": "8q73fy-7r50h"
      },
      "execution_count": null,
      "outputs": [],
      "id": "8q73fy-7r50h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### d) If you have a **\"database lock\"** problem"
      ],
      "metadata": {
        "id": "zEaWoSEh7gi5"
      },
      "id": "zEaWoSEh7gi5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you face a \"database locked\" issue:\n",
        "  1. **restart the session** (Runtime ==> Restart Session)\n",
        "  2. run this cell to get set back up!"
      ],
      "metadata": {
        "id": "mhYoe9Tu2x8j"
      },
      "id": "mhYoe9Tu2x8j"
    },
    {
      "cell_type": "code",
      "source": [
        "database = \"dla_tutorial\" # or whichever database you are working with!! (e.g., 'svitlana', 'dla_tutorial', etc)\n",
        "\n",
        "%reload_ext sql\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "tutorial_db_engine = create_engine(f\"sqlite:///sqlite_data/{database}.db?charset=utf8mb4\")\n",
        "dlatk_lexica_engine = create_engine(f\"sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4\")\n",
        "\n",
        "# set the output limit to 50\n",
        "%config SqlMagic.displaylimit = 50\n",
        "\n",
        "from IPython import get_ipython\n",
        "from sqlalchemy import event\n",
        "\n",
        "# auto‑attach the lexica db whenever tutorial_db_engine connects\n",
        "@event.listens_for(tutorial_db_engine, \"connect\")\n",
        "def _attach_lexica(dbapi_conn, connection_record):\n",
        "    dbapi_conn.execute(\"ATTACH DATABASE 'sqlite_data/dlatk_lexica.db' AS dlatk_lexica;\")\n",
        "\n",
        "%sql tutorial_db_engine"
      ],
      "metadata": {
        "id": "fvWfMDXB7gEr"
      },
      "execution_count": null,
      "outputs": [],
      "id": "fvWfMDXB7gEr"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have everything set up, let's jump right into the tutorial.\n",
        "\n"
      ],
      "metadata": {
        "id": "yLtMA32NZWqo"
      },
      "id": "yLtMA32NZWqo"
    },
    {
      "cell_type": "markdown",
      "id": "1362678e",
      "metadata": {
        "id": "1362678e"
      },
      "source": [
        "## 1) Feature extraction\n",
        "\n",
        "For any text analysis task, DLATK consumes 3 mandatory flags namely -\n",
        "\n",
        "* **`--corpdb`** - which specifies the database to work with\n",
        "* **`--corptable`** - which specifies the message table that contains the text corpus\n",
        "* **`--correl_field`** - which picks our language unit of analysis, say `user_id`, `message_id`, etc.\n",
        "\n",
        "Feature extraction (broadly) discusses representing these units of analysis as either proportion of tokens (ngram extraction) or proportion of lexicons (topic extraction or any dictionary as for that matter)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ae3fbf2",
      "metadata": {
        "id": "5ae3fbf2"
      },
      "source": [
        "When no flags are specified DLATK simply lists the flags available and their usage as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f609816c",
      "metadata": {
        "id": "f609816c"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dcf14fc",
      "metadata": {
        "id": "5dcf14fc"
      },
      "source": [
        "### 1.1. N-gram extraction <a name=\"sec1.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aGTVRV2mUW4a",
      "metadata": {
        "id": "aGTVRV2mUW4a"
      },
      "source": [
        "One method of feature extraction is to use the \"bag-of-words\" model where we represent every unit of language analysis with the token types present in them and their relative frequencies. This is done using **`--add_ngrams -n`** switch as shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e43cce",
      "metadata": {
        "id": "77e43cce"
      },
      "outputs": [],
      "source": [
        "database = \"dla_tutorial\"\n",
        "msgs_table = \"msgs\"\n",
        "correl_field = \"user_id\"\n",
        "gft = 500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ff0ae1c",
      "metadata": {
        "id": "4ff0ae1c"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field {correl_field} \\\n",
        "    --group_freq_thresh {gft} \\\n",
        "    --add_ngrams -n 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecbc222d",
      "metadata": {
        "id": "ecbc222d"
      },
      "source": [
        "Out of the extracted tokens/1-grams, not all of them are significant. Some of them are used by only a few groups and don't represent the population. So, we can remove such outliers using the flag **`--feat_occ_filter --set_p_occ`**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21051581",
      "metadata": {
        "id": "21051581"
      },
      "outputs": [],
      "source": [
        "database = \"dla_tutorial\"\n",
        "msgs_table = \"msgs\"\n",
        "correl_field = \"user_id\"\n",
        "\n",
        "feat_1gram_user = \"feat$1gram$msgs$user_id\"\n",
        "\n",
        "gft = 500\n",
        "occ_threshold = 0.07"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388a7701",
      "metadata": {
        "id": "388a7701"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field {correl_field} \\\n",
        "    --group_freq_thresh {gft} \\\n",
        "    --feat_table '{feat_1gram_user}' \\\n",
        "    --feat_occ_filter --set_p_occ {occ_threshold}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e319df9",
      "metadata": {
        "id": "2e319df9"
      },
      "source": [
        "In addition to 1-grams, we can also extract bi-grams and tri-grams using `--add_ngrams -n` and retain the significant ones among them using **`--feat_colloc_filter --set_pmi_threshold`**. We do this and combine all the resulting feature tables into one giant table using the **`--combine_feat_tables`** flag for further analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "⏰ This takes about 25 minutes!"
      ],
      "metadata": {
        "id": "eArVrWel_jxk"
      },
      "id": "eArVrWel_jxk"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fd936ca",
      "metadata": {
        "id": "2fd936ca"
      },
      "outputs": [],
      "source": [
        "database = \"dla_tutorial\"\n",
        "msgs_table = \"msgs\"\n",
        "correl_field = \"user_id\"\n",
        "\n",
        "gft = 500\n",
        "occ_threshold = 0.05\n",
        "pmi_threshold = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd8e8ec",
      "metadata": {
        "id": "abd8e8ec"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field {correl_field} \\\n",
        "    --group_freq_thresh {gft} \\\n",
        "    --add_ngrams -n 1 2 3 \\\n",
        "    --combine_feat_tables 1to3gram \\\n",
        "    --feat_occ_filter --set_p_occ {occ_threshold} \\\n",
        "    --feat_colloc_filter --set_pmi_threshold {pmi_threshold}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cf9c29e",
      "metadata": {
        "id": "9cf9c29e"
      },
      "source": [
        "### 1.2. Topic extraction <a name=\"sec1.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dedc8ccd",
      "metadata": {
        "id": "dedc8ccd"
      },
      "source": [
        "Another method or step of feature extraction is to use lexicons, either weighted ones like topics or unweighted ones like `LIWC2015`. DLATK does this using the **`--add_lex_table -l`** flags. For weighted lexicon extraction we also include **`--weighted_lexicon`**.\n",
        "\n",
        "We will demonstrate this using 500 topics modeled on Facebook posts (`fb22_all_500t_cp`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cbd2344",
      "metadata": {
        "id": "6cbd2344"
      },
      "outputs": [],
      "source": [
        "database = \"dla_tutorial\"\n",
        "msgs_table = \"msgs\"\n",
        "correl_field = \"user_id\"\n",
        "\n",
        "gft = 500\n",
        "\n",
        "topics_cp_table = \"fb22_all_500t_cp\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a4ca68c",
      "metadata": {
        "id": "7a4ca68c"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field {correl_field} \\\n",
        "    --group_freq_thresh {gft} \\\n",
        "    --add_lex_table -l {topics_cp_table} \\\n",
        "    --weighted_lexicon"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef7c3684",
      "metadata": {
        "id": "ef7c3684"
      },
      "source": [
        "## 2) Feature correlation <a name=\"sec2\"></a>\n",
        "\n",
        "Once we extract the features using the above methods, our next step is to correlate the features against the outcomes of interest. Here are the flags that we need -\n",
        "\n",
        "* **`--correlate`** - tells DLATK to run a correlation.\n",
        "* **`--outcomes`** - takes a list of outcomes to correlate against.\n",
        "\n",
        "And some optional flags like -\n",
        "\n",
        "* **`--controls`** - takes a list of variables to be controlled for.\n",
        "* **`--categories_to_binary`** - tells DLATK to one-hot encode a categorical variable.\n",
        "\n",
        "We can augment this correlation by further producing wordclouds associated with these features using the **`--tagcloud --make_wordclouds`** flag."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f7a8a0e",
      "metadata": {
        "id": "1f7a8a0e"
      },
      "source": [
        "### 2.1 Correlation with tokens <a name=\"sec2.1\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ebe56ed",
      "metadata": {
        "id": "3ebe56ed"
      },
      "source": [
        "We now correlate filtered 1gram tokens against `age` controlling for `gender`, and generate corresponding word clouds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2777dc0e",
      "metadata": {
        "id": "2777dc0e"
      },
      "outputs": [],
      "source": [
        "database = \"dla_tutorial\"\n",
        "msgs_table = \"msgs\"\n",
        "correl_field = \"user_id\"\n",
        "gft = 500\n",
        "\n",
        "feat_1gram_occ07_user = \"feat$1gram$msgs$user_id$0_07\"\n",
        "\n",
        "outcomes_table = \"outcomes\"\n",
        "\n",
        "OUTPUT_NAME = \"1grams_age_CTRL_gender\"\n",
        "OUTPUT_FOLDER = \"./outputs_tutorial_12\"\n",
        "\n",
        "!mkdir -p {OUTPUT_FOLDER}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f251a5a",
      "metadata": {
        "id": "6f251a5a"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field {correl_field} \\\n",
        "    --group_freq_thresh {gft} \\\n",
        "    --correlate \\\n",
        "    --rmatrix --csv \\\n",
        "    --feat_table '{feat_1gram_occ07_user}' \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --outcomes age \\\n",
        "    --controls gender \\\n",
        "    --categories_to_binary gender \\\n",
        "    --tagcloud --make_wordclouds \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME} > {OUTPUT_FOLDER}/{OUTPUT_NAME}_logs.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "💡 Wordclouds and correlation matrix are located in `./outputs_tutorial_12`!"
      ],
      "metadata": {
        "id": "A73m1lyVAVJt"
      },
      "id": "A73m1lyVAVJt"
    },
    {
      "cell_type": "markdown",
      "id": "8ec05ba0",
      "metadata": {
        "id": "8ec05ba0"
      },
      "source": [
        "FYI: similar to 1gram correlations we can also correlate the lexicon features. For this we just need to pass the right feature table to the **`--feat_table`** flag."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f8ad6e8",
      "metadata": {
        "id": "9f8ad6e8"
      },
      "source": [
        "### 2.2. Correlation with topic features <a name=\"sec2.2\"></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1ebd817",
      "metadata": {
        "id": "d1ebd817"
      },
      "source": [
        "We can also correlate topic features against outcomes. However to generate the wordclouds we need to pass a different set of flags namely **`--topic_tagcloud --make_topic_wordclouds`** which picks the words from topic lexicon (passed using **`--topic_lexicon`**).\n",
        "\n",
        "**FYI:** Topic lexicon takes in the frequency table, say `fb22_all_500t_freq`.\n",
        "\n",
        "We will correlate the 500 FB topics features against `age` controlling for `gender`, and also generate the wordclouds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91fbedc4",
      "metadata": {
        "id": "91fbedc4"
      },
      "outputs": [],
      "source": [
        "database = \"dla_tutorial\"\n",
        "msgs_table = \"msgs\"\n",
        "correl_field = \"user_id\"\n",
        "gft = 500\n",
        "\n",
        "feat_topics_user = \"feat$cat_fb22_all_500t_cp_w$msgs$user_id$1gra\"\n",
        "\n",
        "outcomes_table = \"outcomes\"\n",
        "\n",
        "topic_lexicon = 'fb22_all_500t_freq'\n",
        "\n",
        "OUTPUT_FOLDER = \"outputs_tutorial_12\"\n",
        "OUTPUT_NAME = \"fb500_age_CTRL_gender\"\n",
        "\n",
        "!mkdir -p {OUTPUT_FOLDER}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "600f0911",
      "metadata": {
        "id": "600f0911"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field {correl_field} \\\n",
        "    --correlate \\\n",
        "    --rmatrix --csv --sort \\\n",
        "    --group_freq_thresh 500 \\\n",
        "    --outcome_table {outcomes_table} \\\n",
        "    --outcomes age \\\n",
        "    --controls gender \\\n",
        "    --categories_to_binary gender \\\n",
        "    --feat_table '{feat_topics_user}' \\\n",
        "    --topic_tagcloud --make_topic_wordclouds \\\n",
        "    --topic_lexicon {topic_lexicon} \\\n",
        "    --tagcloud_colorscheme blue \\\n",
        "    --output_name {OUTPUT_FOLDER}/{OUTPUT_NAME} > {OUTPUT_FOLDER}/{OUTPUT_NAME}_logs.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "💡 Topic wordclouds and correlations are located in `./outputs_tutorial_12`!\n",
        "\n",
        "Pro tip: you can check for r/beta values in the `wordcloud.png` names or in the `fb500_age_CTRL_gender.csv`!"
      ],
      "metadata": {
        "id": "WCL6QJ9FBgxk"
      },
      "id": "WCL6QJ9FBgxk"
    },
    {
      "cell_type": "markdown",
      "id": "79a3918d",
      "metadata": {
        "id": "79a3918d"
      },
      "source": [
        "## 3) Topic modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-aw2fk1GVsgR",
      "metadata": {
        "id": "-aw2fk1GVsgR"
      },
      "source": [
        "DLATK needs a 1-gram *message* feature table to train the topic model. So, the first step would be to extract the 1-gram features grouped by message_id (rather than on user_id, as we did above).\n",
        "\n",
        "Note that we need to extract 1grams at the message level, and set group-freq-thresh to 0 (to include all documents)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H-qVvRyeVZgY",
      "metadata": {
        "id": "H-qVvRyeVZgY"
      },
      "outputs": [],
      "source": [
        "database = \"dla_tutorial\"\n",
        "msgs_table = \"msgs\"\n",
        "correl_field = \"message_id\"\n",
        "gft = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Y-0IhVOyVcWK",
      "metadata": {
        "id": "Y-0IhVOyVcWK"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field {correl_field} \\\n",
        "    --group_freq_thresh {gft} \\\n",
        "    --add_ngrams -n 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0a226a7",
      "metadata": {
        "id": "e0a226a7"
      },
      "source": [
        "Then we pick the number of stopwords to ignore."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n7nufzfcV0p_",
      "metadata": {
        "id": "n7nufzfcV0p_"
      },
      "outputs": [],
      "source": [
        "database = \"dla_tutorial\"\n",
        "feat_1gram_msg = \"feat$1gram$msgs$message_id\"\n",
        "N = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's extend the `%sql` row display limit to 200."
      ],
      "metadata": {
        "id": "m30Yfzkv989i"
      },
      "id": "m30Yfzkv989i"
    },
    {
      "cell_type": "code",
      "source": [
        "# sets the output limit\n",
        "%config SqlMagic.displaylimit = 200"
      ],
      "metadata": {
        "id": "qnoDZamF9vCY"
      },
      "id": "qnoDZamF9vCY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%sql\n",
        "\n",
        "SELECT ROW_NUMBER() OVER (ORDER BY n_occ DESC) AS n, feat, n_occ\n",
        "FROM (\n",
        "  SELECT feat, SUM(value) AS n_occ\n",
        "  FROM {{feat_1gram_msg}}\n",
        "  GROUP BY feat\n",
        "  ORDER BY n_occ DESC\n",
        "  LIMIT {{N}}) AS a;"
      ],
      "metadata": {
        "id": "xjt_fyAfWYio"
      },
      "id": "xjt_fyAfWYio",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "By-PdhWxV-J5",
      "metadata": {
        "id": "By-PdhWxV-J5"
      },
      "source": [
        "Based on the list above, we'll remove the top-N words as stop-words when we model the topics below.\n",
        "\n",
        "The topic modeling is done in one DLATK command around the flag **`--estimate_lda_topics`**. Before we continue, let's see what the new parameters mean.\n",
        "\n",
        "**`--estimate_lda_topics`** - tells DLATK to model new topics using the 1gram feature table.\n",
        "\n",
        "**`--num_stopwords`** - Specifies the number of stopwords (50-150 are sensible).\n",
        "\n",
        "**`--num_topics`** - Specifies the number of topics to be extracted (you need 50-100 documents per topic. dla_tutorial has 30k messages = 300-600 topics would be possible, at a maximum).\n",
        "\n",
        "**`--lda_alpha`** - This parameter is used to set the parameter of how many topics you expect per document (5 is a good default. Values from 2-5 or so seem sensible. The longer the document, the larger we want to make this number. For something that's as long as a sentence, like a tweet, 2 is a good number).\n",
        "\n",
        "**`--lexicondb`** - Database where the topic tables should be stored.\n",
        "\n",
        "**`--lda_lexicon_name`** - If we set the lexicon name as NAME we get topic tables as NAME_cp and NAME_freq in the end.\n",
        "\n",
        "**`--mallet_path`** - This is needed to specify the path to Mallet which implements Topic Modeling inside DLATK.\n",
        "\n",
        "Now that we know the parameters, let's drop 200 stop words, model 100 topics, and set the alpha (topics per document) to 5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6555ac41",
      "metadata": {
        "id": "6555ac41"
      },
      "outputs": [],
      "source": [
        "database = 'dla_tutorial'\n",
        "msgs_table = 'msgs'\n",
        "feat_1gram_message = 'feat$1gram$msgs$message_id'\n",
        "\n",
        "num_stopwords = 200\n",
        "num_topics = 100\n",
        "alpha = 5\n",
        "lexicon_database = 'dla_tutorial'\n",
        "\n",
        "lexicon_name = f'dlat_{num_topics}_a{alpha}s{num_stopwords}'\n",
        "\n",
        "OUTPUT_FOLDER = './outputs_tutorial_12'\n",
        "!mkdir -p {OUTPUT_FOLDER}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5be2de8f",
      "metadata": {
        "id": "5be2de8f"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --correl_field message_id \\\n",
        "    --feat_table '{feat_1gram_message}' \\\n",
        "    --estimate_lda_topics \\\n",
        "    --num_stopwords {num_stopwords} \\\n",
        "    --num_topics {num_topics} \\\n",
        "    --lda_alpha {alpha} \\\n",
        "    --lexicondb {lexicon_database} \\\n",
        "    --save_lda_files {OUTPUT_FOLDER} \\\n",
        "    --lda_lexicon_name {lexicon_name} \\\n",
        "    --mallet_path /opt/mallet/bin/mallet"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44bfb380",
      "metadata": {
        "id": "44bfb380"
      },
      "source": [
        "And we print the topic wordclouds using the command below that uses the **`--make_all_topic_wordclouds`** flag."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1755f53",
      "metadata": {
        "id": "b1755f53"
      },
      "outputs": [],
      "source": [
        "database = \"dla_tutorial\"\n",
        "msgs_table = \"msgs\"\n",
        "\n",
        "lexicon_database = \"dla_tutorial\"\n",
        "topics_freq_table = 'dlat_100_a5_s200_freq_t50ll'\n",
        "\n",
        "OUTPUT_FOLDER = './outputs_tutorial_12'\n",
        "OUTPUT_NAME = 'dlat_topic_wordclouds'\n",
        "\n",
        "!mkdir -p {OUTPUT_FOLDER}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "909c7a0f",
      "metadata": {
        "id": "909c7a0f"
      },
      "outputs": [],
      "source": [
        "!dlatkInterface.py \\\n",
        "    --corpdb {database} \\\n",
        "    --corptable {msgs_table} \\\n",
        "    --topic_lexicon {topics_freq_table} \\\n",
        "    --lexicondb {lexicon_database} \\\n",
        "    --make_all_topic_wordclouds \\\n",
        "    --tagcloud_colorscheme blue \\\n",
        "    --output {OUTPUT_FOLDER}/{OUTPUT_NAME}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NXR09vntXzCr",
      "metadata": {
        "id": "NXR09vntXzCr"
      },
      "source": [
        "## References <a name=\"ref\"></a>\n",
        "\n",
        "* [DLATK](https://aclanthology.org/D17-2010.pdf) - publicly available at https://github.com/dlatk/dlatk.\n",
        "* [Closed and Open Vocabulary Approaches to Text Analysis: A Review, Quantitative Comparison, and Recommendations](https://jeichstaedt.com/s/2021psychMethods.pdf)\n",
        "* [Topic Modeling - Latent Dirichlet Allocation](https://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‼️ **Save your database and/or output files** ‼️"
      ],
      "metadata": {
        "id": "bS4qTq9zJdUS"
      },
      "id": "bS4qTq9zJdUS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's save all this work into as a database file in your GDrive `sqlite_databases` folder!"
      ],
      "metadata": {
        "id": "qGExUsbZThmO"
      },
      "id": "qGExUsbZThmO"
    },
    {
      "cell_type": "code",
      "source": [
        "database = 'dla_tutorial'"
      ],
      "metadata": {
        "id": "F7RlPan-KkYX"
      },
      "execution_count": null,
      "outputs": [],
      "id": "F7RlPan-KkYX"
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# copy the database file to your Drive\n",
        "!cp -f \"sqlite_data/{database}.db\" \"/content/drive/MyDrive/sqlite_databases/\"\n",
        "\n",
        "print(f\"✅ Database '{database}.db' has been copied to your Google Drive.\")"
      ],
      "metadata": {
        "id": "Ls5d7ZXwJzxV"
      },
      "execution_count": null,
      "outputs": [],
      "id": "Ls5d7ZXwJzxV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's save the output (wordclouds and files) in this tutorial! Here's how you can save it to your Drive (if you want to)!"
      ],
      "metadata": {
        "id": "_R0h2jyyJfah"
      },
      "id": "_R0h2jyyJfah"
    },
    {
      "cell_type": "code",
      "source": [
        "OUTPUT_FOLDER = './output_tutorial_12'"
      ],
      "metadata": {
        "id": "pZl5sNn-J655"
      },
      "execution_count": null,
      "outputs": [],
      "id": "pZl5sNn-J655"
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Copy the database file to your Drive (-r makes it copy the folder and all files/folders inside)\n",
        "!cp -f -r {OUTPUT_FOLDER} \"/content/drive/MyDrive/\"\n",
        "\n",
        "print(f\"✅ '{OUTPUT_FOLDER}' has been copied to your Google Drive.\")"
      ],
      "metadata": {
        "id": "bwD74MqNJzR-"
      },
      "execution_count": null,
      "outputs": [],
      "id": "bwD74MqNJzR-"
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "fbfbc3e3",
        "qR021QY1IcOZ",
        "1362678e",
        "5dcf14fc",
        "9cf9c29e",
        "ef7c3684",
        "1f7a8a0e",
        "9f8ad6e8",
        "79a3918d"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}