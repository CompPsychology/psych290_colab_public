{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CompPsychology/psych290_colab_public/blob/main/notebooks/week-05/W5_HW5_R_statsWithDLATKTables_(hw5_csv).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyS3K1PiqjiA"
      },
      "source": [
        "# W5 Homework 5: R stats with DLATK Tables (2025-04)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(c) Johannes Eichstaedt & the World Well-Being Project, 2023.\n",
        "\n",
        "✋🏻✋🏻 NOTE - You need to create a copy of this notebook before you work through it. Click on \"Save a copy in Drive\" option in the File menu, and safe it to your Google Drive.\n",
        "\n",
        "✉️🐞 If you find a bug/something doesn't work, please slack us a screenshot, or email johannes.courses@gmail.com.\n",
        "\n",
        "\n",
        "This homework has 15 pts + 1 extra credit."
      ],
      "metadata": {
        "id": "Axy6xk2dCT0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1) Setting up Colab with DLATK, SQLite, and R"
      ],
      "metadata": {
        "id": "5QaT52GVTa4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "database = \"homework_05\""
      ],
      "metadata": {
        "id": "fBlJPLo0Mxc7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1a) Install DLATK"
      ],
      "metadata": {
        "id": "vXIZXCgbTq1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing DLATK and necessary packages\n",
        "!git clone -b psych290 https://github.com/dlatk/dlatk.git\n",
        "!pip install -r dlatk/install/requirements.txt\n",
        "!pip install dlatk/\n",
        "!pip install wordcloud langid jupysql"
      ],
      "metadata": {
        "id": "AMAiPZ6wTsvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1b) Download the data for this tutorial & custom R script"
      ],
      "metadata": {
        "id": "1LENmEIGagjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This github repo contains copies of CSVs for `homework_05` and also `tutorial_07` & `dla_tutorial` CSVs! It also copies over our custom R script `psych290RcodeV1.R`"
      ],
      "metadata": {
        "id": "H0Oe4Tw6EZT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this downloads the csvs & script we need for this tutorial\n",
        "!git clone https://github.com/CompPsychology/psych290_data.git"
      ],
      "metadata": {
        "id": "pBf1-ZruagJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have a look on the left in the file browser, you see csv files in ./psych290_data/homework_05.\n"
      ],
      "metadata": {
        "id": "ysvvLMjTawaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "💡 BTW, if you ever need a copy of `psych290RcodeV1.R` (RStudio at home!), [you can download it here!](https://drive.google.com/drive/folders/1LnEKn7tyBiXLsuNl_SXkqljZFRUs9S4k?usp=sharing)"
      ],
      "metadata": {
        "id": "Mq5doVvLa_Nk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1c) Mount Google Drive and copy databases & custom R functions"
      ],
      "metadata": {
        "id": "dtVVbpmAX2l8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In case you don't have that `dlatk_lexica.db` in your Google Drive, look at Tutorial 5."
      ],
      "metadata": {
        "id": "wAAiyEDpCk5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive & copy to Colab\n",
        "\n",
        "# connects & mounts your Google Drive to this colab space\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# this copies dlatk_lexica.db from your Google Drive to Colab\n",
        "!cp -f \"/content/drive/MyDrive/sqlite_databases/dlatk_lexica.db\" \"sqlite_data\""
      ],
      "metadata": {
        "id": "wtLRHzw9YKeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1d) Setup database connection"
      ],
      "metadata": {
        "id": "B6vJHRAsYUVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loads the %%sql extension\n",
        "%load_ext sql\n",
        "\n",
        "# connects the extension to the database - mounts both databases as engines\n",
        "from sqlalchemy import create_engine\n",
        "tutorial_db_engine = create_engine(f\"sqlite:///sqlite_data/{database}.db?charset=utf8mb4\")\n",
        "dlatk_lexica_engine = create_engine(f\"sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4\")\n",
        "\n",
        "# attaches the dlatk_lexica.db so tutorial_db_engine can query both databases\n",
        "from IPython import get_ipython\n",
        "from sqlalchemy import event\n",
        "\n",
        "# auto‑attach the lexica db whenever tutorial_db_engine connects\n",
        "@event.listens_for(tutorial_db_engine, \"connect\")\n",
        "def _attach_lexica(dbapi_conn, connection_record):\n",
        "    dbapi_conn.execute(\"ATTACH DATABASE 'sqlite_data/dlatk_lexica.db' AS dlatk_lexica;\")\n",
        "\n",
        "%sql tutorial_db_engine\n",
        "\n",
        "#set the output limit to 50\n",
        "%config SqlMagic.displaylimit = 50"
      ],
      "metadata": {
        "id": "6a9mgdfQYb8l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1e) (ONLY If nedded: SOFT RELOAD) If you have a **\"database lock\"** problem"
      ],
      "metadata": {
        "id": "zEaWoSEh7gi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you face a \"database locked\" issue, restart the session (Runtime ==> Restart Session) & run this cell to get set back up!\n",
        "\n",
        "Also, run the R-SQLite setup cell (with db_con = ...) 🤓"
      ],
      "metadata": {
        "id": "es2KDKodVkna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "database = \"homework_05\"\n",
        "\n",
        "%reload_ext sql\n",
        "\n",
        "from sqlalchemy import create_engine\n",
        "tutorial_db_engine = create_engine(f\"sqlite:///sqlite_data/{database}.db?charset=utf8mb4\")\n",
        "dlatk_lexica_engine = create_engine(f\"sqlite:///sqlite_data/dlatk_lexica.db?charset=utf8mb4\")\n",
        "\n",
        "# set the output limit to 50\n",
        "%config SqlMagic.displaylimit = 50\n",
        "\n",
        "from IPython import get_ipython\n",
        "from sqlalchemy import event\n",
        "\n",
        "# auto‑attach the lexica db whenever tutorial_db_engine connects\n",
        "@event.listens_for(tutorial_db_engine, \"connect\")\n",
        "def _attach_lexica(dbapi_conn, connection_record):\n",
        "    dbapi_conn.execute(\"ATTACH DATABASE 'sqlite_data/dlatk_lexica.db' AS dlatk_lexica;\")\n",
        "\n",
        "%sql tutorial_db_engine"
      ],
      "metadata": {
        "id": "fvWfMDXB7gEr"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtNcJnr4qjiC"
      },
      "source": [
        "## 2) R Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the R extension."
      ],
      "metadata": {
        "id": "xiVRIGl8Sjic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load %%R extension\n",
        "%load_ext rpy2.ipython"
      ],
      "metadata": {
        "id": "vyW-BJn0NguN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download and load necessary packages!"
      ],
      "metadata": {
        "id": "IoHE2FlISmcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is equivalent to install.packages() but much faster!!\n",
        "!apt-get update\n",
        "\n",
        "!apt install -y \\\n",
        "    r-cran-ggpubr \\\n",
        "    r-cran-psych \\\n",
        "    r-cran-reshape2 \\\n",
        "    r-cran-lm.beta \\\n",
        "    r-cran-rsqlite \\\n",
        "    r-cran-ggthemes \\\n",
        "    r-cran-reshape2 \\\n",
        "    r-cran-psych \\\n",
        "    r-cran-caret"
      ],
      "metadata": {
        "id": "8xyXsh7JNtJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's load the packages and connect R to our SQLite db!"
      ],
      "metadata": {
        "id": "-m0OpcrnGKhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# constructs the pathname\n",
        "database_path = f\"sqlite_data/{database}.db\"\n",
        "database_path"
      ],
      "metadata": {
        "id": "EGPpJV7QF9K5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a445c86d-f862-4396-c6a6-120ba6b50902"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'sqlite_data/homework_05.db'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "autoscroll": false,
        "collapsed": true,
        "id": "fhdf0rZNqjiC"
      },
      "outputs": [],
      "source": [
        "%%R -i database_path\n",
        "\n",
        "source(\"./psych290_data/helper_files/psych290RcodeV1.R\")\n",
        "\n",
        "require(tidyverse)\n",
        "require(RSQLite)\n",
        "require(ggthemes)\n",
        "require(ggpubr)\n",
        "require(grid)\n",
        "require(reshape2)\n",
        "require(psych)\n",
        "require(lm.beta)\n",
        "library(caret)\n",
        "\n",
        "\n",
        "# load DBI for generic database functions and RSQLite as the SQLite backend\n",
        "library(DBI)\n",
        "library(RSQLite)\n",
        "\n",
        "# connects to a file-based sqlite DB\n",
        "db_con <- dbConnect(RSQLite::SQLite(),\n",
        "                    dbname = database_path)\n",
        "\n",
        "# enforce UTF-8 encoding\n",
        "dbExecute(db_con, \"PRAGMA encoding = 'UTF-8';\")\n",
        "\n",
        "# this attaches the dlatk_lexica database\n",
        "dbExecute(db_con, \"ATTACH DATABASE 'sqlite_data/dlatk_lexica.db' AS dlatk_lexica;\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sO-hA4ZuqjiD"
      },
      "source": [
        "## Question 1)\n",
        "\n",
        "(2 points)\n",
        "\n",
        "Following section 3 and 4 of tutorial 07 for how to work with CSVs, please read the HW CSVs into R and create a clean outcome and a message table with the right indexes into your SQLite database.\n",
        "\n",
        "Then, following sections 6, 7, 8, create versions of the message and outcome tables that are shortlisted to the users with only 500+ words.\n",
        "\n",
        "Show the output of the SQLite `PRAGMA table_info` commands for both tables containing the users with 500+ words (`dbGetQuery(db_con, \"PRAGMA table_info(hw5_msgs_n ...)\")`, and the `checkDf2()` output for the shortlisted outcome table. This will allow us to check that your tables are good.\n",
        "\n",
        "**Hint: Make use of tutorial 07!**"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qGJr0twuV3en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-94ryJwqjiE"
      },
      "source": [
        "\n",
        "## Question 2) (3 points)\n",
        "\n",
        "Based on the tables which were shortlisted to users with more than 500 words, using the R code from Tutorial 7, sections 9, 10, 11 (copy it here please):\n",
        "\n",
        "-   a plot of message counts over time, and a sentence that summarizes the count and the range (e.g., \"Users wrote 33,136 posts between January 1st, 1999 and the 28th of August, 2005\")\n",
        "-   a histogram of messages per user, plus the summary sentence WITH MEDIAN (e.g., \"Users wrote an average of 34 (SD = 96, Median = 10, min = 1, max = 1,308) blog posts.\"\n",
        "-   a histogram of words per user, and a summary sentence WITH MEDIAN (e.g., \"Users wrote an average of 8,931 words (Median = 2694, SD = 24,116) for a total of 8,600,995 words.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xML3zzoVWX0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vgMthxnqjiG"
      },
      "source": [
        "# **Important**\n",
        "\n",
        "For all the following questions, please use the shortlisted message and outcome tables (that only contain the users with 500+ words)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3) Please make an (author-level) cross-correlations table between the LIWC (POSEMO), labmt ( valence ) and NRC ( SENT ) features, and author age and gender.\n",
        "\n",
        "First, extract the 1gram feat table for users from the shortlisted message table from question 1 (for users < 500 words).\n",
        "\n",
        "After that, do make sure any extracted dictionary features use the shortlisted message table *(N.b., group_freq_thresh is not applied during feature extraction, only during correlation.)*\n",
        "\n",
        "**DLATK flags you will need**: `...--add_ngrams -n 1`, `...--add_lex_table -l mini_LIWC2015` etc.\n",
        "\n",
        "**R-commands you will need**: dbGetQuery, importFeat (from the R source script), merge, cor() -- or something similar.\n",
        "\n",
        "LIWC is a theory-based, LabMT an annotation-based, and NRC a machine-learning based model. Which one of these three doesn't correlate highly with the other two?\n",
        "\n",
        "**Note:** Remember the sparse encoding in feat tables. Fortunately, dictionaries have the `_intercept` features in them for every user. That means as long as you importFeat() a dictionary table with this `_intercept` feature, you will get all missing zeros!\n",
        "\n",
        "\n",
        "💡💡💡 **Hint**: In this question, you need to extract 1 grams features, then extract dictionary features from `LIWC2015`, `labmt`, and `nrc`. Finally you must construct a cross-correlations table between the three language dictionary features of interest (POSEMO, valence, and SENT), `age`, and `gender` (encoded as `is_female`).\n"
      ],
      "metadata": {
        "id": "3Kc6NRSfHyGD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HHOfBd0GWUPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9gzdb2IqjiH"
      },
      "source": [
        "## Question 4) Please calculate a Cohen's d effect size (standard deviation difference) in how genders 0 and 1 use LIWC POSEMO and NRC Sent. Which dictionary distinguishes better between the genders?\n",
        "\n",
        "R commands -- cohen.d(...)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xLXg6yVJWXCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0UHGXKbqjiH"
      },
      "source": [
        "## Question 5) Following from the previous question, please produce 3 plots plotting LIWC (`POSEMO`), labmt (`valence`) and NRC (`SENT`) against author-age, and add a line-of-best-fit (either linear or LOESS).\n",
        "\n",
        "R-commands you will need: `dbGetQuery`, `importFeat` (from the custom R source script), `merge`, `qplot/ggplot`, `+geom_smooth()` or `+geom_smooth(method = \"lm\")`, the `+theme_Publication()` to make it look kewl (from the R source script)."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aurjp7ANWi3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOpOw2I5qjiH"
      },
      "source": [
        "## Question 6) Can you regress labmt (valence) and NRC (SENT) against age, controlling for gender? Please provide standardized coefficients with p-values.\n",
        "\n",
        "R-commands you will need: `dbGetQuery`, `importFeat` (from the R source script), `merge`, `lm`, and the `lm.beta` package to get betas. FYI: `lm.beta` works by taking the model outputted by `lm`, and then standardizing it, like so: `model <- lm(), model_stand <- lm.beta(model)`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SfNtN0FIWmGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LWtVFrqjiQ"
      },
      "source": [
        "## Question 7) As you may recall, in a contribution in EMNLP (one of the top-3 NLP conferences), friend-of-the-lab Maarten Sap published dictionaries to predict age and gender from language. Please extract the data-driven dictionary for age and gender (`dlatk_lexica.dd_emnlp14_ageGender`). Please make one histogram each of the predicted age, and for the age in the outcomes table.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LxvSBSoAWmsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBhSym3nqjiR"
      },
      "source": [
        "## Question 8)\n",
        "\n",
        "(2 points)\n",
        "\n",
        "(1pt) How well does Sap's age dictionary looks like it works? Please plot author age against dictionary-predicted age, and add a line-of-best fit (either linear or LOESS).\n",
        "\n",
        "R-commands you will need: `dbGetQuery`, `importFeat` (from the R source script), `merge`, `qplot/ggplot`, `+geom_smooth()` or `+geom_smooth(method = \"lm\")`, the `+ theme_Publication()` (from the R source script)\n",
        "\n",
        "(1pt) What's the standardized statistical association between user age and Sap-dictionary-age, controlling for gender (what's the beta)?"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q1H87OWcWsMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wPr86_9hqjiR"
      },
      "source": [
        "## Question 9) (EXTRA CREDIT) Repeat the above, but produce two lines of best fit, one for each author gender. which one has a (slightly) steeper slope?\n",
        "\n",
        "hint: use ggplot's `group`"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AeuFCXmNWuhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX3-TfiIqjiR"
      },
      "source": [
        "## Question 10) How many years is the dictionary 'prediction' off, on average? Please compute the mean-absolute error of the dictionary age 'predictions'.\n",
        "\n",
        "FYI: `MAE = sum( abs(var1 - var2))`. Does this seem like a lot to you?\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dKgOuqIsWxJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Vww6EkXqjiR"
      },
      "source": [
        "## Question 11) Let's investigate how well the gender prediction dictionary works.\n",
        "\n",
        "Please make a histogram of the language-predicted gender values, and group it by the self-reported genders, such that you see overlapping histograms of predicted values, one for each (self-reported) gender.\n",
        "\n",
        "What would be a good threshold on the predicted gender value to distinguish well between the genders?\n",
        "\n",
        "Please draw this as a line in your combined histogram.\n",
        "\n",
        "Hint: `ggplot:: + geom_vline()`\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2pybDufHWyfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_oGZEsPqjiS"
      },
      "source": [
        "## Question 12) Using the reasonable threshold on the continuous gender prediction that you've picked, compute a confusion matrix.\n",
        "\n",
        "You do this by first \"declaring\" the users above or below your threshold to have been classified as 1 or 0. Now that you've turned the continuous gender score into a \"gender classification,\" you can compare these predicted genders against the self-reported genders.\n",
        "\n",
        "Please report accuracy, precision and recall for your choice of the threshold, and the F1 score, which combines precision and recall conservatively -- giving an average that's closer to the lower number (`F1 = 2 / (1/recall + 1/precision)`).\n",
        "\n",
        "Does this seem like satisfactory prediction performance to you?\n",
        "\n",
        "R hints:\n",
        "```\n",
        "df$a_bin <- ifelse (a < X, 1, 0)\n",
        "confusionMatrix(... reference = ground_truth)`\n",
        "```\n",
        "\n",
        "`confusionMatrix()` comes from `caret` package!"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ur4s-xeWW0zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‼️ **Save your database** ‼️"
      ],
      "metadata": {
        "id": "bS4qTq9zJdUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's save all this work into as a new database file in your GDrive `sqlite_databases` folder!"
      ],
      "metadata": {
        "id": "qGExUsbZThmO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "database = 'homework_05'"
      ],
      "metadata": {
        "id": "F7RlPan-KkYX"
      },
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# copy the database file to your Drive\n",
        "!cp -f \"sqlite_data/{database}.db\" \"/content/drive/MyDrive/sqlite_databases/\"\n",
        "\n",
        "print(f\"✅ Database '{database}.db' has been copied to your Google Drive.\")"
      ],
      "metadata": {
        "id": "Ls5d7ZXwJzxV",
        "outputId": "47cf839b-d0f2-4fa3-dbfa-960726b94a64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Database 'homework_05.db' has been copied to your Google Drive.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "Rmd_header": {
      "output": "html_document",
      "title": "W5 Homework 5: Working in R with feature tables, regressions and plots"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zEaWoSEh7gi5"
      ],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}